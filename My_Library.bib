@inproceedings{renartOnlineDecisionMakingUsing2017,
  title = {Online {{Decision}}-{{Making Using Edge Resources}} for {{Content}}-{{Driven Stream Processing}}},
  isbn = {978-1-5386-2686-3},
  doi = {10.1109/eScience.2017.52},
  publisher = {{IEEE}},
  author = {Renart, Eduard and {Balouek-Thomert}, Daniel and Hu, Xuan and Gong, Jie and Parashar, Manish},
  month = oct,
  year = {2017},
  pages = {384-392},
  file = {/Users/benblamey/Zotero/storage/GMB7BJJW/2686a384.pdf}
}

@article{blomerSurveyDistributedFile2015,
  title = {A {{Survey}} on {{Distributed File System Technology}}},
  volume = {608},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/608/1/012039},
  journal = {Journal of Physics: Conference Series},
  author = {Blomer, J},
  month = may,
  year = {2015},
  pages = {012039},
  file = {/Users/benblamey/Zotero/storage/WTAXXHRI/survey-on-dist-filesystems-blomer.pdf}
}

@article{borthakurHadoopDistributedFile2007,
  title = {The Hadoop Distributed File System: {{Architecture}} and Design},
  volume = {11},
  shorttitle = {The Hadoop Distributed File System},
  number = {2007},
  journal = {Hadoop Project Website},
  author = {Borthakur, Dhruba},
  year = {2007},
  pages = {21},
  file = {/Users/benblamey/Zotero/storage/SWZ9F2PD/hdfs_design.pdf}
}

@inproceedings{huntZooKeeperWaitfreeCoordination2010,
  title = {{{ZooKeeper}}: {{Wait}}-Free {{Coordination}} for {{Internet}}-Scale {{Systems}}.},
  volume = {8},
  shorttitle = {{{ZooKeeper}}},
  booktitle = {{{USENIX}} Annual Technical Conference},
  publisher = {{Boston, MA, USA}},
  author = {Hunt, Patrick and Konar, Mahadev and Junqueira, Flavio Paiva and Reed, Benjamin},
  year = {2010},
  pages = {9},
  file = {/Users/benblamey/Zotero/storage/8GIRY9JQ/Hunt_Zookeeper.pdf}
}

@inproceedings{sagirogluBigDataReview2013,
  title = {Big Data: {{A}} Review},
  shorttitle = {Big Data},
  booktitle = {Collaboration {{Technologies}} and {{Systems}} ({{CTS}}), 2013 {{International Conference}} On},
  publisher = {{IEEE}},
  author = {Sagiroglu, Seref and Sinanc, Duygu},
  year = {2013},
  pages = {42--47},
  file = {/Users/benblamey/Zotero/storage/6C972TCP/bigdata-review.pdf}
}

@article{philipchenDataintensiveApplicationsChallenges2014,
  title = {Data-Intensive Applications, Challenges, Techniques and Technologies: {{A}} Survey on {{Big Data}}},
  volume = {275},
  issn = {00200255},
  shorttitle = {Data-Intensive Applications, Challenges, Techniques and Technologies},
  doi = {10.1016/j.ins.2014.01.015},
  language = {en},
  journal = {Information Sciences},
  author = {Philip Chen, C.L. and Zhang, Chun-Yang},
  month = aug,
  year = {2014},
  pages = {314-347},
  file = {/Users/benblamey/Zotero/storage/5GS5VL8U/data-intensive-applications-survey.pdf}
}

@article{xuComprehensiveSurveyClustering2015,
  title = {A {{Comprehensive Survey}} of {{Clustering Algorithms}}},
  volume = {2},
  issn = {2198-5804, 2198-5812},
  doi = {10.1007/s40745-015-0040-1},
  language = {en},
  number = {2},
  journal = {Annals of Data Science},
  author = {Xu, Dongkuan and Tian, Yingjie},
  month = jun,
  year = {2015},
  pages = {165-193},
  file = {/Users/benblamey/Zotero/storage/85AU4DA5/1000013653837.pdf}
}

@inproceedings{cherniackScalableDistributedStream2003,
  title = {Scalable {{Distributed Stream Processing}}.},
  volume = {3},
  booktitle = {{{CIDR}}},
  author = {Cherniack, Mitch and Balakrishnan, Hari and Balazinska, Magdalena and Carney, Donald and Cetintemel, Ugur and Xing, Ying and Zdonik, Stanley B.},
  year = {2003},
  pages = {257--268},
  file = {/Users/benblamey/Zotero/storage/BK6GHVTJ/Cherniack et al. - 2003 - Scalable Distributed Stream Processing..pdf;/Users/benblamey/Zotero/storage/GQZ22UFY/Cherniack et al. - 2003 - Scalable Distributed Stream Processing..pdf}
}

@article{stonebrakerRequirementsRealtimeStream2005,
  title = {The 8 Requirements of Real-Time Stream Processing},
  volume = {34},
  number = {4},
  journal = {ACM SIGMOD Record},
  author = {Stonebraker, Michael and {\c C}etintemel, U{\v g}ur and Zdonik, Stan},
  year = {2005},
  keywords = {benchmarking},
  pages = {42--47},
  file = {/Users/benblamey/Zotero/storage/TEXWZGHY/Stonebraker et al. - 2005 - The 8 requirements of real-time stream processing.pdf;/Users/benblamey/Zotero/storage/XU5MQ82C/Stonebraker et al. - 2005 - The 8 requirements of real-time stream processing.pdf;/Users/benblamey/Zotero/storage/XB4MNRDS/citation.html}
}

@article{pietzschBigDataViewerVisualizationProcessing2015,
  title = {{{BigDataViewer}}: Visualization and Processing for Large Image Data Sets},
  volume = {12},
  copyright = {2015 Nature Publishing Group},
  issn = {1548-7105},
  shorttitle = {{{BigDataViewer}}},
  doi = {10.1038/nmeth.3392},
  abstract = {BigDataViewer: visualization and processing for large image data sets},
  language = {En},
  number = {6},
  journal = {Nature Methods},
  author = {Pietzsch, Tobias and Saalfeld, Stephan and Preibisch, Stephan and Tomancak, Pavel},
  month = jun,
  year = {2015},
  pages = {481},
  file = {/Users/benblamey/Zotero/storage/HB6W9LWT/Pietzsch et al. - 2015 - BigDataViewer visualization and processing for la.pdf;/Users/benblamey/Zotero/storage/D5HQ6DYS/nmeth.html}
}

@inproceedings{shiParallelFishImage2015,
  title = {A {{Parallel Fish Image Processing Pipeline}} of {{High}}-{{Throughput Chromosomal Analysis}}},
  doi = {10.1109/ITME.2015.159},
  abstract = {The detection of tumor cells is critical for early diagnosis of primary and metastatic tumor as well as for monitoring the anti-cancer treatment effect. Fluorescence in situ hybridization (FISH) probes have been utilized to detect the unique characteristics of either over-expressed oncogenes or abnormalities of chromosomes in tumor cell nuclei. Automated quantification of chromosomes and their interphase cell nuclei is required for high-throughput analysis of cellular dynamics, especially in clinical applications. We present a fully automated FISH image processing pipeline that is able to robustly detect and characterize both cells and chromosomes in a parallel way. Nuclei and chromosomes are treated with different strategies because of inconsistent internal features and external needs, and then re-associated for consolidated chromosomal analysis and tumor cell characterization. Experimental results of the combined workflow showed the high accuracy and efficiency of the both chromosome detection and cell segmentation.},
  booktitle = {2015 7th {{International Conference}} on {{Information Technology}} in {{Medicine}} and {{Education}} ({{ITME}})},
  author = {Shi, P. and Wan, M. and Hong, J. and Chen, J. and Zhang, L.},
  month = nov,
  year = {2015},
  keywords = {pipeline processing,anticancer treatment effect,Biological cells,cancer,cell segmentation,Cell Segmentation,cellular biophysics,cellular dynamics,chromosomal analysis,chromosome detection,Chromosome Detection,clinical applications,Computer architecture,early diagnosis,FISH probes,Fluorescence In Situ Hybridization (FISH),fluorescence in situ hybridization probes,fully automated FISH image processing pipeline,high-throughput analysis,high-throughput chromosomal analysis,image classification,Image color analysis,image segmentation,Image segmentation,interphase cell nuclei,medical image processing,metastatic tumor,Microscopic Image Processing,parallel FISH image processing pipeline,patient treatment,Shape,tumor cell characterization,Tumor Cell Characterization,tumor cell detection,tumor cell nuclei,Tumors,tumours},
  pages = {337-342},
  file = {/Users/benblamey/Zotero/storage/MWQ8B4ZG/Shi et al. - 2015 - A Parallel Fish Image Processing Pipeline of High-.pdf;/Users/benblamey/Zotero/storage/UAE6JMUC/7429161.html}
}

@article{heideFlexISPFlexibleCamera2014,
  title = {{{FlexISP}}: {{A Flexible Camera Image Processing Framework}}},
  volume = {33},
  issn = {0730-0301},
  shorttitle = {{{FlexISP}}},
  doi = {10.1145/2661229.2661260},
  abstract = {Conventional pipelines for capturing, displaying, and storing images are usually defined as a series of cascaded modules, each responsible for addressing a particular problem. While this divide-and-conquer approach offers many benefits, it also introduces a cumulative error, as each step in the pipeline only considers the output of the previous step, not the original sensor data. We propose an end-to-end system that is aware of the camera and image model, enforces natural-image priors, while jointly accounting for common image processing steps like demosaicking, denoising, deconvolution, and so forth, all directly in a given output representation (e.g., YUV, DCT). Our system is flexible and we demonstrate it on regular Bayer images as well as images from custom sensors. In all cases, we achieve large improvements in image quality and signal reconstruction compared to state-of-the-art techniques. Finally, we show that our approach is capable of very efficiently handling high-resolution images, making even mobile implementations feasible.},
  number = {6},
  journal = {ACM Trans. Graph.},
  author = {Heide, Felix and Steinberger, Markus and Tsai, Yun-Ta and Rouf, Mushfiqur and Paj\k{a}k, Dawid and Reddy, Dikpal and Gallo, Orazio and Liu, Jing and Heidrich, Wolfgang and Egiazarian, Karen and Kautz, Jan and Pulli, Kari},
  month = nov,
  year = {2014},
  keywords = {image processing,image reconstruction},
  pages = {231:1--231:13},
  file = {/Users/benblamey/Zotero/storage/BSEJ3ZZ2/Heide et al. - 2014 - FlexISP A Flexible Camera Image Processing Framew.pdf}
}

@article{waltScikitimageImageProcessing2014,
  title = {Scikit-Image: Image Processing in {{Python}}},
  volume = {2},
  issn = {2167-8359},
  shorttitle = {Scikit-Image},
  doi = {10.7717/peerj.453},
  abstract = {scikit-image is an image processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal Modified BSD open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-image library, and we showcase several real-world image processing applications that use scikit-image. More information can be found on the project homepage, http://scikit-image.org.},
  language = {en},
  journal = {PeerJ},
  author = {van der Walt, St\'efan and Sch\"onberger, Johannes L. and {Nunez-Iglesias}, Juan and Boulogne, Fran{\c c}ois and Warner, Joshua D. and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
  month = jun,
  year = {2014},
  pages = {e453},
  file = {/Users/benblamey/Zotero/storage/EGQEPMSA/Walt et al. - 2014 - scikit-image image processing in Python.pdf;/Users/benblamey/Zotero/storage/JUNVNQCA/453.html}
}

@inproceedings{mullapudiPolyMageAutomaticOptimization2015,
  address = {New York, NY, USA},
  series = {ASPLOS '15},
  title = {{{PolyMage}}: {{Automatic Optimization}} for {{Image Processing Pipelines}}},
  isbn = {978-1-4503-2835-7},
  shorttitle = {{{PolyMage}}},
  doi = {10.1145/2694344.2694364},
  abstract = {This paper presents the design and implementation of PolyMage, a domain-specific language and compiler for image processing pipelines. An image processing pipeline can be viewed as a graph of interconnected stages which process images successively. Each stage typically performs one of point-wise, stencil, reduction or data-dependent operations on image pixels. Individual stages in a pipeline typically exhibit abundant data parallelism that can be exploited with relative ease. However, the stages also require high memory bandwidth preventing effective utilization of parallelism available on modern architectures. For applications that demand high performance, the traditional options are to use optimized libraries like OpenCV or to optimize manually. While using libraries precludes optimization across library routines, manual optimization accounting for both parallelism and locality is very tedious. The focus of our system, PolyMage, is on automatically generating high-performance implementations of image processing pipelines expressed in a high-level declarative language. Our optimization approach primarily relies on the transformation and code generation capabilities of the polyhedral compiler framework. To the best of our knowledge, this is the first model-driven compiler for image processing pipelines that performs complex fusion, tiling, and storage optimization automatically. Experimental results on a modern multicore system show that the performance achieved by our automatic approach is up to 1.81x better than that achieved through manual tuning in Halide, a state-of-the-art language and compiler for image processing pipelines. For a camera raw image processing pipeline, our performance is comparable to that of a hand-tuned implementation.},
  booktitle = {Proceedings of the {{Twentieth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  publisher = {{ACM}},
  author = {Mullapudi, Ravi Teja and Vasista, Vinay and Bondhugula, Uday},
  year = {2015},
  keywords = {image processing,domain-specific language,locality,multicores,parallelism,polyhedral optimization,tiling,vectorization},
  pages = {429--443},
  file = {/Users/benblamey/Zotero/storage/4TW5ZHT7/Mullapudi et al. - 2015 - PolyMage Automatic Optimization for Image Process.pdf}
}

@inproceedings{chintapalliBenchmarkingStreamingComputation2016,
  title = {Benchmarking Streaming Computation Engines: {{Storm}}, {{Flink}} and {{Spark}} Streaming},
  shorttitle = {Benchmarking Streaming Computation Engines},
  booktitle = {Parallel and {{Distributed Processing Symposium Workshops}}, 2016 {{IEEE International}}},
  publisher = {{IEEE}},
  author = {Chintapalli, Sanket and Dagit, Derek and Evans, Bobby and Farivar, Reza and Graves, Thomas and Holderbaugh, Mark and Liu, Zhuo and Nusbaum, Kyle and Patil, Kishorkumar and Peng, Boyang Jerry},
  year = {2016},
  pages = {1789--1792},
  file = {/Users/benblamey/Zotero/storage/55YKXHF3/07530084.pdf;/Users/benblamey/Zotero/storage/G8SZ7AXE/june291220capitalonefarivar-160710172737.pdf}
}

@misc{pointerApacheFlinkNew2015,
  title = {Apache {{Flink}}: {{New Hadoop}} Contender Squares off against {{Spark}}},
  shorttitle = {Apache {{Flink}}},
  abstract = {A flexible replacement for Hadoop MapReduce that supports real-time and batch processing, Flink offers advantages over Spark},
  language = {en},
  journal = {InfoWorld},
  howpublished = {https://www.infoworld.com/article/2919602/hadoop/flink-hadoops-new-contender-for-mapreduce-spark.html},
  author = {Pointer, Ian},
  year = {2015-05-07T03:00-05:00},
  file = {/Users/benblamey/Zotero/storage/EF6HR9QN/flink-hadoops-new-contender-for-mapreduce-spark.html}
}

@misc{WhichStreamProcessing2016,
  title = {Which {{Stream Processing Engine}}: {{Storm}}, {{Spark}}, {{Samza}}, or {{Flink}}? - {{AltaTerra Research}}},
  shorttitle = {Which {{Stream Processing Engine}}},
  howpublished = {https://web.archive.org/web/20160405211841/http://www.altaterra.net/blogpost/288668/225612/Which-Stream-Processing-Engine-Storm-Spark-Samza-or-Flink/},
  month = apr,
  year = {2016},
  file = {/Users/benblamey/Zotero/storage/8SU3HESH/Which-Stream-Processing-Engine-Storm-Spark-Samza-or-Flink.html}
}

@misc{HighthroughputLowlatencyExactlyonce2015,
  title = {High-Throughput, Low-Latency, and Exactly-Once Stream Processing with {{Apache Flink}}\texttrademark{}},
  abstract = {The popularity of stream data platforms is skyrocketing. Several companies are transitioning parts of their data infrastructure to a streaming paradigm as a solution to increasing demands for real-time access to information. Infrastructures based on streaming data not only enable new types of latency-critical applications and give more actual operationalRead more},
  language = {en-US},
  journal = {data Artisans},
  month = aug,
  year = {2015},
  file = {/Users/benblamey/Zotero/storage/PJZVKES3/06-flink.pdf;/Users/benblamey/Zotero/storage/VXJV2STM/high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink.html}
}

@misc{StreamingBigData2015,
  title = {Streaming {{Big Data}}: {{Storm}}, {{Spark}} and {{Samza}}},
  shorttitle = {Streaming {{Big Data}}},
  abstract = {There are a number of distributed computation systems that can process Big Data in real time or near-real time. This article will start with~a short description of~three~Apache frameworks, and atte\ldots{}},
  language = {en},
  journal = {Blog T},
  month = feb,
  year = {2015},
  file = {/Users/benblamey/Zotero/storage/6MLGM8ER/streaming-big-data-storm-spark-and-samza.html}
}

@inproceedings{krishHatsHeterogeneityawareTiered2014,
  title = {Hats: {{A}} Heterogeneity-Aware Tiered Storage for Hadoop},
  shorttitle = {Hats},
  booktitle = {Cluster, {{Cloud}} and {{Grid Computing}} ({{CCGrid}}), 2014 14th {{IEEE}}/{{ACM International Symposium}} On},
  publisher = {{IEEE}},
  author = {Krish, K. R. and Anwar, Ali and Butt, Ali R.},
  year = {2014},
  pages = {502--511},
  file = {/Users/benblamey/Zotero/storage/UARJ6EBJ/Krish et al. - 2014 - hats A heterogeneity-aware tiered storage for had.pdf;/Users/benblamey/Zotero/storage/6V8NWQP5/6846486.html}
}

@article{anjosBIGhybridSimulatorMapReduce2016,
  title = {{{BIGhybrid}}: A Simulator for {{MapReduce}} Applications in Hybrid Distributed Infrastructures Validated with the {{Grid5000}} Experimental Platform},
  volume = {28},
  shorttitle = {{{BIGhybrid}}},
  number = {8},
  journal = {Concurrency and Computation: Practice and Experience},
  author = {Anjos, Julio and Fedak, Gilles and Geyer, Claudio FR},
  year = {2016},
  pages = {2416--2439},
  file = {/Users/benblamey/Zotero/storage/I44YPX82/Anjos et al. - 2016 - BIGhybrid a simulator for MapReduce applications;/Users/benblamey/Zotero/storage/JRGMPPF2/Anjos et al. - 2016 - BIGhybrid a simulator for MapReduce applications .pdf}
}

@inproceedings{dosanjosSMARTApplicationFramework2015,
  title = {{{SMART}}: An Application Framework for Real Time Big Data Analysis on Heterogeneous Cloud Environments},
  shorttitle = {{{SMART}}},
  booktitle = {Computer and {{Information Technology}}; {{Ubiquitous Computing}} and {{Communications}}; {{Dependable}}, {{Autonomic}} and {{Secure Computing}}; {{Pervasive Intelligence}} and {{Computing}} ({{CIT}}/{{IUCC}}/{{DASC}}/{{PICOM}}), 2015 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  author = {Dos Anjos, Julio CS and Assun{\c c}\~ao, Marcos D. and Bez, Jean and Geyer, Claudio and De Freitas, Edison Pignaton and Carissimi, Alexandre and Costa, Jo\~ao Paulo CL and Fedak, Gilles and Freitag, Felix and Markl, Volker},
  year = {2015},
  pages = {199--206},
  file = {/Users/benblamey/Zotero/storage/YCIVXTC6/Dos Anjos et al. - 2015 - SMART an application framework for real time big .pdf;/Users/benblamey/Zotero/storage/EXPFJ7C8/7363071.html}
}

@inproceedings{anwarManagingVariabilityCloud2016,
  title = {Towards Managing Variability in the Cloud},
  booktitle = {Parallel and {{Distributed Processing Symposium Workshops}}, 2016 {{IEEE International}}},
  publisher = {{IEEE}},
  author = {Anwar, Ali and Cheng, Yue and Butt, Ali R.},
  year = {2016},
  pages = {1081--1084},
  file = {/Users/benblamey/Zotero/storage/X4I3WLS2/Anwar et al. - 2016 - Towards managing variability in the cloud.pdf;/Users/benblamey/Zotero/storage/ANFD7GBC/7529984.html}
}

@inproceedings{chengCastTieringStorage2015,
  title = {Cast: {{Tiering}} Storage for Data Analytics in the Cloud},
  shorttitle = {Cast},
  booktitle = {Proceedings of the 24th {{International Symposium}} on {{High}}-{{Performance Parallel}} and {{Distributed Computing}}},
  publisher = {{ACM}},
  author = {Cheng, Yue and Iqbal, M. Safdar and Gupta, Aayush and Butt, Ali R.},
  year = {2015},
  pages = {45--56},
  file = {/Users/benblamey/Zotero/storage/YPDBYR63/Cheng et al. - 2015 - Cast Tiering storage for data analytics in the cl.pdf;/Users/benblamey/Zotero/storage/3UALUAKF/citation.html}
}

@inproceedings{zhangAdaptiveDataMigration2010,
  title = {Adaptive Data Migration in Multi-Tiered Storage Based Cloud Environment},
  booktitle = {Cloud {{Computing}} ({{CLOUD}}), 2010 {{IEEE}} 3rd {{International Conference}} On},
  publisher = {{IEEE}},
  author = {Zhang, Gong and Chiu, Lawrence and Liu, Ling},
  year = {2010},
  pages = {148--155},
  file = {/Users/benblamey/Zotero/storage/W6R9KBKD/Zhang et al. - 2010 - Adaptive data migration in multi-tiered storage ba.pdf;/Users/benblamey/Zotero/storage/VYJX7YZ8/5558000.html}
}

@inproceedings{ryuFlashStreamMultitieredStorage2013,
  title = {{{FlashStream}}: {{A}} Multi-Tiered Storage Architecture for Adaptive {{HTTP}} Streaming},
  shorttitle = {{{FlashStream}}},
  booktitle = {Proceedings of the 21st {{ACM}} International Conference on {{Multimedia}}},
  publisher = {{ACM}},
  author = {Ryu, Moonkyung and Ramachandran, Umakishore},
  year = {2013},
  pages = {313--322},
  file = {/Users/benblamey/Zotero/storage/A68ZBEXR/Ryu and Ramachandran - 2013 - FlashStream A multi-tiered storage architecture f.pdf;/Users/benblamey/Zotero/storage/IEXHWU7E/citation.html}
}

@inproceedings{ryuWhyAreStateoftheart2012,
  title = {Why Are State-of-the-Art Flash-Based Multi-Tiered Storage Systems Performing Poorly for {{HTTP}} Video Streaming?},
  booktitle = {Proceedings of the 22nd International Workshop on {{Network}} and {{Operating System Support}} for {{Digital Audio}} and {{Video}}},
  publisher = {{ACM}},
  author = {Ryu, Moonkyung and Kim, Hyojun and Ramachandran, Umakishore},
  year = {2012},
  pages = {3--8},
  file = {/Users/benblamey/Zotero/storage/P335VNNQ/Ryu et al. - 2012 - Why are state-of-the-art flash-based multi-tiered .pdf;/Users/benblamey/Zotero/storage/QNQNCL5G/citation.html}
}

@article{hellandImmutabilityChangesEverything2015,
  title = {Immutability {{Changes Everything}}},
  volume = {13},
  issn = {1542-7730},
  doi = {10.1145/2857274.2884038},
  abstract = {We need it, we can afford it, and the time is now.},
  number = {9},
  journal = {Queue},
  author = {Helland, Pat},
  month = nov,
  year = {2015},
  pages = {40:101--40:125},
  file = {/Users/benblamey/Zotero/storage/JTZ2Y4GD/Helland - 2015 - Immutability Changes Everything.pdf}
}

@inproceedings{kulkarniTwitterHeronStream2015,
  address = {New York, NY, USA},
  series = {SIGMOD '15},
  title = {Twitter {{Heron}}: {{Stream Processing}} at {{Scale}}},
  isbn = {978-1-4503-2758-9},
  shorttitle = {Twitter {{Heron}}},
  doi = {10.1145/2723372.2742788},
  abstract = {Storm has long served as the main platform for real-time analytics at Twitter. However, as the scale of data being processed in real-time at Twitter has increased, along with an increase in the diversity and the number of use cases, many limitations of Storm have become apparent. We need a system that scales better, has better debug-ability, has better performance, and is easier to manage -- all while working in a shared cluster infrastructure. We considered various alternatives to meet these needs, and in the end concluded that we needed to build a new real-time stream data processing system. This paper presents the design and implementation of this new system, called Heron. Heron is now the de facto stream data processing engine inside Twitter, and in this paper we also share our experiences from running Heron in production. In this paper, we also provide empirical evidence demonstrating the efficiency and scalability of Heron.

This article is summarized in:
the morning paper

an interesting/influential/important paper from the world of CS every weekday morning, as selected by Adrian Colyer},
  booktitle = {Proceedings of the 2015 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  publisher = {{ACM}},
  author = {Kulkarni, Sanjeev and Bhagat, Nikunj and Fu, Maosong and Kedigehalli, Vikas and Kellogg, Christopher and Mittal, Sailesh and Patel, Jignesh M. and Ramasamy, Karthik and Taneja, Siddarth},
  year = {2015},
  keywords = {real-time data processing.,stream data processing systems},
  pages = {239--250},
  file = {/Users/benblamey/Zotero/storage/LWPG7NHN/Kulkarni et al. - 2015 - Twitter Heron Stream Processing at Scale.pdf}
}

@article{gulisanoStreamCloudElasticScalable2012,
  title = {{{StreamCloud}}: {{An Elastic}} and {{Scalable Data Streaming System}}},
  volume = {23},
  issn = {1045-9219},
  shorttitle = {{{StreamCloud}}},
  doi = {10.1109/TPDS.2012.24},
  abstract = {Many applications in several domains such as telecommunications, network security, large-scale sensor networks, require online processing of continuous data flows. They produce very high loads that requires aggregating the processing capacity of many nodes. Current Stream Processing Engines do not scale with the input load due to single-node bottlenecks. Additionally, they are based on static configurations that lead to either under or overprovisioning. In this paper, we present StreamCloud, a scalable and elastic stream processing engine for processing large data stream volumes. StreamCloud uses a novel parallelization technique that splits queries into subqueries that are allocated to independent sets of nodes in a way that minimizes the distribution overhead. Its elastic protocols exhibit low intrusiveness, enabling effective adjustment of resources to the incoming load. Elasticity is combined with dynamic load balancing to minimize the computational resources used. The paper presents the system design, implementation, and a thorough evaluation of the scalability and elasticity of the fully implemented system.},
  number = {12},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  author = {Gulisano, V. and {Jim\'enez-Peris}, R. and {Pati\~no-Mart\'inez}, M. and Soriente, C. and Valduriez, P.},
  month = dec,
  year = {2012},
  keywords = {cloud computing,Cloud computing,continuous data flows,data handling,Data streaming,dynamic load balancing,elastic data streaming system,elastic protocols,elasticity,Elasticity,Load management,parallel processing,parallelization technique,Peer to peer computing,resource allocation,scalability,Scalability,scalable data streaming system,Semantics,stream processing engines,StreamCloud,Streaming media},
  pages = {2351-2365},
  file = {/Users/benblamey/Zotero/storage/5MW6DKFF/Gulisano et al. - 2012 - StreamCloud An Elastic and Scalable Data Streamin.pdf;/Users/benblamey/Zotero/storage/NE8VQ57A/6127868.html}
}

@article{pereraReproducibleExperimentsComparing2016,
  title = {Reproducible {{Experiments}} for {{Comparing Apache Flink}} and {{Apache Spark}} on {{Public Clouds}}},
  journal = {arXiv preprint arXiv:1610.04493},
  author = {Perera, Shelan and Perera, Ashansa and Hakimzadeh, Kamal},
  year = {2016},
  file = {/Users/benblamey/Zotero/storage/KD3JRFHD/ce070ec3151f4af4477da3798dd5196e1476.pdf}
}

@article{bordinBenchmarkSuiteDistributed2017,
  title = {A Benchmark Suite for Distributed Stream Processing Systems},
  author = {Bordin, Maycon Viana},
  year = {2017},
  file = {/Users/benblamey/Zotero/storage/9GQXJQE3/001023932.pdf}
}

@misc{BigDataBenchBigData,
  title = {{BigDataBench | A Big Data and AI ( Artificial Intelligence ) Benchmark Suite, ICT, Chinese Academy of Sciences}},
  language = {zh-CN},
  file = {/Users/benblamey/Zotero/storage/XS6TEBQE/prof.ict.ac.cn.html}
}

@inproceedings{liSparkBenchComprehensiveBenchmarking2015,
  address = {New York, NY, USA},
  series = {CF '15},
  title = {{{SparkBench}}: {{A Comprehensive Benchmarking Suite}} for in {{Memory Data Analytic Platform Spark}}},
  isbn = {978-1-4503-3358-0},
  shorttitle = {{{SparkBench}}},
  doi = {10.1145/2742854.2747283},
  abstract = {Spark has been increasingly adopted by industries in recent years for big data analysis by providing a fault tolerant, scalable and easy-to-use in memory abstraction. Moreover, the community has been actively developing a rich ecosystem around Spark, making it even more attractive. However, there is not yet a Spark specify benchmark existing in the literature to guide the development and cluster deployment of Spark to better fit resource demands of user applications. In this paper, we present SparkBench, a Spark specific benchmarking suite, which includes a comprehensive set of applications. SparkBench covers four main categories of applications, including machine learning, graph computation, SQL query and streaming applications. We also characterize the resource consumption, data flow and timing information of each application and evaluate the performance impact of a key configuration parameter to guide the design and optimization of Spark data analytic platform.},
  booktitle = {Proceedings of the 12th {{ACM International Conference}} on {{Computing Frontiers}}},
  publisher = {{ACM}},
  author = {Li, Min and Tan, Jian and Wang, Yandong and Zhang, Li and Salapura, Valentina},
  year = {2015},
  keywords = {cloud computing,benchmarking,evaluation,in memory data analytics,spark},
  pages = {53:1--53:8},
  file = {/Users/benblamey/Zotero/storage/AZZWU8UG/Li et al. - 2015 - SparkBench A Comprehensive Benchmarking Suite for.pdf}
}

@article{norinderIntroducingConformalPrediction2014,
  title = {Introducing {{Conformal Prediction}} in {{Predictive Modeling}}. {{A Transparent}} and {{Flexible Alternative}} to {{Applicability Domain Determination}}},
  volume = {54},
  issn = {1549-9596, 1549-960X},
  doi = {10.1021/ci5001168},
  language = {en},
  number = {6},
  journal = {Journal of Chemical Information and Modeling},
  author = {Norinder, Ulf and Carlsson, Lars and Boyer, Scott and Eklund, Martin},
  month = jun,
  year = {2014},
  pages = {1596-1603},
  file = {/Users/benblamey/Zotero/storage/84FSKY3D/introducing CP.pdf}
}

@article{gururamPerformanceTuningEvaluation,
  title = {Performance {{Tuning}} and {{Evaluation}} of {{Iterative Algorithms}} in {{Spark}}},
  author = {Gururam, Janani},
  file = {/Users/benblamey/Zotero/storage/A5KPLK8Y/Gururam.pdf}
}

@techreport{zahariaDiscretizedStreamsFaulttolerant2012,
  title = {Discretized Streams: {{A}} Fault-Tolerant Model for Scalable Stream Processing},
  shorttitle = {Discretized Streams},
  institution = {{CALIFORNIA UNIV BERKELEY DEPT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE}},
  author = {Zaharia, Matei and Das, Tathagata and Li, Haoyuan and Hunter, Timothy and Shenker, Scott and Stoica, Ion},
  year = {2012},
  file = {/Users/benblamey/Zotero/storage/EHK6PAAE/EECS-2012-259.pdf}
}

@misc{ImmutabilityChangesEverything,
  title = {Immutability {{Changes Everything}} - {{ACM Queue}}},
  howpublished = {https://queue.acm.org/detail.cfm?id=2884038},
  file = {/Users/benblamey/Zotero/storage/7S69F7LW/CIDR15_Paper16.pdf;/Users/benblamey/Zotero/storage/UAHCYNIC/detail.html}
}

@article{angermuellerDeepLearningComputational2016,
  title = {Deep Learning for Computational Biology},
  volume = {12},
  copyright = {\textcopyright{} 2016 The Authors. Published under the terms of the CC BY 4.0 license. This is an open access article under the terms of the Creative Commons Attribution 4.0 License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.},
  issn = {1744-4292, 1744-4292},
  doi = {10.15252/msb.20156651},
  abstract = {Technological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples. This rapid increase in biological data dimension and acquisition rate is challenging conventional analysis strategies. Modern machine learning methods, such as deep learning, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions. In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging. We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights. In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology.},
  language = {en},
  number = {7},
  journal = {Molecular Systems Biology},
  author = {Angermueller, Christof and P\"arnamaa, Tanel and Parts, Leopold and Stegle, Oliver},
  month = jul,
  year = {2016},
  keywords = {cellular imaging,computational biology,deep learning,machine learning,regulatory genomics},
  pages = {878},
  file = {/Users/benblamey/Zotero/storage/S7XSS6ET/Angermueller et al. - 2016 - Deep learning for computational biology.pdf;/Users/benblamey/Zotero/storage/EXQFFNAS/878.html},
  pmid = {27474269}
}

@article{kambatlaTrendsBigData2014,
  series = {Special Issue on Perspectives on Parallel and Distributed Processing},
  title = {Trends in Big Data Analytics},
  volume = {74},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2014.01.003},
  abstract = {One of the major applications of future generation parallel and distributed systems is in big-data analytics. Data repositories for such applications currently exceed exabytes and are rapidly increasing in size. Beyond their sheer magnitude, these datasets and associated applications' considerations pose significant challenges for method and software development. Datasets are often distributed and their size and privacy considerations warrant distributed techniques. Data often resides on platforms with widely varying computational and network capabilities. Considerations of fault-tolerance, security, and access control are critical in many applications (Dean and Ghemawat, 2004; Apache hadoop). Analysis tasks often have hard deadlines, and data quality is a major concern in yet other applications. For most emerging applications, data-driven models and methods, capable of operating at scale, are as-yet unknown. Even when known methods can be scaled, validation of results is a major issue. Characteristics of hardware platforms and the software stack fundamentally impact data analytics. In this article, we provide an overview of the state-of-the-art and focus on emerging trends to highlight the hardware, software, and application landscape of big-data analytics.},
  number = {7},
  journal = {Journal of Parallel and Distributed Computing},
  author = {Kambatla, Karthik and Kollias, Giorgos and Kumar, Vipin and Grama, Ananth},
  month = jul,
  year = {2014},
  keywords = {Analytics,Big-data,Data centers,Distributed systems},
  pages = {2561-2573},
  file = {/Users/benblamey/Zotero/storage/8RZI84LQ/Kambatla et al. - 2014 - Trends in big data analytics.pdf;/Users/benblamey/Zotero/storage/XLEYBQ2N/S0743731514000057.html}
}

@article{assuncaoBigDataComputing2015,
  series = {Special Issue on Scalable Systems for Big Data Management and Analytics},
  title = {Big {{Data}} Computing and Clouds: {{Trends}} and Future Directions},
  volume = {79-80},
  issn = {0743-7315},
  shorttitle = {Big {{Data}} Computing and Clouds},
  doi = {10.1016/j.jpdc.2014.08.003},
  abstract = {This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.},
  journal = {Journal of Parallel and Distributed Computing},
  author = {Assun{\c c}\~ao, Marcos D. and Calheiros, Rodrigo N. and Bianchi, Silvia and Netto, Marco A. S. and Buyya, Rajkumar},
  month = may,
  year = {2015},
  keywords = {Cloud computing,Analytics,Big Data,Data management},
  pages = {3-15},
  file = {/Users/benblamey/Zotero/storage/TBXVJMJN/Assunção et al. - 2015 - Big Data computing and clouds Trends and future d.pdf;/Users/benblamey/Zotero/storage/G9HACMIZ/S0743731514001452.html}
}

@article{casanovaVersatileScalableAccurate2014,
  title = {Versatile, Scalable, and Accurate Simulation of Distributed Applications and Platforms},
  volume = {74},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2014.06.008},
  abstract = {The study of parallel and distributed applications and platforms, whether in the cluster, grid, peer-to-peer, volunteer, or cloud computing domain, often mandates empirical evaluation of proposed algorithmic and system solutions via simulation. Unlike direct experimentation via an application deployment on a real-world testbed, simulation enables fully repeatable and configurable experiments for arbitrary hypothetical scenarios. Two key concerns are accuracy (so that simulation results are scientifically sound) and scalability (so that simulation experiments can be fast and memory-efficient). While the scalability of a simulator is easily measured, the accuracy of many state-of-the-art simulators is largely unknown because they have not been sufficiently validated. In this work we describe recent accuracy and scalability advances made in the context of the SimGrid simulation framework. A design goal of SimGrid is that it should be versatile, i.e., applicable across all aforementioned domains. We present quantitative results that show that SimGrid compares favorably with state-of-the-art domain-specific simulators in terms of scalability, accuracy, or the trade-off between the two. An important implication is that, contrary to popular wisdom, striving for versatility in a simulator is not an impediment but instead is conducive to improving both accuracy and scalability.},
  number = {10},
  journal = {Journal of Parallel and Distributed Computing},
  author = {Casanova, Henri and Giersch, Arnaud and Legrand, Arnaud and Quinson, Martin and Suter, Fr\'ed\'eric},
  month = oct,
  year = {2014},
  keywords = {Scalability,SimGrid,Simulation,Validation,Versatility},
  pages = {2899-2917},
  file = {/Users/benblamey/Zotero/storage/W4A9YLRQ/Casanova et al. - 2014 - Versatile, scalable, and accurate simulation of di.pdf;/Users/benblamey/Zotero/storage/TFGMZ6LY/S0743731514001105.html}
}

@article{shuklaModeldrivenSchedulingDistributed2018,
  title = {Model-Driven Scheduling for Distributed Stream Processing Systems},
  volume = {117},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2018.02.003},
  abstract = {Distributed Stream Processing Systems (DSPS) are ``Fast Data'' platforms that allow streaming applications to be composed and executed with low latency on commodity clusters and Clouds. Such applications are composed as a Directed Acyclic Graph (DAG) of tasks, with data parallel execution using concurrent task threads on distributed resource slots. Scheduling such DAGs for DSPS has two parts\textemdash{}allocation of threads and resources for a DAG, and mapping threads to resources. Existing schedulers often address just one of these, make the assumption that performance linearly scales, or use ad hoc empirical tuning at runtime. Instead, we propose model-driven techniques for both mapping and allocation that rely on low-overhead a priori performance modeling of tasks. Our scheduling algorithms are able to offer predictable and low resource needs that is suitable for elastic pay-as-you-go Cloud resources, support a high input rate through high VM utilization, and can be combined with other mapping approaches as well. These are validated for micro and application benchmarks, and compared with contemporary schedulers, for the Apache Storm DSPS.},
  journal = {Journal of Parallel and Distributed Computing},
  author = {Shukla, Anshu and Simmhan, Yogesh},
  month = jul,
  year = {2018},
  keywords = {Cloud computing,Distributed systems,Big data,Performance models,Scheduling algorithms,Stream processing},
  pages = {98-114},
  file = {/Users/benblamey/Zotero/storage/XXPY4HU2/Shukla and Simmhan - 2018 - Model-driven scheduling for distributed stream pro.pdf;/Users/benblamey/Zotero/storage/DZC34HCX/S0743731518300686.html}
}

@article{shuklaReliableRapidElasticity2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.00605},
  primaryClass = {cs},
  title = {Toward {{Reliable}} and {{Rapid Elasticity}} for {{Streaming Dataflows}} on {{Clouds}}},
  abstract = {The pervasive availability of streaming data is driving interest in distributed Fast Data platforms for streaming applications. Such latency-sensitive applications need to respond to dynamism in the input rates and task behavior using scale-in and -out on elastic Cloud resources. Platforms like Apache Storm do not provide robust capabilities for responding to such dynamism and for rapid task migration across VMs. We propose several dataflow checkpoint and migration approaches that allow a running streaming dataflow to migrate, without any loss of in-flight messages or their internal tasks states, while reducing the time to recover and stabilize. We implement and evaluate these migration strategies on Apache Storm using micro and application dataflows for scaling in and out on up to 2-21 Azure VMs. Our results show that we can migrate dataflows of large sizes within 50 sec, in comparison to Storm's default approach that takes over \$100\textasciitilde{}sec\$. We also find that our approaches stabilize the application much earlier and there is no failure and re-processing of messages.},
  journal = {arXiv:1712.00605 [cs]},
  author = {Shukla, Anshu and Simmhan, Yogesh},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/Users/benblamey/Zotero/storage/MV8CP3AS/Shukla and Simmhan - 2017 - Toward Reliable and Rapid Elasticity for Streaming.pdf;/Users/benblamey/Zotero/storage/H8WKBPA2/1712.html}
}

@inproceedings{shuklaBenchmarkingDistributedStream2016,
  series = {Lecture Notes in Computer Science},
  title = {Benchmarking {{Distributed Stream Processing Platforms}} for {{IoT Applications}}},
  isbn = {978-3-319-54333-8 978-3-319-54334-5},
  doi = {10.1007/978-3-319-54334-5_7},
  abstract = {Internet of Things (IoT) is a technology paradigm where millions of sensors monitor, and help inform or manage, physical, environmental and human systems in real-time. The inherent closed-loop responsiveness and decision making of IoT applications makes them ideal candidates for using low latency and scalable stream processing platforms. Distributed Stream Processing Systems (DSPS) are becoming essential components of any IoT stack, but the efficacy and performance of contemporary DSPS have not been rigorously studied for IoT data streams and applications. Here, we develop a benchmark suite and performance metrics to evaluate DSPS for streaming IoT applications. The benchmark includes 13 common IoT tasks classified across functional categories and forming micro-benchmarks, and two IoT applications for statistical summarization and predictive analytics that leverage various dataflow patterns of DSPS. These are coupled with stream workloads from real IoT observations on smart cities. We validate the benchmark for the popular Apache Storm DSPS, and present the results.},
  language = {en},
  booktitle = {Performance {{Evaluation}} and {{Benchmarking}}. {{Traditional}} - {{Big Data}} - {{Internet}} of {{Things}}},
  publisher = {{Springer, Cham}},
  author = {Shukla, Anshu and Simmhan, Yogesh},
  month = sep,
  year = {2016},
  pages = {90-106},
  file = {/Users/benblamey/Zotero/storage/JUEUHPWR/Shukla and Simmhan - 2016 - Benchmarking Distributed Stream Processing Platfor.pdf;/Users/benblamey/Zotero/storage/R88XESKK/978-3-319-54334-5_7.html}
}

@inproceedings{toshniwalStormTwitter2014,
  title = {Storm@twitter},
  isbn = {978-1-4503-2376-5},
  doi = {10.1145/2588555.2595641},
  abstract = {This paper describes the use of Storm at Twitter. Storm is a realtime fault-tolerant and distributed stream data processing system. Storm is currently being used to run various critical computations in Twitter at scale, and in real-time. This paper describes the architecture of Storm and its methods for distributed scale-out and fault-tolerance. This paper also describes how queries (aka. topologies) are executed in Storm, and presents some operational stories based on running Storm at Twitter. We also present results from an empirical evaluation demonstrating the resilience of Storm in dealing with machine failures. Storm is under active development at Twitter and we also present some potential directions for future work.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Toshniwal, Ankit and Donham, Jake and Bhagat, Nikunj and Mittal, Sailesh and Ryaboy, Dmitriy and Taneja, Siddarth and Shukla, Amit and Ramasamy, Karthik and Patel, Jignesh M. and Kulkarni, Sanjeev and Jackson, Jason and Gade, Krishna and Fu, Maosong},
  year = {2014},
  pages = {147-156},
  file = {/Users/benblamey/Zotero/storage/NQIN8ENG/Toshniwal et al. - 2014 - Storm@twitter.pdf}
}

@article{zahariaSparkClusterComputing,
  title = {Spark: {{Cluster Computing}} with {{Working Sets}}},
  abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  pages = {7},
  file = {/Users/benblamey/Zotero/storage/B5SEDE7V/Zaharia et al. - Spark Cluster Computing with Working Sets.pdf}
}


@inproceedings{torruangwatthanaHarmonicIOScalableData2018,
  title={HarmonicIO: Scalable Data Stream Processing for Scientific Datasets},
  author={Torruangwatthana, Preechakorn and Wieslander, Hakan and Blamey, Ben and Hellander, Andreas and Toor, Salman},
  booktitle={2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year={2018},
}


@article{stonebrakerCaseSharedNothing,
  title = {The {{Case}} for {{Shared Nothing}}},
  abstract = {There are three dominent themes in building high transaction rate multiprocessor systems, namely shared memory (e.g. Synapse, IBM/AP configurations), shared disk (e.g. VAX/cluster, any multi-ported disk system), and shared nothing (e.g. Tandem, Tolerant). This paper argues that shared nothing is the preferred approach.},
  language = {en},
  author = {Stonebraker, Michael},
  pages = {5},
  file = {/Users/benblamey/Zotero/storage/9CHXJ524/Stonebraker - The Case for Shared Nothing.pdf}
}

@inproceedings{ryuExtensibleVideoProcessing2013,
  title = {Extensible {{Video Processing Framework}} in {{Apache Hadoop}}},
  isbn = {978-0-7695-5095-4},
  doi = {10.1109/CloudCom.2013.153},
  abstract = {Digital video is prominent big data spread all over the Internet. It is large not only in size but also in required processing power to extract useful information. Fast processing of excessive video reels is essential on criminal investigations, such as terrorism. This demo presents an extensible video processing framework in Apache Hadoop to parallelize video processing tasks in a cloud environment. Except for video transcoding systems, there have been few systems that can perform various video processing in cloud computing environments. The framework employs FFmpeg for a video coder, and OpenCV for a image processing engine. To optimize the performance, it exploits MapReduce implementation details to minimize video image copy. Moreover, FFmpeg source code was modified and extended, to access and exchange essential data and information with Hadoop, effectively. A face tracking system was implemented on top of the framework for the demo, which traces the continuous face movements in a sequence of video frames. Since the system provides a web-based interface, people can try the system on site. In an 8-core environment with two quad-core systems, the system shows 75\% of scalability.},
  language = {en},
  publisher = {{IEEE}},
  author = {Ryu, Chungmo and Lee, Daecheol and Jang, Minwook and Kim, Cheolgi and Seo, Euiseong},
  month = dec,
  year = {2013},
  pages = {305-310},
  file = {/Users/benblamey/Zotero/storage/X7JDDQBC/Ryu et al. - 2013 - Extensible Video Processing Framework in Apache Ha.pdf}
}

@book{dasBIGDataAnalytics,
  title = {{{BIG Data Analytics}}: {{A Framework}} for {{Unstructured Data Analysis}}},
  shorttitle = {{{BIG Data Analytics}}},
  abstract = {Abstract- Nowadays, most of information saved in companies are unstructured models. Retrieval and extraction of the information is essential works and importance in semantic web areas. Many of these requirements will be depend on the unstructured data analysis. More than 80 \% of all potentially useful business information is unstructured data, in kind of sensor readings, console logs and so on. The large number and complexity of unstructured data opens up many new possibilities for the analyst. Text mining and natural language processing are two techniques with their methods for knowledge discovery from textual context in documents. This is an approach to organize a complex unstructured data and to retrieve necessary information. The paper is to find an efficient way of storing unstructured data and appropriate approach of fetching data. Unstructured data targeted in this work to organize, is the public tweets of Twitter. Building an Big Data application that gets stream of public tweets from twitter which is latter stored in the HBase using Hadoop cluster and followed by data analysis for data retrieved from HBase by REST calls is the pragmatic approach of this project.},
  author = {Das, T. K. and Kumar, P. Mohan},
  file = {/Users/benblamey/Zotero/storage/C5FK5MKJ/Das and Kumar - BIG Data Analytics A Framework for Unstructured D.pdf;/Users/benblamey/Zotero/storage/S3VAMGN6/summary.html}
}

@article{niemenmaaHadoopBAMDirectlyManipulating2012,
  title = {Hadoop-{{BAM}}: Directly Manipulating next Generation Sequencing Data in the Cloud},
  volume = {28},
  issn = {1367-4803},
  shorttitle = {Hadoop-{{BAM}}},
  doi = {10.1093/bioinformatics/bts054},
  abstract = {Summary: Hadoop-BAM is a novel library for the scalable manipulation of aligned next-generation sequencing data in the Hadoop distributed computing framework. It acts as an integration layer between analysis applications and BAM files that are processed using Hadoop. Hadoop-BAM solves the issues related to BAM data access by presenting a convenient API for implementing map and reduce functions that can directly operate on BAM records. It builds on top of the Picard SAM JDK, so tools that rely on the Picard API are expected to be easily convertible to support large-scale distributed processing. In this article we demonstrate the use of Hadoop-BAM by building a coverage summarizing tool for the Chipster genome browser. Our results show that Hadoop offers good scalability, and one should avoid moving data in and out of Hadoop between analysis steps.Availability: Available under the open-source MIT license at http://sourceforge.net/projects/hadoop-bam/Contact:matti.niemenmaa@aalto.fiSupplementary information:Supplementary material is available at Bioinformatics online.},
  language = {en},
  number = {6},
  journal = {Bioinformatics},
  author = {Niemenmaa, Matti and Kallio, Aleksi and Schumacher, Andr\'e and Klemel\"a, Petri and Korpelainen, Eija and Heljanko, Keijo},
  month = mar,
  year = {2012},
  pages = {876-877},
  file = {/Users/benblamey/Zotero/storage/SENQNEIH/Niemenmaa et al. - 2012 - Hadoop-BAM directly manipulating next generation .pdf;/Users/benblamey/Zotero/storage/KG4WVIZW/312774.html}
}

@inproceedings{tanApproachFastParallel2014,
  title = {An Approach for Fast and Parallel Video Processing on {{Apache Hadoop}} Clusters},
  doi = {10.1109/ICME.2014.6890135},
  abstract = {This paper proposes an approach for fast and parallel video processing on MapReduce-based clusters such as Apache Hadoop. By utilizing clusters, the approach is able to handle large-scale of video data and the processing time can be significantly reduced. Technique details of performing video analysis on clusters are revealed, including method of porting typical video processing algorithms designed for a single computer to the proposed system. As case studies, face detection and motion detection and tracking algorithms have been implemented on clusters. Performance experiments on an Apache Hadoop cluster of six computers show that the system is able to reduce the running time of the two implemented algorithms to below 25\% of that of a single computer. The applications of the system include smart city video surveillance, services provided by video sites and satellite image processing.},
  booktitle = {2014 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Tan, H. and Chen, L.},
  month = jul,
  year = {2014},
  keywords = {data handling,Algorithm design and analysis,Apache Hadoop clusters,Computers,face detection,Face detection,face detection algorithm,face recognition,fast video processing,Hadoop,image motion analysis,large-scale video data handling,Libraries,MapReduce,MapReduce-based clusters,Motion detection,motion detection algorithm,motion detection and tracking,motion tracking algorithm,object detection,object tracking,parallel programming,parallel video processing,processing time reduction,Programming,public domain software,satellite image processing,smart city video surveillance,Tracking,video analysis,video processing,video signal processing,video sites,video surveillance},
  pages = {1-6},
  file = {/Users/benblamey/Zotero/storage/MZMRMXSS/Tan and Chen - 2014 - An approach for fast and parallel video processing.pdf;/Users/benblamey/Zotero/storage/63MISN5C/6890135.html}
}

@article{shirazReviewDistributedApplication2013,
  title = {A {{Review}} on {{Distributed Application Processing Frameworks}} in {{Smart Mobile Devices}} for {{Mobile Cloud Computing}}},
  volume = {15},
  issn = {1553-877X},
  doi = {10.1109/SURV.2012.111412.00045},
  abstract = {The latest developments in mobile devices technology have made smartphones as the future computing and service access devices. Users expect to run computational intensive applications on Smart Mobile Devices (SMDs) in the same way as powerful stationary computers. However in spite of all the advancements in recent years, SMDs are still low potential computing devices, which are constrained by CPU potentials, memory capacity and battery life time. Mobile Cloud Computing (MCC) is the latest practical solution for alleviating this incapacitation by extending the services and resources of computational clouds to SMDs on demand basis. In MCC, application offloading is ascertained as a software level solution for augmenting application processing capabilities of SMDs. The current offloading algorithms offload computational intensive applications to remote servers by employing different cloud models. A challenging aspect of such algorithms is the establishment of distributed application processing platform at runtime which requires additional computing resources on SMDs. This paper reviews existing Distributed Application Processing Frameworks (DAPFs) for SMDs in MCC domain. The objective is to highlight issues and challenges to existing DAPFs in developing, implementing, and executing computational intensive mobile applications within MCC domain. It proposes thematic taxonomy of current DAPFs, reviews current offloading frameworks by using thematic taxonomy and analyzes the implications and critical aspects of current offloading frameworks. Further, it investigates commonalities and deviations in such frameworks on the basis significant parameters such as offloading scope, migration granularity, partitioning approach, and migration pattern. Finally, we put forward open research issues in distributed application processing for MCC that remain to be addressed.},
  number = {3},
  journal = {IEEE Communications Surveys Tutorials},
  author = {Shiraz, M. and Gani, A. and Khokhar, R. H. and Buyya, R.},
  year = {Third 2013},
  keywords = {cloud computing,Cloud computing,Application Offloading,computational clouds resources,computational intensive applications,computational intensive mobile applications,Computational modeling,computing devices,CPU potentials,distributed application processing,distributed application processing frameworks,Distributed processing,Distributed Systems,Elastic Applications,MCC domain,memory capacity,migration pattern,mobile cloud computing,Mobile Cloud Computing,Mobile communication,mobile computing,mobile devices technology,mobile handsets,Mobile handsets,Runtime,service access devices,smart mobile devices,smart phones,software level solution,stationary computers,thematic taxonomy},
  pages = {1294-1313},
  file = {/Users/benblamey/Zotero/storage/FXPQQU8T/Shiraz et al. - 2013 - A Review on Distributed Application Processing Fra.pdf;/Users/benblamey/Zotero/storage/MXPH2D4G/6365155.html}
}

@inproceedings{liuSurveyRealtimeProcessing2014,
  title = {Survey of Real-Time Processing Systems for Big Data},
  isbn = {978-1-4503-2627-8},
  doi = {10.1145/2628194.2628251},
  abstract = {In recent years, real-time processing and analytics systems for big data\textendash{}in the context of Business Intelligence (BI)\textendash{}have received a growing attention. The traditional BI platforms that perform regular updates on daily, weekly or monthly basis are no longer adequate to satisfy the fast-changing business environments. However, due to the nature of big data, it has become a challenge to achieve the real-time capability using the traditional technologies. The recent distributed computing technology, MapReduce, provides off-the-shelf high scalability that can significantly shorten the processing time for big data; Its open-source implementation such as Hadoop has become the de-facto standard for processing big data, however, Hadoop has the limitation of supporting real-time updates. The improvements in Hadoop for the real-time capability, and the other alternative real-time frameworks have been emerging in recent years. This paper presents a survey of the open source technologies that support big data processing in a real-time/near real-time fashion, including their system architectures and platforms.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Liu, Xiufeng and Iftikhar, Nadeem and Xie, Xike},
  year = {2014},
  pages = {356-361},
  file = {/Users/benblamey/Zotero/storage/VIR7RYB4/Liu et al. - 2014 - Survey of real-time processing systems for big dat.pdf}
}

@misc{slimbaltagiFlinkVsSpark06:52:05UTC,
  type = {Data \& {{Analytics}}},
  title = {Flink vs. {{Spark}}},
  abstract = {Flink vs. Spark: this is the slide deck of my talk at the 2015 Flink Forward},
  author = {Slim Baltagi},
  year = {06:52:05 UTC}
}

@article{awanArchitecturalImpactPerformance2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1604.08484},
  primaryClass = {cs},
  title = {Architectural {{Impact}} on {{Performance}} of {{In}}-Memory {{Data Analytics}}: {{Apache Spark Case Study}}},
  shorttitle = {Architectural {{Impact}} on {{Performance}} of {{In}}-Memory {{Data Analytics}}},
  abstract = {While cluster computing frameworks are continuously evolving to provide real-time data analysis capabilities, Apache Spark has managed to be at the forefront of big data analytics for being a unified framework for both, batch and stream data processing. However, recent studies on micro-architectural characterization of in-memory data analytics are limited to only batch processing workloads. We compare micro-architectural performance of batch processing and stream processing workloads in Apache Spark using hardware performance counters on a dual socket server. In our evaluation experiments, we have found that batch processing are stream processing workloads have similar micro-architectural characteristics and are bounded by the latency of frequent data access to DRAM. For data accesses we have found that simultaneous multi-threading is effective in hiding the data latencies. We have also observed that (i) data locality on NUMA nodes can improve the performance by 10\% on average and(ii) disabling next-line L1-D prefetchers can reduce the execution time by up-to 14$\backslash$\% and (iii) multiple small executors can provide up-to 36$\backslash$\% speedup over single large executor.},
  journal = {arXiv:1604.08484 [cs]},
  author = {Awan, Ahsan Javed and Brorsson, Mats and Vlassov, Vladimir and Ayguade, Eduard},
  month = apr,
  year = {2016},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Hardware Architecture,Computer Science - Performance},
  file = {/Users/benblamey/Zotero/storage/M3G48RHN/Awan et al. - 2016 - Architectural Impact on Performance of In-memory D.pdf;/Users/benblamey/Zotero/storage/MIZWYGW9/1604.html}
}

@inproceedings{chenGOVERNORSmootherStream2017,
  title = {{{GOVERNOR}}: {{Smoother Stream Processing Through Smarter Backpressure}}},
  shorttitle = {{{GOVERNOR}}},
  doi = {10.1109/ICAC.2017.31},
  abstract = {Distributed micro-batch streaming systems, such as Spark Streaming, employ backpressure mechanisms to maintain a stable, high throughput stream of results that is robust to runtime dynamics. Checkpointing in stream processing systems is a process that creates periodic snapshots of the data flow for fault tolerance. These checkpoints can be expensive to produce and add significant delay to the data processing. The checkpointing latencies are also variable at runtime, which in turn compounds the challenges for the backpressure mechanism to maintain stable performance. Consequently, the interferences caused by the checkpointing may degrade system performance significantly, even leading to exhaustion of resources or system crash.This paper describes GOVERNOR, a controller that factors the checkpointing costs into the backpressure mechanism. It not only guarantees a smooth execution of the stream processing but also reduces the throughput loss caused by interferences of the checkpointing. Our experimental results on four stateful streaming operators with real-world data sources demonstrate that Governor implemented in Spark Streaming can achieve 26\% throughput improvement, and lower the risk of system crash, with negligible overhead.},
  booktitle = {2017 {{IEEE International Conference}} on {{Autonomic Computing}} ({{ICAC}})},
  author = {Chen, X. and Vigfusson, Y. and Blough, D. M. and Zheng, F. and Wu, K. L. and Hu, L.},
  month = jul,
  year = {2017},
  keywords = {checkpointing,Checkpointing,Computer crashes,Delays,distributed micro-batch streaming systems,Governor,Process control,smarter backpressure mechanism,smoother stream processing,Sparks,Throughput},
  pages = {145-154},
  file = {/Users/benblamey/Zotero/storage/57YQWZ93/8005343.html}
}

@misc{MakingApacheSpark2017,
  title = {Making {{Apache Spark}} the {{Fastest Open Source Streaming Engine}} - {{The Databricks Blog}}},
  abstract = {Structured Streaming is not only the the simplest streaming engine, but for many workloads it is the fastest. By leveraging all of the work done on the Catalyst query optimizer and the Tungsten execution engine, Structured Streaming brings the power of Spark SQL to real-time streaming. Our benchmarks showed 5x or better throughput than other popular streaming engines when running the Yahoo Streaming Benchmark.},
  language = {en-US},
  month = jun,
  year = {2017},
  file = {/Users/benblamey/Zotero/storage/WLJV6JIK/simple-super-fast-streaming-engine-apache-spark.html}
}

@misc{grierExtendingYahooStreaming2016,
  title = {Extending the {{Yahoo}}! {{Streaming Benchmark}}},
  abstract = {Update December 18, 2017:~Nearly 2 years after this initial post, we discussed the Yahoo streaming benchmark in another blog post where we cover some of the issues we see with modern benchmarking methods.~Until very recently, I've been working at Twitter and focusing primarily on stream processing systems. While researching theRead more},
  language = {en-US},
  journal = {Data Artisans},
  author = {Grier, Jamie},
  month = feb,
  year = {2016},
  keywords = {benchmark},
  file = {/Users/benblamey/Zotero/storage/FRZ4YTN8/extending-the-yahoo-streaming-benchmark.html}
}

@misc{xinApacheSparkFastest2014,
  title = {Apache {{Spark}} the Fastest Open Source Engine for Sorting a Petabyte},
  abstract = {Update November 5, 2014: Our benchmark entry has been reviewed by the benchmark committee and Apache Spark has won the Daytona GraySort contest for 2014! Please see this new blog post for update. Apache Spark has seen phenomenal adoption, being widely slated as the successor to Hadoop MapReduce, and being deployed in clusters from a \ldots{}},
  language = {en-US},
  journal = {Databricks},
  author = {Xin, Reynold},
  month = oct,
  year = {2014},
  file = {/Users/benblamey/Zotero/storage/VLCRFI5A/spark-petabyte-sort.html}
}

@inproceedings{qianBenchmarkingModernDistributed2016,
  title = {Benchmarking Modern Distributed Streaming Platforms},
  doi = {10.1109/ICIT.2016.7474816},
  abstract = {The prevalence of big data technology has generated increasing demands in large-scale streaming data processing. However, for certain tasks it is still challenging to appropriately select a platform due to the diversity of choices and the complexity of configurations. This paper focuses on benchmarking some principal streaming platforms. We achieve our goals on StreamBench, a streaming benchmark tool based on which we introduce proper modifications and extensions. We then accomplish performance comparisons among different big data platforms, including Apache Spark, Apache Storm and Apache Samza. In terms of performance criteria, we consider both computational capability and fault-tolerance ability. Finally, we give a summary on some key knobs for performance tuning as well as on hardware utilization.},
  booktitle = {2016 {{IEEE International Conference}} on {{Industrial Technology}} ({{ICIT}})},
  author = {Qian, S. and Wu, G. and Huang, J. and Das, T.},
  month = mar,
  year = {2016},
  keywords = {Big Data,Sparks,Throughput,Apache Samza,Apache Spark,Apache Storm,benchmark,Benchmark testing,big data,Big Data technology,distributed processing,distributed streaming computing,distributed streaming platforms,Fault tolerance,Fault tolerant systems,Hardware,large-scale streaming data processing,spark streaming,storm,Storms,StreamBench tool},
  file = {/Users/benblamey/Zotero/storage/37FL23LU/Qian et al. - 2016 - Benchmarking modern distributed streaming platform.pdf;/Users/benblamey/Zotero/storage/AWV6AKIE/7474816.html}
}

@article{carboneApacheFlinkStream2015,
  title = {Apache {{Flink}}\texttrademark{}: {{Stream}} and {{Batch Processing}} in a {{Single Engine}}},
  volume = {36},
  abstract = {Apache Flink1 is an open-source system for processing streaming and batch data. Flink is built on the philosophy that many classes of data processing applications, including real-time analytics, continuous data pipelines, historic data processing (batch), and iterative algorithms (machine learning, graph analysis) can be expressed and executed as pipelined fault-tolerant dataflows. In this paper, we present Flink's architecture and expand on how a (seemingly diverse) set of use cases can be unified under a single execution model.},
  language = {en},
  number = {4},
  journal = {Bulletin of the IEEE Computer Society Technical Committee on Data Engineering},
  author = {Carbone, Paris and Katsifodimos, Asterios and Ewen, Stephan and Markl, Volker and Haridi, Seif and Tzoumas, Kostas},
  year = {2015},
  keywords = {benchmarking},
  file = {/Users/benblamey/Zotero/storage/LESN8LV5/Carbone et al. - Apache Flink™ Stream and Batch Processing in a Si.pdf}
}

@article{zahariaApacheSparkUnified2016,
  title = {Apache {{Spark}}: {{A Unified Engine}} for {{Big Data Processing}}},
  volume = {59},
  issn = {0001-0782},
  shorttitle = {Apache {{Spark}}},
  doi = {10.1145/2934664},
  abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.},
  number = {11},
  journal = {Commun. ACM},
  author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
  month = oct,
  year = {2016},
  pages = {56--65},
  file = {/Users/benblamey/Zotero/storage/RSUAX7ZP/Zaharia et al. - 2016 - Apache Spark A Unified Engine for Big Data Proces.pdf}
}

@misc{apacheflumeApacheFlume2016,
  howpublished = {https://flume.apache.org/},
  author = {{Apache Flume}},
  year = {2016},
  file = {/Users/benblamey/Zotero/storage/3EJNM7ZR/flume.apache.org.html}
}

@misc{ApacheHadoop2011,
  author = {{Apache Hadoop}},
  howpublished = {http://hadoop.apache.org/},
  year = {2011},
  file = {/Users/benblamey/Zotero/storage/G7T9TYXP/hadoop.apache.org.html}
}

@misc{apachesparkShuffleOperationsRDD,
  title = {Shuffle {{Operations}} - {{RDD Programming Guide}} - {{Spark}} 2.3.1 {{Documentation}}},
  howpublished = {https://spark.apache.org/docs/latest/rdd-programming-guide.html\#shuffle-operations},
  author = {\{Apache Spark\}},
  file = {/Users/benblamey/Zotero/storage/LP8GJ5LV/rdd-programming-guide.html}
}

@misc{MNISTHandwrittenDigit,
  title = {{{MNIST}} Handwritten Digit Database, {{Yann LeCun}}, {{Corinna Cortes}} and {{Chris Burges}}},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  file = {/Users/benblamey/Zotero/storage/YJ9HIA77/mnist.html}
}

@book{Junqueira:2013:ZDP:2904421,
  edition = {1st},
  title = {{{ZooKeeper}}: {{Distributed Process Coordination}}},
  isbn = {1-4493-6130-7 978-1-4493-6130-3},
  publisher = {{O'Reilly Media, Inc.}},
  author = {Junqueira, Flavio and Reed, Benjamin},
  year = {2013}
}

@incollection{CellMethods,
  series = {Methods in Cell Biology},
  title = {Chapter 5 - {{Fluorescence Live Cell Imaging}}},
  volume = {123},
  booktitle = {Quantitative {{Imaging}} in {{Cell Biology}}},
  publisher = {{Academic Press}},
  author = {Ettinger, Andreas and Wittmann, Torsten},
  editor = {Waters, Jennifer C. and Wittman, Torsten},
  year = {2014},
  pages = {77-94},
  doi = {https://doi.org/10.1016/B978-0-12-420138-5.00005-7\%0020},
  issn = {0091-679X}
}

@article{LNP,
  title = {Optimization of {{Lipid Nanoparticle Formulations}} for {{mRNA Delivery}} in {{Vivo}} with {{Fractional Factorial}} and {{Definitive Screening Designs}}},
  volume = {15},
  doi = {10.1021/acs.nanolett.5b02497},
  number = {11},
  journal = {Nano Letters},
  author = {Kauffman, Kevin J. and Dorkin, J. Robert and Yang, Jung H. and Heartlein, Michael W. and DeRosa, Frank and Mir, Faryal F. and Fenton, Owen S. and Anderson, Daniel G.},
  year = {2015},
  pages = {7300-7306},
  eprint = {http://dx.doi.org/10.1021/acs.nanolett.5b02497},
  pmid = {26469188}
}

@inproceedings{abadi2005design,
  address = {Asilomar, CA},
  title = {The {{Design}} of the {{Borealis Stream Processing Engine}}},
  booktitle = {Second {{Biennial Conference}} on {{Innovative Data Systems Research}} ({{CIDR}} 2005)},
  author = {Abadi, Daniel J and Ahmad, Yanif and Balazinska, Magdalena and Cetintemel, Ugur and Cherniack, Mitch and Hwang, Jeong-Hyon and Lindner, Wolfgang and Maskey, Anurag S and Rasin, Alexander and Ryvkina, Esther and Tatbul, Nesime and Xing, Ying and Zdonik, Stan},
  month = jan,
  year = {2005}
}

@incollection{gama2007data,
  title = {Data Stream Processing},
  booktitle = {Learning from {{Data Streams}}},
  publisher = {{Springer}},
  author = {Gama, Jo\~ao and Rodrigues, Pedro Pereira},
  year = {2007},
  pages = {25-39}
}

@inproceedings{kreps2011kafka,
  title = {Kafka: {{A}} Distributed Messaging System for Log Processing},
  booktitle = {Proceedings of the {{NetDB}}},
  author = {Kreps, Jay and Narkhede, Neha and Rao, Jun and {others}},
  year = {2011},
  pages = {1-7}
}

@inproceedings{ssc,
  title = {{{SNIC Science Cloud}} ({{SSC}}): {{A National}}-{{Scale Cloud Infrastructure}} for {{Swedish Academia}}},
  booktitle = {E-{{Science}} (e-{{Science}}), 2017 {{IEEE}} 13th {{International Conference}} On},
  author = {Toor, Salman and Lindberg, Mathias and Falman, Ingemar and Vallin, Andreas and Mohill, Olof and Freyhult, Pontus and Nilsson, Linus and Agback, Martin and Viklund, Lars and Zazzik, Henric and {others}},
  year = {2017},
  pages = {219-227},
  organization = {{IEEE}}
}

@inproceedings{Spring07streamflex:high-throughput,
  title = {{{StreamFlex}}: High-Throughput Stream Programming in Java},
  booktitle = {In {{OOPSLA}}},
  publisher = {{ACM}},
  author = {Spring, Jesper H. and Polytechnique, Ecole and Lausanne, F\'ed\'erale De and Privat, Jean},
  year = {2007},
  pages = {211-228}
}

@inproceedings{Zaharia:2012:RDD:2228298.2228301,
  address = {San Jose, CA},
  series = {NSDI'12},
  title = {Resilient {{Distributed Datasets}}: {{A Fault}}-Tolerant {{Abstraction}} for {{In}}-Memory {{Cluster Computing}}},
  booktitle = {Proceedings of the 9th {{USENIX Conference}} on {{Networked Systems Design}} and {{Implementation}}},
  publisher = {{USENIX Association}},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauley, Murphy and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
  year = {2012},
  pages = {2-2},
  numpages = {1},
  acmid = {2228301}
}

@inproceedings{Heinze:2014:CDS:2611286.2611309,
  address = {Mumbai, India},
  series = {DEBS '14},
  title = {Cloud-Based {{Data Stream Processing}}},
  isbn = {978-1-4503-2737-4},
  doi = {10.1145/2611286.2611309},
  booktitle = {Proceedings of the 8th {{ACM International Conference}} on {{Distributed Event}}-{{Based Systems}}},
  publisher = {{ACM}},
  author = {Heinze, Thomas and Aniello, Leonardo and Querzoni, Leonardo and Jerzak, Zbigniew},
  year = {2014},
  keywords = {cloud-based data stream processing,fault tolerance,load balancing},
  pages = {238-245},
  numpages = {8},
  acmid = {2611309}
}

@inproceedings{7384369,
  title = {Conceptual {{Survey}} on {{Data Stream Processing Systems}}},
  doi = {10.1109/ICPADS.2015.106},
  booktitle = {2015 {{IEEE}} 21st {{International Conference}} on {{Parallel}} and {{Distributed Systems}} ({{ICPADS}})},
  author = {Hesse, G. and Lorenz, M.},
  month = dec,
  year = {2015},
  keywords = {data handling,Sparks,Throughput,Storms,data stream processing systems,Flink,Internet of Things,Java,Market research,Samza,Spark,Storm,Stream Processing,system components,Topology},
  pages = {797-802}
}

@incollection{Nguyen2013,
  address = {Cham},
  title = {Demand-{{Based Scheduling Priorities}} for {{Performance Optimization}} of {{Stream Programs}} on {{Parallel Platforms}}},
  isbn = {978-3-319-03859-9},
  booktitle = {Algorithms and {{Architectures}} for {{Parallel Processing}}: 13th {{International Conference}}, {{ICA3PP}} 2013, {{Vietri}} Sul {{Mare}}, {{Italy}}, {{December}} 18-20, 2013, {{Proceedings}}, {{Part I}}},
  publisher = {{Springer International Publishing}},
  author = {Nguyen, Vu Thien Nga and Kirner, Raimund},
  editor = {Ko\l{}odziej, Joanna and Di Martino, Beniamino and Talia, Domenico and Xiong, Kaiqi},
  year = {2013},
  pages = {357-369},
  doi = {10.1007/978-3-319-03859-9_31}
}

@article{docker,
  title = {Docker: {{Lightweight Linux Containers}} for {{Consistent Development}} and {{Deployment}}},
  volume = {2014},
  issn = {1075-3583},
  number = {239},
  journal = {Linux J.},
  author = {Merkel, Dirk},
  month = mar,
  year = {2014},
  publisher = {{Belltown Media}},
  location = {Houston, TX},
  issue_date = {March 2014},
  articleno = {2},
  acmid = {2600241}
}

@inproceedings{bonomiFogComputingIts2012,
  address = {New York, NY, USA},
  series = {MCC '12},
  title = {Fog {{Computing}} and {{Its Role}} in the {{Internet}} of {{Things}}},
  isbn = {978-1-4503-1519-7},
  doi = {10.1145/2342509.2342513},
  abstract = {Fog Computing extends the Cloud Computing paradigm to the edge of the network, thus enabling a new breed of applications and services. Defining characteristics of the Fog are: a) Low latency and location awareness; b) Wide-spread geographical distribution; c) Mobility; d) Very large number of nodes, e) Predominant role of wireless access, f) Strong presence of streaming and real time applications, g) Heterogeneity. In this paper we argue that the above characteristics make the Fog the appropriate platform for a number of critical Internet of Things (IoT) services and applications, namely, Connected Vehicle, Smart Grid, Smart Cities, and, in general, Wireless Sensors and Actuators Networks (WSANs).},
  booktitle = {Proceedings of the {{First Edition}} of the {{MCC Workshop}} on {{Mobile Cloud Computing}}},
  publisher = {{ACM}},
  author = {Bonomi, Flavio and Milito, Rodolfo and Zhu, Jiang and Addepalli, Sateesh},
  year = {2012},
  keywords = {cloud computing,analytics,fog computing,iot,real time systems,software defined networks,wsan},
  pages = {13--16}
}

@inproceedings{marcuSparkFlinkUnderstanding2016,
  title = {Spark {{Versus Flink}}: {{Understanding Performance}} in {{Big Data Analytics Frameworks}}},
  shorttitle = {Spark {{Versus Flink}}},
  doi = {10.1109/CLUSTER.2016.22},
  abstract = {Big Data analytics has recently gained increasing popularity as a tool to process large amounts of data on-demand. Spark and Flink are two Apache-hosted data analytics frameworks that facilitate the development of multi-step data pipelines using directly acyclic graph patterns. Making the most out of these frameworks is challenging because efficient executions strongly rely on complex parameter configurations and on an in-depth understanding of the underlying architectural choices. Although extensive research has been devoted to improving and evaluating the performance of such analytics frameworks, most of them benchmark the platforms against Hadoop, as a baseline, a rather unfair comparison considering the fundamentally different design principles. This paper aims to bring some justice in this respect, by directly evaluating the performance of Spark and Flink. Our goal is to identify and explain the impact of the different architectural choices and the parameter configurations on the perceived end-to-end performance. To this end, we develop a methodology for correlating the parameter settings and the operators execution plan with the resource usage. We use this methodology to dissect the performance of Spark and Flink with several representative batch and iterative workloads on up to 100 nodes. Our key finding is that there none of the two framework outperforms the other for all data types, sizes and job patterns. This paper performs a fine characterization of the cases when each framework is superior, and we highlight how this performance correlates to operators, to resource usage and to the specifics of the internal framework design.},
  booktitle = {2016 {{IEEE International Conference}} on {{Cluster Computing}}},
  author = {Marcu, O. C. and Costan, A. and Antoniu, G. and {P\'erez-Hern\'andez}, M. S.},
  month = sep,
  year = {2016},
  keywords = {Big Data,Big data,Hadoop,Libraries,Sparks,Benchmark testing,Flink,Spark,Apache-hosted data analytics,big data analytics,Data analysis,data on-demand,directed graphs,directly acyclic graph patterns,Iterative methods,multistep data pipelines,Optimization,performance evaluation,software performance evaluation},
  file = {/Users/benblamey/Zotero/storage/CFL7SASA/Marcu et al. - 2016 - Spark Versus Flink Understanding Performance in B.pdf;/Users/benblamey/Zotero/storage/J6L3LX75/7776539.html}
}

@misc{vironovaabMiniTEMAnalysisPurity2018,
  title = {{MiniTEM analysis of purity and integrity of virus particles}},
  abstract = {MiniTEM is the innovation that puts the power of TEM technology in your hands Rapid nanoparticle characterization Meaningful data on particle morphology, size distribution and purity Easy to use and place Developed specifically for nanoparticle characterization Resolution of 1 nm Accelerating voltage 25kV The system can be plugged into a standard wall socket and can \ldots{}},
  language = {sv-SE},
  journal = {Vironova},
  author = {{Vironova AB}},
  year = {2018},
  file = {/Users/benblamey/Zotero/storage/ZJBB77IA/minitem.html}
}

@article{schindelinFijiOpenSource2012,
  title = {Fiji - an {{Open Source}} Platform for Biological Image Analysis},
  volume = {9},
  issn = {1548-7091},
  doi = {10.1038/nmeth.2019},
  abstract = {Fiji is a distribution of the popular Open Source software ImageJ focused on biological image analysis. Fiji uses modern software engineering practices to combine powerful software libraries with a broad range of scripting languages to enable rapid prototyping of image processing algorithms. Fiji facilitates the transformation of novel algorithms into ImageJ plugins that can be shared with end users through an integrated update system. We propose Fiji as a platform for productive collaboration between computer science and biology research communities.},
  number = {7},
  journal = {Nature methods},
  author = {Schindelin, Johannes and {Arganda-Carreras}, Ignacio and Frise, Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan and Schmid, Benjamin and Tinevez, Jean-Yves and White, Daniel James and Hartenstein, Volker and Eliceiri, Kevin and Tomancak, Pavel and Cardona, Albert},
  month = jun,
  year = {2012},
  file = {/Users/benblamey/Zotero/storage/2P38XAPQ/Schindelin et al. - 2012 - Fiji - an Open Source platform for biological imag.pdf},
  pmid = {22743772},
  pmcid = {PMC3855844}
}

@article{wollmanHighThroughputMicroscopy2007,
  title = {High Throughput Microscopy: From Raw Images to Discoveries},
  volume = {120},
  copyright = {\textcopyright{} The Company of Biologists Limited 2007},
  issn = {0021-9533, 1477-9137},
  shorttitle = {High Throughput Microscopy},
  doi = {10.1242/jcs.013623},
  abstract = {Skip to Next Section
Technological advances in automated microscopy now allow rapid acquisition of many images without human intervention, images that can be used for large-scale screens. The main challenge in such screens is the conversion of the raw images into interpretable information and hence discoveries. This post-acquisition component of image-based screens requires computational steps to identify cells, choose the cells of interest, assess their phenotype, and identify statistically significant ‘hits'. Designing such an analysis pipeline requires careful consideration of the necessary hardware and software components, image analysis, statistical analysis and data presentation tools. Given the increasing availability of such hardware and software, these types of experiments have come within the reach of individual labs, heralding many interesting new ways of acquiring biological knowledge.},
  language = {en},
  number = {21},
  journal = {Journal of Cell Science},
  author = {Wollman, Roy and Stuurman, Nico},
  month = nov,
  year = {2007},
  keywords = {Genome-wide screen,High-throughput microscopy (HTM),Image analysis,RNAi},
  pages = {3715-3722},
  file = {/Users/benblamey/Zotero/storage/QPRJ5HK3/Wollman and Stuurman - 2007 - High throughput microscopy from raw images to dis.pdf;/Users/benblamey/Zotero/storage/PTVK4DTS/3715.html},
  pmid = {17959627}
}

@techreport{pagePageRankCitationRanking1999,
  title = {The {{PageRank}} Citation Ranking: {{Bringing}} Order to the Web.},
  shorttitle = {The {{PageRank}} Citation Ranking},
  institution = {{Stanford InfoLab}},
  author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
  year = {1999},
  file = {/Users/benblamey/Zotero/storage/WMZLWLIT/Page et al. - 1999 - The PageRank citation ranking Bringing order to t.pdf;/Users/benblamey/Zotero/storage/ZAELADDL/1999-66.pdf}
}

@misc{salvatoresanfilippoRedis2009,
  title = {Redis},
  howpublished = {https://redis.io/},
  author = {{Salvatore Sanfilippo}},
  year = {2009},
  file = {/Users/benblamey/Zotero/storage/J5BB8327/redis.io.html}
}

@inproceedings{liaoEnforcementRealTime2015,
  title = {An Enforcement of Real Time Scheduling in {{Spark Streaming}}},
  doi = {10.1109/IGCC.2015.7393730},
  abstract = {With the exponential growth in continuous data streams, real time streaming processing has been gaining a lot of popularity. Spark Streaming is one of the open source frameworks for reliable, high-throughput and low latency stream processing. Though it is a near real time stream processing framework running on commodity hardware, real time event processing is not guaranteed in its scheduling system. Profiling results indicate that the total delay time of events with unstable inputs is more volatile and presents big fluctuations. In this paper, we propose a simple, yet effective scheduling strategy to reduce the worst case event processing time by dynamic adjusting the time window of batch intervals. It is a real time enhancement to Spark Streaming based on Spark's framework. The proposed strategy is evaluated using two streaming benchmarks and our preliminary results demonstrate the feasibility of our approach with unstable event streams.},
  booktitle = {2015 {{Sixth International Green}} and {{Sustainable Computing Conference}} ({{IGSC}})},
  author = {Liao, Xinyi and Gao, Zhiwei and Ji, Weixing and Wang, Yizhuo},
  month = dec,
  year = {2015},
  keywords = {public domain software,Delays,Sparks,big data,batch intervals,commodity hardware,continuous data streams,Data processing,dynamic adjusting,effective scheduling strategy,low latency stream processing,media streaming,open source frameworks,Processor scheduling,real time enhancement,real time event processing,real time scheduling,real time scheduling system,real time stream processing framework,real time streaming processing,Real-time systems,Reliability,scheduling,Scheduling,Spark framework,Spark Streaming,streaming benchmarks,streaming processing,time window,unstable event streams,worst case event processing time},
  pages = {1-6},
  file = {/Users/benblamey/Zotero/storage/ZX8RI85S/7393730.html}
}

@misc{pointerThingsWeHate2015,
  title = {5 Things We Hate about {{Spark}}},
  abstract = {Spark has dethroned MapReduce and changed big data forever, but that rapid ascent has been accompanied by persistent frustrations},
  language = {en},
  journal = {InfoWorld},
  howpublished = {https://www.infoworld.com/article/3004460},
  author = {Pointer, Ian},
  month = nov,
  year = {2015},
  file = {/Users/benblamey/Zotero/storage/U4ZX69VG/5-things-we-hate-about-spark.html}
}

@article{CV7000SDISCONTINUEDYokogawa,
  title = {{{CV7000S}} - {{Yokogawa Electric Corporation}}},
  abstract = {Cell Voyager 7000S (CV7000S) is a high content screening system with our original confocal scanner unit, a live cell stage incubator and a build-in liquid handler which enable long term live cell imaging and rapid kinetic analysis. CV7000S is the most powerful microscopic system for drug discovery based on phenotypic screening as well as life science researches.},
  author = {{Yokogawa Electric Corporation}},
  year = {2016}
}

@mastersthesis{lugnegaard2018building,
  title = {Building a High Throughput Microscope Simulator Using the {{Apache Kafka}} Streaming Framework},
  language = {EN},
  school = {Uppsala University},
  author = {Lugneg\aa{}rd, Lovisa},
  year = {2018},
  file = {/Users/benblamey/Zotero/storage/C75QB4R3/Lugnegård - 2018 - Building a high throughput microscope simulator us.pdf}
}

@article{depardonAnalysisSixDistributed,
  title = {Analysis of {{Six Distributed File Systems}}},
  author = {Depardon, Benjamin and Mahec, Ga\"el Le and S\'eguin, Cyril},
  pages = {45},
  file = {/Users/benblamey/Zotero/storage/MK5RJCX5/Depardon et al. - Analysis of Six Distributed File Systems.pdf}
}

@article{schottEmbryoMinerNewFramework2018,
  title = {{{EmbryoMiner}}: {{A}} New Framework for Interactive Knowledge Discovery in Large-Scale Cell Tracking Data of Developing Embryos},
  volume = {14},
  issn = {1553-7358},
  shorttitle = {{{EmbryoMiner}}},
  doi = {10.1371/journal.pcbi.1006128},
  abstract = {State-of-the-art light-sheet and confocal microscopes allow recording of entire embryos in 3D and over time (3D+t) for many hours. Fluorescently labeled structures can be segmented and tracked automatically in these terabyte-scale 3D+t images, resulting in thousands of cell migration trajectories that provide detailed insights to large-scale tissue reorganization at the cellular level. Here we present EmbryoMiner, a new interactive open-source framework suitable for in-depth analyses and comparisons of entire embryos, including an extensive set of trajectory features. Starting at the whole-embryo level, the framework can be used to iteratively focus on a region of interest within the embryo, to investigate and test specific trajectory-based hypotheses and to extract quantitative features from the isolated trajectories. Thus, the new framework provides a valuable new way to quantitatively compare corresponding anatomical regions in different embryos that were manually selected based on biological prior knowledge. As a proof of concept, we analyzed 3D+t light-sheet microscopy images of zebrafish embryos, showcasing potential user applications that can be performed using the new framework.},
  language = {en},
  number = {4},
  journal = {PLOS Computational Biology},
  author = {Schott, Benjamin and Traub, Manuel and Schlagenhauf, Cornelia and Takamiya, Masanari and Antritter, Thomas and Bartschat, Andreas and L\"offler, Katharina and Blessing, Denis and Otte, Jens C. and Kobitski, Andrei Y. and Nienhaus, G. Ulrich and Str\"ahle, Uwe and Mikut, Ralf and Stegmaier, Johannes},
  month = apr,
  year = {2018},
  keywords = {Data mining,Data visualization,Embryos,Epithelium,Fluorescence imaging,Neural crest,Vision,Zebrafish},
  pages = {e1006128},
  file = {/Users/benblamey/Zotero/storage/B4VX7VE3/article.pdf;/Users/benblamey/Zotero/storage/WG6C7CQW/Schott et al. - 2018 - EmbryoMiner A new framework for interactive knowl.pdf}
}

@article{janhunenSATSMTAnswer2012,
  title = {{{SAT}} and {{SMT}} for {{Answer Set Programming}}},
  language = {en},
  author = {Janhunen, Tomi},
  year = {2012},
  pages = {70},
  file = {/Users/benblamey/Zotero/storage/67VH4K5Y/Janhunen - 2012 - SAT and SMT for Answer Set Programming.pdf}
}

@misc{TaleThreeApache2016,
  title = {A {{Tale}} of {{Three Apache Spark APIs}}: {{RDDs}}, {{DataFrames}}, and {{Datasets}}},
  shorttitle = {A {{Tale}} of {{Three Apache Spark APIs}}},
  abstract = {In summation, the choice of when to use RDD or DataFrame and/or Dataset seems obvious. While the former offers you low-level functionality and control, the latter allows custom view and structure, offers high-level and domain specific operations, saves space, and executes at superior speeds.},
  language = {en-US},
  journal = {Databricks},
  month = jul,
  year = {2016},
  file = {/Users/benblamey/Zotero/storage/TMLJ839M/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html}
}

@book{usenixassociationProceedingsGeneralTrack2004,
  address = {Berkeley, Calif},
  title = {Proceedings of the General Track, 2004 {{USENIX}} Annual Technical Conference: {{June}} 30 - {{July}} 02, 2004, {{Boston}}, {{MA}}, {{USA}}},
  isbn = {978-1-931971-21-8},
  shorttitle = {Proceedings of the General Track, 2004 {{USENIX}} Annual Technical Conference},
  abstract = {There has been much research devoted to improving the performance of data analytics frameworks, but comparatively little effort has been spent systematically identifying the performance bottlenecks of these systems. In this paper, we develop blocked time analysis, a methodology for quantifying performance bottlenecks in distributed computation frameworks, and use it to analyze the Spark framework's performance on two SQL benchmarks and a production workload. Contrary to our expectations, we find that (i) CPU (and not I/O) is often the bottleneck, (ii) improving network performance can improve job completion time by a median of at most 2\%, and (iii) the causes of most stragglers can be identified.},
  language = {en},
  publisher = {{USENIX Assiation}},
  editor = {USENIX Association},
  year = {2004},
  file = {/Users/benblamey/Zotero/storage/8KE6FBIW/USENIX Association - 2004 - Proceedings of the general track, 2004 USENIX annu.pdf},
  note = {OCLC: 254252016}
}

@article{ramaneExperimentalEvaluationPerformance2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.1931},
  primaryClass = {cs},
  title = {An {{Experimental Evaluation}} of {{Performance}} of {{A Hadoop Cluster}} on {{Replica Management}}},
  abstract = {Hadoop is an open source implementation of the MapReduce Framework in the realm of distributed processing. A Hadoop cluster is a unique type of computational cluster designed for storing and analyzing large data sets across cluster of workstations. To handle massive scale data, Hadoop exploits the Hadoop Distributed File System termed as HDFS. The HDFS similar to most distributed file systems share a familiar problem on data sharing and availability among compute nodes, often which leads to decrease in performance. This paper is an experimental evaluation of Hadoop's computing performance which is made by designing a rack aware cluster that utilizes the Hadoop's default block placement policy to improve data availability. Additionally, an adaptive data replication scheme that relies on access count prediction using Langrange's interpolation is adapted to fit the scenario. To prove, experiments were conducted on a rack aware cluster setup which significantly reduced the task completion time, but once the volume of the data being processed increases there is a considerable cutback in computational speeds due to update cost. Further the threshold level for balance between the update cost and replication factor is identified and presented graphically.},
  journal = {arXiv:1411.1931 [cs]},
  author = {Ramane, Muralikrishnan and Krishnamoorthy, Sharmila and Gowtham, Sasikala},
  month = nov,
  year = {2014},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/Users/benblamey/Zotero/storage/TUEJQ3Q9/Ramane et al. - 2014 - An Experimental Evaluation of Performance of A Had.pdf;/Users/benblamey/Zotero/storage/3765QXT7/1411.html}
}

@article{khanliPHFSDynamicReplication2011,
  title = {{{PHFS}}: {{A}} Dynamic Replication Method, to Decrease Access Latency in the Multi-Tier Data Grid},
  volume = {27},
  issn = {0167-739X},
  shorttitle = {{{PHFS}}},
  doi = {10.1016/j.future.2010.08.013},
  abstract = {Data replication is a method to improve the performance of data access in distributed systems. Dynamic replication is a kind of replication that adapts replication configuration with the change of users' behavior during the time to ensure the benefits of replication. In this paper, we propose a new dynamic replication method in a multi-tier data grid called predictive hierarchical fast spread (PHFS) which is an extended version of fast spread (a dynamic replication method in the data grid). Considering spatial locality, PHFS tries to predict future needs and pre-replicates them in hierarchal manner to increase locality in accesses and consequently improves performance. In this paper, we compare PHFS and CFS (common fast spread) with an example from the perspective of access latency. The results show that PHFS causes lower latency and better performance in comparison with CFS.},
  number = {3},
  journal = {Future Generation Computer Systems},
  author = {Khanli, Leyli Mohammad and Isazadeh, Ayaz and Shishavan, Tahmuras N.},
  month = mar,
  year = {2011},
  keywords = {CFS,Data Grid,Dynamic replication,PHFS},
  pages = {233-244},
  file = {/Users/benblamey/Zotero/storage/N4SZZUC5/Khanli et al. - 2011 - PHFS A dynamic replication method, to decrease ac.pdf;/Users/benblamey/Zotero/storage/F48A5BP2/S0167739X10001676.html}
}

@misc{zotero-300,
  howpublished = {https://www.researchgate.net/profile/Tarek\_Hamrouni/publication/285369168\_A\_survey\_of\_dynamic\_replication\_and\_replica\_selection\_strategies\_based\_on\_data\_mining\_techniques\_in\_data\_grids/links/5a80663d0f7e9be137c8f03f/A-survey-of-dynamic-replication-and-replica-selection-strategies-based-on-data-mining-techniques-in-data-grids.pdf},
  file = {/Users/benblamey/Zotero/storage/87SSGZBR/A-survey-of-dynamic-replication-and-replica-selection-strategies-based-on-data-mining-technique.html}
}

@misc{EngineeringApplicationsArtificial,
  title = {Engineering {{Applications}} of {{Artificial Intelligence}}},
  abstract = {Engineering Applications of Artificial Intelligence | Citations: 3,329 | Artificial Intelligence (AI) techniques are now being used by the practising engineer to solve a whole range of hitherto intractable problems. This journal provides an international forum for rapid publication of work describing the practical application of AI methods in all... | Read 1100 articles with impact on ResearchGate, the professional network for scientists.},
  language = {en},
  journal = {ResearchGate},
  howpublished = {https://www.researchgate.net/journal/0952-1976\_Engineering\_Applications\_of\_Artificial\_Intelligence},
  file = {/Users/benblamey/Zotero/storage/9IZM6Z3L/0952-1976_Engineering_Applications_of_Artificial_Intelligence.html}
}

@article{hamrouniSurveyDynamicReplication2016,
  title = {A Survey of Dynamic Replication and Replica Selection Strategies Based on Data Mining Techniques in Data Grids},
  volume = {48},
  doi = {10.1016/j.engappai.2015.11.002},
  abstract = {Mining grid data is an interesting research field which aims at analyzing grid systems with data mining techniques in order to efficiently discover new meaningful knowledge to enhance grid management. In this paper, we focus particularly on how extracted knowledge enables enhancing data replication and replica selection strategies which are important data management techniques commonly used in data grids. Indeed, relevant knowledge such as file access patterns, file correlations, user or job access behavior, prediction of future behavior or network performance, and so on, can be efficiently discovered. These findings are then used to enhance both data replication and replica selection strategies. Various works in this respect are then discussed along with their merits and demerits. In addition, we propose a new guideline to data mining application in the context of data replication and replica selection strategies.},
  journal = {Engineering Applications of Artificial Intelligence},
  author = {Hamrouni, Tarek and Slimani, Sarra and Charrada, Faouzi},
  month = feb,
  year = {2016},
  pages = {140-158},
  file = {/Users/benblamey/Zotero/storage/CEKZFYZM/Hamrouni et al. - 2016 - A survey of dynamic replication and replica select.pdf}
}

@article{hamrouniCriticalSurveyData2015,
  title = {A {{Critical Survey}} of {{Data Grid Replication Strategies Based}} on {{Data Mining Techniques}}},
  volume = {51},
  issn = {18770509},
  doi = {10.1016/j.procs.2015.05.434},
  abstract = {Replication is one common way to effectively address challenges for improving the data management in data grids. It has attracted a lot of work and many replication strategies have therefore been proposed. Most of these strategies consider a single file-based granularity and do not take into account file access patterns or possible file correlations. However, file correlations become an increasingly important consideration for performance enhancement in data grids. In this regard, the knowledge about file correlations can be extracted from historical and operational data using the techniques of the data mining field. Data mining techniques have proved to offer a powerful tool facilitating the extraction of meaningful knowledge from large data sets. As a consequence of the convergence of data mining and data grid, mining grid data is an interesting research field which aims at analyzing grid systems with data mining techniques in order to efficiently discover new meaningful knowledge to enhance data management in data grids. More precisely, in this paper, the extracted knowledge is used to enhance replica management. Gaps in the current literature and opportunities for further research are presented. In addition, we propose a new guideline to data mining application in the context of data grid replication strategies. To the best of our knowledge, this is the first survey mainly dedicated to data grid replication strategies based on data mining techniques.},
  language = {en},
  journal = {Procedia Computer Science},
  author = {Hamrouni, Tarek and Slimani, Sarra and Charrada, Faouzi Ben},
  year = {2015},
  pages = {2779-2788},
  file = {/Users/benblamey/Zotero/storage/TMDBDTYT/Hamrouni et al. - 2015 - A Critical Survey of Data Grid Replication Strateg.pdf}
}

@article{baldiniServerlessComputingCurrent2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03178},
  primaryClass = {cs},
  title = {Serverless {{Computing}}: {{Current Trends}} and {{Open Problems}}},
  shorttitle = {Serverless {{Computing}}},
  abstract = {Serverless computing has emerged as a new compelling paradigm for the deployment of applications and services. It represents an evolution of cloud programming models, abstractions, and platforms, and is a testament to the maturity and wide adoption of cloud technologies. In this chapter, we survey existing serverless platforms from industry, academia, and open source projects, identify key characteristics and use cases, and describe technical challenges and open problems.},
  journal = {arXiv:1706.03178 [cs]},
  author = {Baldini, Ioana and Castro, Paul and Chang, Kerry and Cheng, Perry and Fink, Stephen and Ishakian, Vatche and Mitchell, Nick and Muthusamy, Vinod and Rabbah, Rodric and Slominski, Aleksander and Suter, Philippe},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/Users/benblamey/Zotero/storage/G6QUDJPA/Baldini et al. - 2017 - Serverless Computing Current Trends and Open Prob.pdf;/Users/benblamey/Zotero/storage/WTYFSKKQ/1706.html}
}

@inproceedings{schmuckGPFSSharedDiskFile2002,
  title = {{{GPFS}}: {{A Shared}}-{{Disk File System}} for {{Large Computing Clusters}}.},
  volume = {2},
  shorttitle = {{{GPFS}}},
  booktitle = {{{FAST}}},
  author = {Schmuck, Frank B. and Haskin, Roger L.},
  year = {2002},
  file = {/Users/benblamey/Zotero/storage/46DQTM8G/Schmuck and Haskin - 2002 - GPFS A Shared-Disk File System for Large Computin.pdf;/Users/benblamey/Zotero/storage/PLNQAPWY/Schmuck and Haskin - 2002 - GPFS A Shared-Disk File System for Large Computin.pdf}
}

@incollection{fuhrmannDCacheStorageSystem2006a,
  address = {Berlin, Heidelberg},
  title = {{{dCache}}, {{Storage System}} for the {{Future}}},
  volume = {4128},
  isbn = {978-3-540-37783-2 978-3-540-37784-9},
  abstract = {In 2007, the most challenging high energy physics experiment ever, the Large Hardon Collider(LHC), at CERN, will produce a sustained stream of data in the order of 300MB/sec, equivalent to a stack of CDs as high as the Eiffel Tower once per week. This data is, while produced, distributed and persistently stored at several dozens of sites around the world, building the LHC data grid. The destination sites are expected to provide the necessary middle-ware, so called Storage Elements, offering standard protocols to receive the data and to store it at the site specific Storage Systems. A major player in the set of Storage Elements is the dCache/SRM system. dCache/SRM has proven to be capable of managing the storage and exchange of several hundreds of terabytes of data, transparently distributed among dozens of disk storage nodes. One of the key design features of the dCache is that although the location and multiplicity of the data is autonomously determined by the system, based on configuration, cpu load and disk space, the name space is uniquely represented within a single file system tree. The system has shown to significantly improve the efficiency of connected tape storage systems, by caching, 'gather \& flush' and scheduled staging techniques. Furthermore, it optimizes the throughput to and from data clients as well as smoothing the load of the connected disk storage nodes by dynamically replicating datasets on the detection of load hot spots. The system is tolerant against failures of its data servers which enables administrators to go for commodity disk storage components. Access to the data is provided by various standard protocols. Furthermore the software is coming with an implementation of the Storage Resource Manager protocol (SRM), which is evolving to an open standard for grid middleware to communicate with site specific storage fabrics.},
  language = {en},
  booktitle = {Euro-{{Par}} 2006 {{Parallel Processing}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Fuhrmann, Patrick and G\"ulzow, Volker},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Nagel, Wolfgang E. and Walter, Wolfgang V. and Lehner, Wolfgang},
  year = {2006},
  pages = {1106-1113},
  file = {/Users/benblamey/Zotero/storage/CL3669KV/Fuhrmann and Gülzow - 2006 - dCache, Storage System for the Future.pdf},
  doi = {10.1007/11823285_116}
}

@inproceedings{barringCASTORDistributedStorage2007,
  title = {{{CASTOR}}: {{A Distributed Storage Resource Facility}} for {{High Performance Data Processing}} at {{CERN}}},
  shorttitle = {{{CASTOR}}},
  abstract = {viding the data storage capacity and performance for managing the data produced by the LHC experiments. These Mass storage systems at CERN have evolved over time data will be processed globally using the LHC Computing to meet growing requirements, in terms of both scalability Grid [8]. and fault resiliency. The CERN Advanced STORage sys The main data store, called Tier 0, needs to store all tem (CASTOR) and its new disk cache management layer data coming from the LHC experiments, i.e. ATLAS, AL (CASTOR2) have been developed to meet the challenges ICE, CMS and LHCb, and to run the initial data reconstrucraised by the experiments using the new accelerator that tion. CASTOR provides a Central Data Recording facility CERN is building: the Large Hadron CollideI ' (LHC) [4J. (CDR) and storage for the associated Reconstruction fa This system must be able to cope with hundreds ofmillions cilities. CASTOR handles data transfers to the Tier 1 sites, offiles, tens ofpetabytes of storage and handle a constant where the data are replicated and further reconstruction and throughput of several gigabytes per second. In this paper, analysis take place. Finally, many Tier 2 sites are linked to we detail CASTOR's architecture and implementation and each Tier I and act as data customers for the physics analpresent some operational aspects. We finally list the perysis data. Figure 1 gives a graphical view of the different formance levels achieved by the current version both in a concurrent activities CASTOR is handling. production environment and during internal tests.},
  author = {Barring, Olof and Earl, Alasdair and Maria, Rosa and Rioja, Garcia and Ponce, Sebastien and Taurelli, Giulia and Waldron, Dennis and Coelho, Miguel and Santos, Dos},
  year = {2007},
  file = {/Users/benblamey/Zotero/storage/FVYES6VZ/Barring et al. - CASTOR A Distributed Storage Resource Facility fo.pdf;/Users/benblamey/Zotero/storage/XMHIWIKE/summary.html}
}

@book{lundstromDesignImplementationMongoDB2011,
  title = {Design and {{Implementation}} of a {{MongoDB Driver}} for {{Prolog}}},
  abstract = {Design and Implementation of a MongoDB Driver for Prolog},
  language = {eng},
  author = {Lundstr\"om, Sebastian},
  year = {2011},
  file = {/Users/benblamey/Zotero/storage/L9B8ANMP/Lundström - 2011 - Design and Implementation of a MongoDB Driver for .pdf;/Users/benblamey/Zotero/storage/6R5AZSSV/record.html}
}

@inproceedings{weilCRUSHControlledScalable2006,
  title = {{{CRUSH}}: {{Controlled}}, {{Scalable}}, {{Decentralized Placement}} of {{Replicated Data}}},
  isbn = {978-0-7695-2700-0},
  shorttitle = {{{CRUSH}}},
  doi = {10.1109/SC.2006.19},
  abstract = {Emerging large-scale distributed storage systems are faced with the task of distributing petabytes of data among tens or hundreds of thousands of storage devices. Such systems must evenly distribute data and workload to efficiently utilize available resources and maximize system performance, while facilitating system growth and managing hardware failures. We have developed CRUSH, a scalable pseudorandom data distribution function designed for distributed object-based storage systems that efficiently maps data objects to storage devices without relying on a central directory. Because large systems are inherently dynamic, CRUSH is designed to facilitate the addition and removal of storage while minimizing unnecessary data movement. The algorithm accommodates a wide variety of data replication and reliability mechanisms and distributes data in terms of userdefined policies that enforce separation of replicas across failure domains.},
  language = {en},
  publisher = {{IEEE}},
  author = {Weil, Sage and Brandt, Scott and Miller, Ethan and Maltzahn, Carlos},
  month = nov,
  year = {2006},
  pages = {31-31},
  file = {/Users/benblamey/Zotero/storage/MALUD295/Weil et al. - 2006 - CRUSH Controlled, Scalable, Decentralized Placeme.pdf}
}

@misc{ObjectStorageThings2017,
  title = {Object {{Storage}}: 8 {{Things}} to {{Know}}},
  shorttitle = {Object {{Storage}}},
  abstract = {Learn about the fast-growing technology that's reshaping enterprise storage.},
  language = {en},
  journal = {Network Computing},
  howpublished = {http://www.networkcomputing.com/storage/object-storage-8-things-know/1376952232},
  month = oct,
  year = {2017},
  file = {/Users/benblamey/Zotero/storage/4WPBKS2M/1376952232.html}
}

@book{sowaConceptualStructuresInformation1984,
  address = {Boston, MA, USA},
  title = {Conceptual {{Structures}}: {{Information Processing}} in {{Mind}} and {{Machine}}},
  isbn = {978-0-201-14472-7},
  shorttitle = {Conceptual {{Structures}}},
  publisher = {{Addison-Wesley Longman Publishing Co., Inc.}},
  author = {Sowa, J. F.},
  year = {1984}
}

@misc{keyPlus500Shares102017,
  title = {Plus500 Shares up over 10 per Cent after Doubling Underlying Profit},
  abstract = {Shares in spreadbetting firm~Plus500 were up almost 13 per cent this morning after the group reported a record first half performance.},
  language = {en},
  howpublished = {http://www.cityam.com/269824/plus500-share-price-rockets-after-doubling-first-half},
  author = {Key, Alys, Oliver Gill},
  month = aug,
  year = {2017},
  file = {/Users/benblamey/Zotero/storage/UPVT3F27/plus500-share-price-rockets-after-doubling-first-half.html}
}

@article{vazeOnlineKnapsackProblem2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.10652},
  primaryClass = {cs},
  title = {Online {{Knapsack Problem}} under {{Expected Capacity Constraint}}},
  abstract = {Online knapsack problem is considered, where items arrive in a sequential fashion that have two attributes; value and weight. Each arriving item has to be accepted or rejected on its arrival irrevocably. The objective is to maximize the sum of the value of the accepted items such that the sum of their weights is below a budget/capacity. Conventionally a hard budget/capacity constraint is considered, for which variety of results are available. In modern applications, e.g., in wireless networks, data centres, cloud computing, etc., enforcing the capacity constraint in expectation is sufficient. With this motivation, we consider the knapsack problem with an expected capacity constraint. For the special case of knapsack problem, called the secretary problem, where the weight of each item is unity, we propose an algorithm whose probability of selecting any one of the optimal items is equal to \$1-1/e\$ and provide a matching lower bound. For the general knapsack problem, we propose an algorithm whose competitive ratio is shown to be \$1/4e\$ that is significantly better than the best known competitive ratio of \$1/10e\$ for the knapsack problem with the hard capacity constraint.},
  journal = {arXiv:1711.10652 [cs]},
  author = {Vaze, Rahul},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/benblamey/Zotero/storage/LS324226/Vaze - 2017 - Online Knapsack Problem under Expected Capacity Co.pdf;/Users/benblamey/Zotero/storage/NCNHAHA8/1711.html}
}

@article{kleinbergMultipleChoiceSecretaryAlgorithm2015,
  title = {A {{Multiple}}-{{Choice Secretary Algorithm}} with {{Applications}} to {{Online Auctions}}},
  abstract = {In the classical secretary problem, a set S of numbers is presented to an online algorithm in random order. At any time the algorithm may stop and choose the current element, and the goal is to maximize the probability of choosing the largest element in the set. We study a variation in which the algorithm is allowed to choose k elements, and the goal is to maximize their sum. We present an algorithm whose competitive ratio is 1-O( 1/k). To our knowledge, this is the first algorithm whose competitive ratio approaches 1 as k $\rightarrow$ $\infty$. As an application we solve an open problem in the theory of online auction mechanisms.},
  language = {en},
  author = {Kleinberg, Robert},
  year = {2015},
  pages = {2},
  file = {/Users/benblamey/Zotero/storage/Z7DLT5T7/Kleinberg - A Multiple-Choice Secretary Algorithm with Applica.pdf}
}

@inproceedings{blellochSortingAsymmetricRead2015,
  title = {Sorting with {{Asymmetric Read}} and {{Write Costs}}},
  isbn = {978-1-4503-3588-1},
  doi = {10.1145/2755573.2755604},
  abstract = {Emerging memory technologies have a significant gap between the cost, both in time and in energy, of writing to memory versus reading from memory. In this paper we present models and algorithms that account for this difference, with a focus on write-efficient sorting algorithms. First, we consider the PRAM model with asymmetric write cost, and show that sorting can be performed in O(n) writes, O(n log n) reads, and logarithmic depth (parallel time). Next, we consider a variant of the External Memory (EM) model that charges k $>$ 1 for writing a block of size B to the secondary memory, and present variants of three EM sorting algorithms (multi-way mergesort, sample sort, and heapsort using buffer trees) that asymptotically reduce the number of writes over the original algorithms, and perform roughly k block reads for every block write. Finally, we define a variant of the Ideal-Cache model with asymmetric write costs, and present write-efficient, cache-oblivious parallel algorithms for sorting, FFTs, and matrix multiplication. Adapting prior bounds for work-stealing and parallel-depth-first schedulers to the asymmetric setting, these yield parallel cache complexity bounds for machines with private caches or with a shared cache, respectively.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Blelloch, Guy E. and Fineman, Jeremy T. and Gibbons, Phillip B. and Gu, Yan and Shun, Julian},
  year = {2015},
  pages = {1-12},
  file = {/Users/benblamey/Zotero/storage/7RDIQBQK/Blelloch et al. - 2015 - Sorting with Asymmetric Read and Write Costs.pdf}
}

@inproceedings{gionisAlgorithmsDiscoveringBucket2006,
  title = {Algorithms for Discovering Bucket Orders from Data},
  isbn = {978-1-59593-339-3},
  doi = {10.1145/1150402.1150468},
  abstract = {Ordering and ranking items of different types are important tasks in various applications, such as query processing and scientific data mining. A total order for the items can be misleading, since there are groups of items that have practically equal ranks.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Gionis, Aristides and Mannila, Heikki and Puolam\"aki, Kai and Ukkonen, Antti},
  year = {2006},
  pages = {561},
  file = {/Users/benblamey/Zotero/storage/PIWTQ4S9/Gionis et al. - 2006 - Algorithms for discovering bucket orders from data.pdf}
}

@incollection{kenkreDiscoveringBucketOrders2011,
  address = {Philadelphia, PA},
  title = {On {{Discovering Bucket Orders}} from {{Preference Data}}},
  isbn = {978-0-89871-992-5 978-1-61197-281-8},
  abstract = {Consider as an example, the problem of aggregatThe problem of ordering a set of entities which contain inherent ties among them arises in many applications. Notion of ``bucket order'' has emerged as a popular mechanism of ranking in such settings. A bucket order is an ordered partition of the set of entities into ``buckets''. There is a total order on the buckets, but the entities within a bucket are treated as tied. In this paper, we focus on discovering bucket order from data captured in the form of user preferences. We consider two settings: one in which the discrepancies in the input preferences are ``local'' (when collected from experts) and the other in which discrepancies could be arbitrary (when collected from a large population). We present a formal model to capture the setting of local discrepancies and consider the following question: ``how many experts need to be queried to discover the underlying bucket order$\surd$ on n entities?''. We prove an upperbound of O( log n). In the case of arbitrary discrepancies, we model it as the bucket order problem of discovering a bucket order that best fits the data (captured as pairwise preference statistics). We present a new approach which exploits a connection between the discovery of buckets and the correlation clustering problem. We present empirical evaluation of our algorithms on real and artificially generated datasets.},
  language = {en},
  booktitle = {Proceedings of the 2011 {{SIAM International Conference}} on {{Data Mining}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Kenkre, Sreyash and Khan, Arindam and Pandit, Vinayaka},
  editor = {Liu, Bing and Liu, Huan and Clifton, Chris and Washio, Takashi and Kamath, Chandrika},
  month = apr,
  year = {2011},
  pages = {872-883},
  file = {/Users/benblamey/Zotero/storage/YY7Z2K8S/Kenkre et al. - 2011 - On Discovering Bucket Orders from Preference Data.pdf},
  doi = {10.1137/1.9781611972818.75}
}

@inproceedings{liuGenericHighlyScalable2018,
  title = {A {{Generic}} and {{Highly Scalable Framework}} for the {{Automation}} and {{Execution}} of {{Scientific Data Processing}} and {{Simulation Workflows}}},
  doi = {10.1109/ICSA.2018.00024},
  abstract = {In order to perform complex data processing and co-simulation workflows for research on data driven energy systems, a generic, modular and highly scalable process operation framework is presented in this article. This framework consistently applies web technologies to build up a microservices architecture. It automates the startup, synchronization, and management of scientific data processing and simulation tools (e.g. Python, Matlab, OpenModelica) as part of larger transdisciplinary, multi-domain data processing and co-simulation workflows. It uses container virtualization on the underlying cluster computing environment to control and manage different simulation nodes.Within the framework's processing workflow, software executables can be distributed to different nodes on the cluster, easily access data and communicate with other components via communication adapters and a high-performance messaging channel infrastructure. By integrating Apache NiFi, the framework also provides an easy-to-use web user interface to allow users to model, perform and operate workflows for future energy system solutions. As soon as a complex workflow is set up in the process operation framework, researchers can use the workflow without any setup or configuration on their local workstations and without knowing any details of the underlying infrastructure or software environment.},
  booktitle = {2018 {{IEEE International Conference}} on {{Software Architecture}} ({{ICSA}})},
  author = {Liu, J. and Braun, E. and D\"upmeier, C. and Kuckertz, P. and Ryberg, D. S. and Robinius, M. and Stolten, D. and Hagenmeyer, V.},
  month = apr,
  year = {2018},
  keywords = {Computer architecture,Big Data,Computational modeling,access data,automation,cluster computing environment,co-simulation,co-simulation workflows,communication adapters,complex data processing,complex workflow,computing cluster,container virtualization,Containers,Data models,data processing,easy-to-use Web user interface,energy system,framework processing workflow,future energy system solutions,generic process operation framework,high-performance messaging channel infrastructure,highly scalable process operation framework,information retrieval,integrating Apache NiFi,Internet,larger transdisciplinary,local workstations,microservices,microservices architecture,modular process operation framework,multidomain data processing,scientific data processing,simulation nodes,simulation tools,software environment,software executables,underlying infrastructure,user interfaces,Virtualization,Web technologies},
  pages = {145-14510},
  file = {/Users/benblamey/Zotero/storage/EFXZXX69/8417148.html}
}

@inproceedings{dobbelaereKafkaRabbitMQComparative2017,
  address = {New York, NY, USA},
  series = {DEBS '17},
  title = {Kafka {{Versus RabbitMQ}}: {{A Comparative Study}} of {{Two Industry Reference Publish}}/{{Subscribe Implementations}}: {{Industry Paper}}},
  isbn = {978-1-4503-5065-5},
  shorttitle = {Kafka {{Versus RabbitMQ}}},
  doi = {10.1145/3093742.3093908},
  abstract = {Publish/subscribe is a distributed interaction paradigm well adapted to the deployment of scalable and loosely coupled systems. Apache Kafka and RabbitMQ are two popular open-source and commercially-supported pub/sub systems that have been around for almost a decade and have seen wide adoption. Given the popularity of these two systems and the fact that both are branded as pub/sub systems, two frequently asked questions in the relevant online forums are: how do they compare against each other and which one to use? In this paper, we frame the arguments in a holistic approach by establishing a common comparison framework based on the core functionalities of pub/sub systems. Using this framework, we then venture into a qualitative and quantitative (i.e. empirical) comparison of the common features of the two systems. Additionally, we also highlight the distinct features that each of these systems has. After enumerating a set of use cases that are best suited for RabbitMQ or Kafka, we try to guide the reader through a determination table to choose the best architecture given his/her particular set of requirements.},
  booktitle = {Proceedings of the 11th {{ACM International Conference}} on {{Distributed}} and {{Event}}-Based {{Systems}}},
  publisher = {{ACM}},
  author = {Dobbelaere, Philippe and Esmaili, Kyumars Sheykh},
  year = {2017},
  keywords = {Reliability,AMQP,Apache Kafka,Log Files,Message Brokers,Performance,Publish/Subscribe Systems,RabbitMQ},
  pages = {227--238}
}

@book{dunningStreamingArchitectureNew2016,
  title = {Streaming {{Architecture}}: {{New Designs Using Apache Kafka}} and {{MapR Streams}}},
  isbn = {978-1-4919-5390-7},
  shorttitle = {Streaming {{Architecture}}},
  abstract = {More and more data-driven companies are looking to adopt stream processing and streaming analytics. With this concise ebook, you'll learn best practices for designing a reliable architecture that supports this emerging big-data paradigm.Authors Ted Dunning and Ellen Friedman (Real World Hadoop) help you explore some of the best technologies to handle stream processing and analytics, with a focus on the upstream queuing or message-passing layer. To illustrate the effectiveness of these technologies, this book also includes specific use cases.Ideal for developers and non-technical people alike, this book describes:Key elements in good design for streaming analytics, focusing on the essential characteristics of the messaging layerNew messaging technologies, including Apache Kafka and MapR Streams, with links to sample codeTechnology choices for streaming analytics: Apache Spark Streaming, Apache Flink, Apache Storm, and Apache ApexHow stream-based architectures are helpful to support microservicesSpecific use cases such as fraud detection and geo-distributed data streamsTed Dunning is Chief Applications Architect at MapR Technologies, and active in the open source community. He currently serves as VP for Incubator at the Apache Foundation, as a champion and mentor for a large number of projects, and as committer and PMC member of the Apache ZooKeeper and Drill projects. Ted is on Twitter as @ted\_dunning.Ellen Friedman, a committer for the Apache Drill and Apache Mahout projects, is a solutions consultant and well-known speaker and author, currently writing mainly about big data topics. With a PhD in Biochemistry, she has years of experience as a research scientist and has written about a variety of technical topics. Ellen is on Twitter as @Ellen\_Friedman.},
  language = {en},
  publisher = {{"O'Reilly Media, Inc."}},
  author = {Dunning, Ted and Friedman, Ellen},
  month = may,
  year = {2016},
  keywords = {Computers / Data Processing,Computers / Data Transmission Systems / Electronic Data Interchange,Computers / Data Transmission Systems / General,Computers / Databases / Data Mining,Computers / Databases / General,Computers / Databases / Servers}
}

@inproceedings{liuGenericHighlyScalable2018a,
  title = {A {{Generic}} and {{Highly Scalable Framework}} for the {{Automation}} and {{Execution}} of {{Scientific Data Processing}} and {{Simulation Workflows}}},
  doi = {10.1109/ICSA.2018.00024},
  abstract = {In order to perform complex data processing and co-simulation workflows for research on data driven energy systems, a generic, modular and highly scalable process operation framework is presented in this article. This framework consistently applies web technologies to build up a microservices architecture. It automates the startup, synchronization, and management of scientific data processing and simulation tools (e.g. Python, Matlab, OpenModelica) as part of larger transdisciplinary, multi-domain data processing and co-simulation workflows. It uses container virtualization on the underlying cluster computing environment to control and manage different simulation nodes.Within the framework's processing workflow, software executables can be distributed to different nodes on the cluster, easily access data and communicate with other components via communication adapters and a high-performance messaging channel infrastructure. By integrating Apache NiFi, the framework also provides an easy-to-use web user interface to allow users to model, perform and operate workflows for future energy system solutions. As soon as a complex workflow is set up in the process operation framework, researchers can use the workflow without any setup or configuration on their local workstations and without knowing any details of the underlying infrastructure or software environment.},
  booktitle = {2018 {{IEEE International Conference}} on {{Software Architecture}} ({{ICSA}})},
  author = {Liu, J. and Braun, E. and D\"upmeier, C. and Kuckertz, P. and Ryberg, D. S. and Robinius, M. and Stolten, D. and Hagenmeyer, V.},
  month = apr,
  year = {2018},
  keywords = {Computer architecture,Big Data,Computational modeling,access data,automation,cluster computing environment,co-simulation,co-simulation workflows,communication adapters,complex data processing,complex workflow,computing cluster,container virtualization,Containers,Data models,data processing,easy-to-use Web user interface,energy system,framework processing workflow,future energy system solutions,generic process operation framework,high-performance messaging channel infrastructure,highly scalable process operation framework,information retrieval,integrating Apache NiFi,Internet,larger transdisciplinary,local workstations,microservices,microservices architecture,modular process operation framework,multidomain data processing,scientific data processing,simulation nodes,simulation tools,software environment,software executables,underlying infrastructure,user interfaces,Virtualization,Web technologies},
  pages = {145-14510},
  file = {/Users/benblamey/Zotero/storage/ISAAT37M/8417148.html}
}

@misc{KafkaRabbitMQp227DobbelaerePdf,
  title = {Kafka versus {{RabbitMQ}}-P227-{{Dobbelaere}}.Pdf - {{Industry Paper Kafka}} versus {{RabbitMQ A}} Comparative Study of Two Industry Reference Publish/Subscribe},
  abstract = {View Kafka versus RabbitMQ-p227-Dobbelaere.pdf from CPSC 545 at Seattle University. Industry Paper: Kafka versus RabbitMQ
A comparative study of two industry reference publish/subscribe},
  language = {en},
  howpublished = {https://www.coursehero.com/file/30725178/Kafka-versus-RabbitMQ-p227-Dobbelaerepdf/},
  file = {/Users/benblamey/Zotero/storage/JZXC6KJV/Kafka-versus-RabbitMQ-p227-Dobbelaerepdf.html}
}

@article{dobbelaereKafkaRabbitMQ2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.00333},
  primaryClass = {cs},
  title = {Kafka versus {{RabbitMQ}}},
  abstract = {Publish/subscribe is a distributed interaction paradigm well adapted to the deployment of scalable and loosely coupled systems. Apache Kafka and RabbitMQ are two popular open-source and commercially-supported pub/sub systems that have been around for almost a decade and have seen wide adoption. Given the popularity of these two systems and the fact that both are branded as pub/sub systems, two frequently asked questions in the relevant online forums are: how do they compare against each other and which one to use? In this paper, we frame the arguments in a holistic approach by establishing a common comparison framework based on the core functionalities of pub/sub systems. Using this framework, we then venture into a qualitative and quantitative (i.e. empirical) comparison of the common features of the two systems. Additionally, we also highlight the distinct features that each of these systems has. After enumerating a set of use cases that are best suited for RabbitMQ or Kafka, we try to guide the reader through a determination table to choose the best architecture given his/her particular set of requirements.},
  journal = {arXiv:1709.00333 [cs]},
  author = {Dobbelaere, Philippe and Esmaili, Kyumars Sheykh},
  month = sep,
  year = {2017},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Performance},
  file = {/Users/benblamey/Zotero/storage/FGPB8UYZ/Dobbelaere and Esmaili - 2017 - Kafka versus RabbitMQ.pdf;/Users/benblamey/Zotero/storage/6WWX9NWV/1709.html}
}

@article{blameyApacheSparkStreaming2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.07724},
  primaryClass = {cs},
  title = {Apache {{Spark Streaming}} and {{HarmonicIO}}: {{A Performance}} and {{Architecture Comparison}}},
  shorttitle = {Apache {{Spark Streaming}} and {{HarmonicIO}}},
  abstract = {Studies have demonstrated that Apache Spark, Flink and related frameworks can perform stream processing at very high frequencies, whilst tending to focus on small messages with a computationally light ‘map' stage for each message; a common enterprise use case. We add to these benchmarks by broadening the domain to include loads with larger messages (leading to network-bound throughput), and that are computationally intensive (leading to CPU-bound throughput) in the map phase; in order to evaluate applicability of these frameworks to scientific computing applications. We present a performance benchmark comparison between Apache Spark Streaming (ASS) under both file and TCP streaming modes; and HarmonicIO, comparing maximum throughput over a broad domain of message sizes and CPU loads. We find that relative performance varies considerably across this domain, with the chosen means of stream source integration having a big impact. We offer recommendations for choosing and configuring the frameworks, and present a benchmarking toolset developed for this study.},
  journal = {arXiv:1807.07724 [cs]},
  author = {Blamey, Ben and Hellander, Andreas and Toor, Salman},
  month = jul,
  year = {2018},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/Users/benblamey/Zotero/storage/4I38T8HJ/Blamey et al. - 2018 - Apache Spark Streaming and HarmonicIO A Performan.pdf;/Users/benblamey/Zotero/storage/6EC4DHUQ/1807.html}
}

@article{freemanSecretaryProblemIts1983,
  title = {The Secretary Problem and Its Extensions: {{A}} Review},
  shorttitle = {The Secretary Problem and Its Extensions},
  journal = {International Statistical Review/Revue Internationale de Statistique},
  author = {Freeman, P. R.},
  year = {1983},
  pages = {189--206},
  file = {/Users/benblamey/Zotero/storage/DN33XX2L/Freeman - 1983 - The secretary problem and its extensions A review.pdf;/Users/benblamey/Zotero/storage/FA3NUXHB/1402748.html}
}

@article{thomasMININGUNSTRUCTUREDSOFTWARE,
  title = {{{MINING UNSTRUCTURED SOFTWARE REPOSITORIES USING IR MODELS}}},
  language = {en},
  author = {Thomas, Stephen W},
  pages = {249},
  file = {/Users/benblamey/Zotero/storage/36YLXEHB/Thomas - MINING UNSTRUCTURED SOFTWARE REPOSITORIES USING IR.pdf}
}

@article{maddisonStructuredGenerativeModels,
  title = {Structured {{Generative Models}} of {{Natural Source Code}}},
  abstract = {We study the problem of building generative models of natural source code (NSC); that is, source code written by humans and meant to be understood by humans. Our primary contribution is to describe new generative models that are tailored to NSC. The models are based on probabilistic context free grammars (PCFGs) and neuro-probabilistic language models (Mnih \& Teh, 2012), which are extended to incorporate additional source code-specific structure. These models can be efficiently trained on a corpus of source code and outperform a variety of less structured baselines in terms of predictive log likelihoods on held-out data.},
  language = {en},
  author = {Maddison, Chris J and Tarlow, Daniel},
  pages = {9},
  file = {/Users/benblamey/Zotero/storage/ESJDKZ2G/Maddison and Tarlow - Structured Generative Models of Natural Source Cod.pdf}
}

@inproceedings{karaivanovPhraseBasedStatisticalTranslation2014,
  address = {Portland, Oregon, USA},
  title = {Phrase-{{Based Statistical Translation}} of {{Programming Languages}}},
  isbn = {978-1-4503-3210-1},
  doi = {10.1145/2661136.2661148},
  abstract = {Phrase-based statistical machine translation approaches have been highly successful in translating between natural languages and are heavily used by commercial systems (e.g. Google Translate).},
  language = {en},
  booktitle = {Proceedings of the 2014 {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} \& {{Software}} - {{Onward}}! '14},
  publisher = {{ACM Press}},
  author = {Karaivanov, Svetoslav and Raychev, Veselin and Vechev, Martin},
  year = {2014},
  pages = {173-184},
  file = {/Users/benblamey/Zotero/storage/J6RJVC8V/Karaivanov et al. - 2014 - Phrase-Based Statistical Translation of Programmin.pdf}
}

@article{allamanisMiningIdiomsSource2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1404.0417},
  title = {Mining {{Idioms}} from {{Source Code}}},
  doi = {10.1145/2635868.2635901},
  abstract = {We present the first method for automatically mining code idioms from a corpus of previously written, idiomatic software projects. We take the view that a code idiom is a syntactic fragment that recurs across projects and has a single semantic role. Idioms may have metavariables, such as the body of a for loop. Modern IDEs commonly provide facilities for manually defining idioms and inserting them on demand, but this does not help programmers to write idiomatic code in languages or using libraries with which they are unfamiliar. We present Haggis, a system for mining code idioms that builds on recent advanced techniques from statistical natural language processing, namely, nonparametric Bayesian probabilistic tree substitution grammars. We apply Haggis to several of the most popular open source projects from GitHub. We present a wide range of evidence that the resulting idioms are semantically meaningful, demonstrating that they do indeed recur across software projects and that they occur more frequently in illustrative code examples collected from a Q\&A site. Manual examination of the most common idioms indicate that they describe important program concepts, including object creation, exception handling, and resource management.},
  language = {en},
  journal = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering - FSE 2014},
  author = {Allamanis, Miltiadis and Sutton, Charles},
  year = {2014},
  keywords = {Computer Science - Software Engineering},
  pages = {472-483},
  file = {/Users/benblamey/Zotero/storage/WS7KWAK9/Allamanis and Sutton - 2014 - Mining Idioms from Source Code.pdf}
}

@inproceedings{prokschEvaluatingEvaluationsCode2016,
  address = {Singapore, Singapore},
  title = {Evaluating the Evaluations of Code Recommender Systems: A Reality Check},
  isbn = {978-1-4503-3845-5},
  shorttitle = {Evaluating the Evaluations of Code Recommender Systems},
  doi = {10.1145/2970276.2970330},
  abstract = {While researchers develop many new exciting code recommender systems, such as method-call completion, code-snippet completion, or code search, an accurate evaluation of such systems is always a challenge. We analyzed the current literature and found that most of the current evaluations rely on artificial queries extracted from released code, which begs the question: Do such evaluations reflect real-life usages? To answer this question, we capture 6,189 fine-grained development histories from real IDE interactions. We use them as a ground truth and extract 7,157 real queries for a specific method-call recommender system. We compare the results of such real queries with different artificial evaluation strategies and check several assumptions that are repeatedly used in research, but never empirically evaluated. We find that an evolving context that is often observed in practice has a major effect on the prediction quality of recommender systems, but is not commonly reflected in artificial evaluations.},
  language = {en},
  booktitle = {Proceedings of the 31st {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} - {{ASE}} 2016},
  publisher = {{ACM Press}},
  author = {Proksch, Sebastian and Amann, Sven and Nadi, Sarah and Mezini, Mira},
  year = {2016},
  pages = {111-121},
  file = {/Users/benblamey/Zotero/storage/JMFYSSZH/Proksch et al. - 2016 - Evaluating the evaluations of code recommender sys.pdf}
}

@article{guptaDeepFixFixingCommon,
  title = {{{DeepFix}}: {{Fixing Common C Language Errors}} by {{Deep Learning}}},
  abstract = {The problem of automatically fixing programming errors is a very active research topic in software engineering. This is a challenging problem as fixing even a single error may require analysis of the entire program. In practice, a number of errors arise due to programmer's inexperience with the programming language or lack of attention to detail. We call these common programming errors. These are analogous to grammatical errors in natural languages. Compilers detect such errors, but their error messages are usually inaccurate. In this work, we present an end-to-end solution, called DeepFix, that can fix multiple such errors in a program without relying on any external tool to locate or fix them. At the heart of DeepFix is a multi-layered sequence-to-sequence neural network with attention which is trained to predict erroneous program locations along with the required correct statements. On a set of 6971 erroneous C programs written by students for 93 programming tasks, DeepFix could fix 1881 (27\%) programs completely and 1338 (19\%) programs partially.},
  language = {en},
  author = {Gupta, Rahul and Pal, Soham and Kanade, Aditya and Shevade, Shirish},
  pages = {7},
  file = {/Users/benblamey/Zotero/storage/IICP55J6/Gupta et al. - DeepFix Fixing Common C Language Errors by Deep L.pdf}
}

@article{shirazReviewDistributedApplication23,
  title = {A {{Review}} on {{Distributed Application Processing Frameworks}} in {{Smart Mobile Devices}} for {{Mobile Cloud Computing}}},
  volume = {15},
  issn = {1553-877X},
  doi = {10.1109/SURV.2012.111412.00045},
  abstract = {The latest developments in mobile devices technology have made smartphones as the future computing and service access devices. Users expect to run computational intensive applications on Smart Mobile Devices (SMDs) in the same way as powerful stationary computers. However in spite of all the advancements in recent years, SMDs are still low potential computing devices, which are constrained by CPU potentials, memory capacity and battery life time. Mobile Cloud Computing (MCC) is the latest practical solution for alleviating this incapacitation by extending the services and resources of computational clouds to SMDs on demand basis. In MCC, application offloading is ascertained as a software level solution for augmenting application processing capabilities of SMDs. The current offloading algorithms offload computational intensive applications to remote servers by employing different cloud models. A challenging aspect of such algorithms is the establishment of distributed application processing platform at runtime which requires additional computing resources on SMDs. This paper reviews existing Distributed Application Processing Frameworks (DAPFs) for SMDs in MCC domain. The objective is to highlight issues and challenges to existing DAPFs in developing, implementing, and executing computational intensive mobile applications within MCC domain. It proposes thematic taxonomy of current DAPFs, reviews current offloading frameworks by using thematic taxonomy and analyzes the implications and critical aspects of current offloading frameworks. Further, it investigates commonalities and deviations in such frameworks on the basis significant parameters such as offloading scope, migration granularity, partitioning approach, and migration pattern. Finally, we put forward open research issues in distributed application processing for MCC that remain to be addressed.},
  language = {en},
  number = {3},
  journal = {IEEE Communications Surveys \& Tutorials},
  author = {Shiraz, Muhammad and Gani, Abdullah and Khokhar, Rashid Hafeez and Buyya, Rajkumar},
  year = {23},
  pages = {1294-1313},
  file = {/Users/benblamey/Zotero/storage/AI3GRDT2/Shiraz et al. - 2013 - A Review on Distributed Application Processing Fra.pdf}
}

@inproceedings{liuSurveyRealtimeProcessing2014a,
  address = {Porto, Portugal},
  title = {Survey of Real-Time Processing Systems for Big Data},
  isbn = {978-1-4503-2627-8},
  doi = {10.1145/2628194.2628251},
  abstract = {In recent years, real-time processing and analytics systems for big data\textendash{}in the context of Business Intelligence (BI)\textendash{}have received a growing attention. The traditional BI platforms that perform regular updates on daily, weekly or monthly basis are no longer adequate to satisfy the fast-changing business environments. However, due to the nature of big data, it has become a challenge to achieve the real-time capability using the traditional technologies. The recent distributed computing technology, MapReduce, provides off-the-shelf high scalability that can significantly shorten the processing time for big data; Its open-source implementation such as Hadoop has become the de-facto standard for processing big data, however, Hadoop has the limitation of supporting real-time updates. The improvements in Hadoop for the real-time capability, and the other alternative real-time frameworks have been emerging in recent years. This paper presents a survey of the open source technologies that support big data processing in a real-time/near real-time fashion, including their system architectures and platforms.},
  language = {en},
  booktitle = {Proceedings of the 18th {{International Database Engineering}} \& {{Applications Symposium}} on - {{IDEAS}} '14},
  publisher = {{ACM Press}},
  author = {Liu, Xiufeng and Iftikhar, Nadeem and Xie, Xike},
  year = {2014},
  pages = {356-361},
  file = {/Users/benblamey/Zotero/storage/52TWYKVH/Liu et al. - 2014 - Survey of real-time processing systems for big dat.pdf}
}

@article{tibshiraniValeriePatrickHastie,
  title = {Valerie and {{Patrick Hastie}}},
  language = {en},
  author = {Tibshirani, Sami and Friedman, Harry},
  pages = {764},
  file = {/Users/benblamey/Zotero/storage/Q2LL5V2R/Tibshirani and Friedman - Valerie and Patrick Hastie.pdf}
}

@article{hoeferCombinatorialSecretaryProblems2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.01290},
  primaryClass = {cs},
  title = {Combinatorial {{Secretary Problems}} with {{Ordinal Information}}},
  abstract = {The secretary problem is a classic model for online decision making. Recently, combinatorial extensions such as matroid or matching secretary problems have become an important tool to study algorithmic problems in dynamic markets. Here the decision maker must know the numerical value of each arriving element, which can be a demanding informational assumption. In this paper, we initiate the study of combinatorial secretary problems with ordinal information, in which the decision maker only needs to be aware of a preference order consistent with the values of arrived elements. The goal is to design online algorithms with small competitive ratios. For a variety of combinatorial problems, such as bipartite matching, general packing LPs, and independent set with bounded local independence number, we design new algorithms that obtain constant competitive ratios. For the matroid secretary problem, we observe that many existing algorithms for special matroid structures maintain their competitive ratios even in the ordinal model. In these cases, the restriction to ordinal information does not represent any additional obstacle. Moreover, we show that ordinal variants of the submodular matroid secretary problems can be solved using algorithms for the linear versions by extending [Feldman and Zenklusen, 2015]. In contrast, we provide a lower bound of \$$\backslash$Omega($\backslash$sqrt\{n\}/($\backslash$log n))\$ for algorithms that are oblivious to the matroid structure, where \$n\$ is the total number of elements. This contrasts an upper bound of \$O($\backslash$log n)\$ in the cardinal model, and it shows that the technique of thresholding is not sufficient for good algorithms in the ordinal model.},
  journal = {arXiv:1702.01290 [cs]},
  author = {Hoefer, Martin and Kodric, Bojana},
  month = feb,
  year = {2017},
  keywords = {Computer Science - Data Structures and Algorithms,F.2.2},
  file = {/Users/benblamey/Zotero/storage/EB3RUDAX/Hoefer and Kodric - 2017 - Combinatorial Secretary Problems with Ordinal Info.pdf;/Users/benblamey/Zotero/storage/F4XHYVU7/1702.html}
}

@article{fiatTempSecretaryProblem2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.01732},
  primaryClass = {cs},
  title = {The {{Temp Secretary Problem}}},
  abstract = {We consider a generalization of the secretary problem where contracts are temporary, and for a fixed duration $\gamma$. This models online hiring of temporary employees, or online auctions for re-usable resources. The problem is related to the question of finding a large independent set in a random unit interval graph.},
  language = {en},
  journal = {arXiv:1507.01732 [cs]},
  author = {Fiat, Amos and Gorelik, Ilia and Kaplan, Haim and Novgorodov, Slava},
  month = jul,
  year = {2015},
  keywords = {Computer Science - Data Structures and Algorithms,F.2.2,68Q25},
  file = {/Users/benblamey/Zotero/storage/KJ6NEXKV/Fiat et al. - 2015 - The Temp Secretary Problem.pdf}
}

@article{gueltonPythranEnablingStatic2015,
  title = {Pythran: Enabling Static Optimization of Scientific {{Python}} Programs},
  volume = {8},
  issn = {1749-4699},
  shorttitle = {Pythran},
  doi = {10.1088/1749-4680/8/1/014001},
  abstract = {Pythran is a young open source static compiler that turns modules written in a subset of Python into native ones. Based on the fact that scientific modules do not rely much on the dynamic features of the language, it trades them in favor of powerful, eventually inter procedural, optimizations. These include detection of pure functions, temporary allocation removal, constant folding, Numpy ufunc fusion and parallelization, explicit thread-level parallelism through OpenMP annotations, false variable polymorphism pruning, and automatic vector instruction generation such as AVX or SSE.},
  language = {en},
  number = {1},
  journal = {Computational Science \& Discovery},
  author = {Guelton, Serge and Brunet, Pierrick and Amini, Mehdi and Merlini, Adrien and Corbillon, Xavier and Raynaud, Alan},
  month = mar,
  year = {2015},
  pages = {014001},
  file = {/Users/benblamey/Zotero/storage/6IQXGE44/Guelton et al. - 2015 - Pythran enabling static optimization of scientifi.pdf}
}

@misc{KeynotesHighPerformance,
  title = {Keynotes: {{High Performance Computing}} and {{Big Data}}: {{Challenges}} for the {{Future}} | {{CCGRID}} 2018 | {{GW}} | 18th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}} 2018},
  howpublished = {https://ccgrid2018.seas.gwu.edu/keynotes-jack-dongarra/},
  file = {/Users/benblamey/Zotero/storage/EAIVD7ME/Keynotes High Performance Computing and Big Data.pdf;/Users/benblamey/Zotero/storage/RKKD8P7J/keynotes-jack-dongarra.html}
}

@article{dynkin1963optimum,
  title = {The Optimum Choice of the Instant for Stopping a {{Markov}} Process},
  volume = {4},
  journal = {Soviet Mathematics},
  author = {Dynkin, Eugene B},
  year = {1963},
  pages = {627-629}
}

@article{blameyApacheSparkStreaming2018a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.07724},
  primaryClass = {cs},
  title = {Apache {{Spark Streaming}} and {{HarmonicIO}}: {{A Performance}} and {{Architecture Comparison}}},
  shorttitle = {Apache {{Spark Streaming}} and {{HarmonicIO}}},
  abstract = {Studies have demonstrated that Apache Spark, Flink and related frameworks can perform stream processing at very high frequencies, whilst tending to focus on small messages with a computationally light ‘map' stage for each message; a common enterprise use case. We add to these benchmarks by broadening the domain to include loads with larger messages (leading to network-bound throughput), and that are computationally intensive (leading to CPU-bound throughput) in the map phase; in order to evaluate applicability of these frameworks to scientific computing applications. We present a performance benchmark comparison between Apache Spark Streaming (ASS) under both file and TCP streaming modes; and HarmonicIO, comparing maximum throughput over a broad domain of message sizes and CPU loads. We find that relative performance varies considerably across this domain, with the chosen means of stream source integration having a big impact. We offer recommendations for choosing and configuring the frameworks, and present a benchmarking toolset developed for this study.},
  journal = {arXiv:1807.07724 [cs]},
  author = {Blamey, Ben and Hellander, Andreas and Toor, Salman},
  month = jul,
  year = {2018},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/Users/benblamey/Zotero/storage/T4WJLUPD/Blamey et al. - 2018 - Apache Spark Streaming and HarmonicIO A Performan.pdf;/Users/benblamey/Zotero/storage/5VV9NF8Y/1807.html}
}

@article{freeman1983secretary,
  title = {The Secretary Problem and Its Extensions: {{A}} Review},
  journal = {International Statistical Review/Revue Internationale de Statistique},
  author = {Freeman, PR},
  year = {1983},
  pages = {189-206},
  publisher = {{JSTOR}}
}

@article{liuCachingWirelessEdge2016,
  title = {Caching at the Wireless Edge: Design Aspects, Challenges, and Future Directions},
  volume = {54},
  issn = {0163-6804},
  shorttitle = {Caching at the Wireless Edge},
  doi = {10.1109/MCOM.2016.7565183},
  abstract = {Caching at the wireless edge is a promising way to boost spectral efficiency and reduce energy consumption of wireless systems. These improvements are rooted in the fact that popular contents are reused, asynchronously, by many users. In this article we first introduce methods to predict the popularity distributions and user preferences, and the impact of erroneous information. We then discuss the two aspects of caching systems, content placement and delivery. We expound the key differences between wired and wireless caching, and outline the differences in the system arising from where the caching takes place (e.g., at base stations or on the wireless devices themselves). Special attention is paid to the essential limitations in wireless caching, and possible trade-offs between spectral efficiency, energy efficiency, and cache size.},
  number = {9},
  journal = {IEEE Communications Magazine},
  author = {Liu, D. and Chen, B. and Yang, C. and Molisch, A. F.},
  month = sep,
  year = {2016},
  keywords = {Streaming media,Mobile communication,Throughput,cache size,cache storage,caching system,design aspect,Device-to-device communication,energy consumption reduction,energy efficiency,erroneous information,Mobile computing,popularity distribution,radiocommunication,spectral efficiency,telecommunication power management,wireless edge,Wireless networks,wireless system},
  pages = {22-28},
  file = {/Users/benblamey/Zotero/storage/H6ZJLLNH/Liu et al. - 2016 - Caching at the wireless edge design aspects, chal.pdf;/Users/benblamey/Zotero/storage/IAAZIKT4/7565183.html}
}

@inproceedings{krishEfficientHierarchicalStorage2016,
  title = {On {{Efficient Hierarchical Storage}} for {{Big Data Processing}}},
  doi = {10.1109/CCGrid.2016.61},
  abstract = {A promising trend in storage management for big data frameworks, such as Hadoop and Spark, is the emergence of heterogeneous and hybrid storage systems that employ different types of storage devices, e.g. SSDs, RAMDisks, etc., alongside traditional HDDs. However, scheduling data accesses or requests to an appropriate storage device is non-trivial and depends on several factors such as data locality, device performance, and application compute and storage resources utilization. To this end, we present DUX, an application-attuned dynamic data management system for data processing frameworks, which aims to improve overall application I/O throughput by efficiently using SSDs only for workloads that are expected to benefit from them rather than the extant approach of storing a fraction of the overall workloads in SSDs. The novelty of DUX lies in profiling application performance on SSDs and HDDs, analyzing the resulting I/O behavior, and considering the available SSDs at runtime to dynamically place data in an appropriate storage tier. Evaluation of DUX with trace-driven simulations using synthetic Facebook workloads shows that even when using 5.5\texttimes{} fewer SSDs compared to a SSD-only solution, DUX incurs only a small (5\%) performance overhead, and thus offers an affordable and efficient storage tier management.},
  booktitle = {2016 16th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}} ({{CCGrid}})},
  author = {Krish, K. R. and Wadhwa, B. and Iqbal, M. S. and Rafique, M. M. and Butt, A. R.},
  month = may,
  year = {2016},
  keywords = {parallel processing,resource allocation,Big Data,Big data,Hadoop,MapReduce,Market research,Spark,scheduling,application I/O throughput,application performance profiling,application-attuned dynamic data management system,Bandwidth,big data processing,data-intensive computing,DUX,Facebook,heterogeneous storage,heterogeneous storage systems,hierarchical storage,hybrid storage systems,I/O behavior,Performance evaluation,performance prediction,Prefetching,RAMDisk,scheduling data access,scheduling data requests,SSD-only solution,storage devices,storage management,Storage management,storage resource utilization,storage tier management,synthetic Facebook workloads,tiered storage,trace-driven simulations},
  pages = {403-408},
  file = {/Users/benblamey/Zotero/storage/WJULUQ9J/Krish et al. - 2016 - On Efficient Hierarchical Storage for Big Data Pro.pdf;/Users/benblamey/Zotero/storage/WKTHF6Z9/7515715.html}
}

@article{niuHybridStorageSystems2018,
  title = {Hybrid {{Storage Systems}}: {{A Survey}} of {{Architectures}} and {{Algorithms}}},
  volume = {6},
  issn = {2169-3536},
  shorttitle = {Hybrid {{Storage Systems}}},
  doi = {10.1109/ACCESS.2018.2803302},
  abstract = {Data center storage architectures face rapidly increasing demands for data volume and quality of service requirements today. Hybrid storage systems have turned out to be the one of the most popular choices in fulfilling these demands. A mixture of various types of storage devices and structures enables architects to address performance and capacity concerns of users within one storage infrastructure. In this paper, we present an extensive literature review on the state-of-the-art research for hybrid storage systems. First, different types of hybrid storage architectures are explored and categorized thoroughly. Second, the corresponding algorithms and policies, such as caching, scheduling, resource allocation and so on, are discussed profoundly. Finally, the advantages and disadvantages of these hybrid storage architectures are compared and analyzed intensively, in terms of system performance, solid state drive lifespan, energy consumption, and so on, in order to motivate some future research directions.},
  journal = {IEEE Access},
  author = {Niu, J. and Xu, J. and Xie, L.},
  year = {2018},
  keywords = {resource allocation,Data centers,cache storage,hybrid storage systems,Performance evaluation,storage devices,storage management,Caching algorithms,computer centres,data center storage architectures,data migration,data volume,hot data identification,hybrid storage architectures,hybrid storage system,Nonvolatile memory,Quality of service,Random access memory,Resource management,storage infrastructure,system performance,System performance},
  pages = {13385-13406},
  file = {/Users/benblamey/Zotero/storage/IZ2C2WKG/Niu et al. - 2018 - Hybrid Storage Systems A Survey of Architectures .pdf;/Users/benblamey/Zotero/storage/RZ4YA8HJ/8283744.html}
}

@inproceedings{dealBudgetTransferLowCost2018,
  title = {Budget-{{Transfer}}: {{A Low Cost Inter}}-{{Service Data Storage}} and {{Transfer Scheme}}},
  shorttitle = {Budget-{{Transfer}}},
  doi = {10.1109/BigDataCongress.2018.00022},
  abstract = {With the offerings of compelling cloud storage services from various cloud service providers, numerous web and mobile applications are leveraging cloud to store data for long-term usage. In this paper, we propose Budget-Transfer, a unique scheme to reduce the long-term cost of storing large data sets using cloud storage services. In contrast to most existing works, we study the storage cost-minimization problem by leveraging various available storage services that can provide different levels of performance at different pricing cost, under the constraint of data-access performance requirement. The key idea of Budget-Transfer is to continually transfer large data sets between different cloud storage services so as to satisfy performance requirements while avoiding overpaying for unnecessarily high performance guarantees. Budget-Transfer selects which service to use for each request with a goal towards minimizing the overall storage cost, rather than selecting whichever would be the locally cheapest service. Thus, the accumulative data-transfer and data-storage cost over a long period of time to satisfy a sequence of data-access requests can be reduced for the system. Simulation results show that Budget-Transfer performs well under various system parameters and request patterns, and can significantly reduce costs compared to other schemes.},
  booktitle = {2018 {{IEEE International Congress}} on {{Big Data}} ({{BigData Congress}})},
  author = {Deal, G. and Peng, Y. and Qin, H.},
  month = jul,
  year = {2018},
  keywords = {cloud computing,Cloud computing,Data models,information retrieval,storage management,accumulative data-transfer,available storage services,Budget-Transfer,cloud service providers,cloud storage services,data storage,data transfer,Data transfer,data-access performance requirement,data-access requests,inter-service,low cost,low cost inter-service data storage,Memory,Pricing,Redundancy,Silicon,storage cost-minimization problem,Transfer scheme},
  pages = {112-119},
  file = {/Users/benblamey/Zotero/storage/E8E4QDAY/Deal et al. - 2018 - Budget-Transfer A Low Cost Inter-Service Data Sto.pdf;/Users/benblamey/Zotero/storage/AVIPB492/8457738.html}
}

@article{shankarNumpywrenServerlessLinear2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.09679},
  primaryClass = {cs},
  title = {Numpywren: Serverless Linear Algebra},
  shorttitle = {Numpywren},
  abstract = {Linear algebra operations are widely used in scientific computing and machine learning applications. However, it is challenging for scientists and data analysts to run linear algebra at scales beyond a single machine. Traditional approaches either require access to supercomputing clusters, or impose configuration and cluster management challenges. In this paper we show how the disaggregation of storage and compute resources in so-called "serverless" environments, combined with compute-intensive workload characteristics, can be exploited to achieve elastic scalability and ease of management. We present numpywren, a system for linear algebra built on a serverless architecture. We also introduce LAmbdaPACK, a domain-specific language designed to implement highly parallel linear algebra algorithms in a serverless setting. We show that, for certain linear algebra algorithms such as matrix multiply, singular value decomposition, and Cholesky decomposition, numpywren's performance (completion time) is within 33\% of ScaLAPACK, and its compute efficiency (total CPU-hours) is up to 240\% better due to elasticity, while providing an easier to use interface and better fault tolerance. At the same time, we show that the inability of serverless runtimes to exploit locality across the cores in a machine fundamentally limits their network efficiency, which limits performance on other algorithms such as QR factorization. This highlights how cloud providers could better support these types of computations through small changes in their infrastructure.},
  journal = {arXiv:1810.09679 [cs]},
  author = {Shankar, Vaishaal and Krauth, Karl and Pu, Qifan and Jonas, Eric and Venkataraman, Shivaram and Stoica, Ion and Recht, Benjamin and {Ragan-Kelley}, Jonathan},
  month = oct,
  year = {2018},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/Users/benblamey/Zotero/storage/F6C562TG/Shankar et al. - 2018 - numpywren serverless linear algebra.pdf;/Users/benblamey/Zotero/storage/VDDUKICW/1810.html}
}

@inproceedings{hsuNovelAutomatedCloud2018,
  title = {A {{Novel Automated Cloud Storage Tiering System}} through {{Hot}}-{{Cold Data Classification}}},
  isbn = {978-1-5386-7235-8},
  doi = {10.1109/CLOUD.2018.00069},
  abstract = {With information technology growing and rapidly increasing ICT equipment, a massive amount of data have been generated and stored in the cloud. However, the majority of them are infrequently accessed data. Data temperature describes the frequency of data access. Hot storage is dedicated to storing frequently accessed data while cold storage is designed for infrequently accessed data. To cope with the issue of exponential data growth in cloud, it is essential to allocate different categories of data to proper storage media. In this research, we propose an automated cloud storage tiering system for the task of data temperature prediction through hot-cold data classification and data migration, which allocates the predicted categorized data to the corresponding storage media. There are three major contributions in this paper. Firstly, the feasibility: by successfully predicting the infrequent access data and moving them to the cold storage, we obtain significant cost savings. Secondly, the reliability: while having the benefit of storage-cost saving, our proposed system also ensures customers satisfaction by enhancing the ratio of data access through hot storage. Lastly, the flexibility: operational strategy varies from cloud storage service providers. Our system characterizes different scenarios and provides the customized optimal solution.},
  booktitle = {2018 {{IEEE}} 11th {{International Conference}} on {{Cloud Computing}} ({{CLOUD}})},
  author = {Hsu, Y. and Irie, R. and Murata, S. and Matsuoka, M.},
  month = jul,
  year = {2018},
  keywords = {cloud computing,storage management,customer satisfaction,pattern classification},
  pages = {492-499},
  file = {/Users/benblamey/Zotero/storage/YZFGKJI4/723501a492-abs.html}
}

@inproceedings{mansouriCostOptimizationAlgorithms2018,
  address = {San Francisco, CA, USA},
  title = {Cost {{Optimization Algorithms}} for {{Hot}} and {{Cool Tiers Cloud Storage Services}}},
  isbn = {978-1-5386-7235-8},
  doi = {10.1109/CLOUD.2018.00086},
  booktitle = {2018 {{IEEE}} 11th {{International Conference}} on {{Cloud Computing}} ({{CLOUD}})},
  publisher = {{IEEE}},
  author = {Mansouri, Yaser and Erradi, Abdelkarim},
  month = jul,
  year = {2018},
  pages = {622-629}
}

@article{mansouriMoveNotMove2016,
  title = {To Move or Not to Move: {{Cost}} Optimization in a Dual Cloud-Based Storage Architecture},
  volume = {75},
  issn = {1084-8045},
  shorttitle = {To Move or Not to Move},
  doi = {10.1016/j.jnca.2016.08.029},
  abstract = {IT enterprises have recently witnessed a dramatic increase in data volume and faced with challenges of storing and retrieving their data. Thanks to the fact that cloud infrastructures offer storage and network resources in several geographically dispersed data centers (DCs), data can be stored and shared in scalable and highly available manner with little or no capital investment. Due to diversity of pricing options and variety of storage and network resources offered by cloud providers, enterprises encounter nontrivial choice of what combination of storage options should be used in order to minimize the monetary cost of managing data in large volumes. To minimize the cost of data storage management in the cloud, we propose two data object placement algorithms, one optimal and another near optimal, that minimize residential (i.e., storage, data access operations), delay, and potential migration costs in a dual cloud-based storage architecture (i.e., the combination of a temporal and a backup DC). We evaluate our algorithms using real-world traces from Twitter. Results confirm the importance and effectiveness of the proposed algorithms and highlight the benefits of leveraging pricing differences and data migration across cloud storage providers (CSPs).},
  journal = {Journal of Network and Computer Applications},
  author = {Mansouri, Yaser and Buyya, Rajkumar},
  month = nov,
  year = {2016},
  keywords = {Cloud storage,Cost optimization,Data storage management,Dual cloud-based storage architecture},
  pages = {223-235},
  file = {/Users/benblamey/Zotero/storage/2SWA8JNT/Mansouri and Buyya - 2016 - To move or not to move Cost optimization in a dua.pdf;/Users/benblamey/Zotero/storage/BSV7SNNU/S1084804516301977.html}
}

@article{mansouriCostOptimizationDynamic2017,
  title = {Cost {{Optimization}} for {{Dynamic Replication}} and {{Migration}} of {{Data}} in {{Cloud Data Centers}}},
  issn = {2168-7161},
  doi = {10.1109/TCC.2017.2659728},
  journal = {IEEE Transactions on Cloud Computing},
  author = {Mansouri, Yaser and Nadjaran Toosi, Adel and Buyya, Rajkumar},
  year = {2017},
  pages = {1-1}
}

@misc{COPSCostBased2017,
  title = {{{COPS}}: {{Cost Based Object Placement Strategies}} on {{Hybrid Storage System}} for {{DBaaS Cloud}} - {{IEEE Conference Publication}}},
  howpublished = {https://ieeexplore.ieee.org/document/7973754},
  year = {2017},
  file = {/Users/benblamey/Zotero/storage/K3G8NCWM/7973754.html}
}

@inproceedings{reissEnablingRealTimeQuerying2007,
  address = {Bamff, AB, Canada},
  title = {Enabling {{Real}}-{{Time Querying}} of {{Live}} and {{Historical Stream Data}}},
  isbn = {978-0-7695-2868-7},
  doi = {10.1109/SSDBM.2007.34},
  abstract = {Applications that query data streams in order to identify trends, patterns, or anomalies can often benefit from comparing the live stream data with archived historical stream data. However, searching this historical data in real time has been considered so far to be prohibitively expensive. One of the main bottlenecks is the update costs of the indices over the archived data. In this paper, we address this problem by using our highly-efficient bitmap indexing technology (called FastBit) and demonstrate that the index update operations are sufficiently efficient for this bottleneck to be removed. We describe our prototype system based on the TelegraphCQ streaming query processor and the FastBit bitmap index. We present a detailed performance evaluation of our system using a complex query workload for analyzing real network traffic data. The combined system uses TelegraphCQ to analyze streams of traffic information and FastBit to correlate current behaviors with historical trends. We demonstrate that our system can simultaneously analyze (1) live streams with high data rates and (2) a large repository of historical stream data.},
  language = {en},
  booktitle = {19th {{International Conference}} on {{Scientific}} and {{Statistical Database Management}} ({{SSDBM}} 2007)},
  publisher = {{IEEE}},
  author = {Reiss, Frederick and Stockinger, Kurt and Wu, Kesheng and Shoshani, Arie and Hellerstein, Joseph M.},
  month = jul,
  year = {2007},
  pages = {28-28},
  file = {/Users/benblamey/Zotero/storage/22WM5TP7/Reiss et al. - 2007 - Enabling Real-Time Querying of Live and Historical.pdf}
}

@book{lamMiningTopKFrequent2010,
  title = {Mining {{Top}}-{{K Frequent Items}} in a {{Data Stream}} with {{Flexible Sliding Windows}}},
  abstract = {We study the problem of finding the k most frequent items in a stream of items for the recently proposed max-frequency measure. Based on the properties of an item, the maxfrequency of an item is counted over a sliding window of which the length changes dynamically. Besides being parameterless, this way of measuring the support of items was shown to have the advantage of a faster detection of bursts in a stream, especially if the set of items is heterogeneous. The algorithm that was proposed for maintaining all frequent items, however, scales poorly when the number of items becomes large. Therefore, in this paper we propose, instead of reporting all frequent items, to only mine the top-k most frequent ones. First we prove that in order to solve this problem exactly, we still need a prohibitive amount of memory (at least linear in the number of items). Yet, under some reasonable conditions, we show both theoretically and empirically that a memory-efficient algorithm exists. A prototype of this algorithm is implemented and we present its performance w.r.t. memory-efficiency on real-life data and in controlled experiments with synthetic data.},
  author = {Lam, Hoang Thanh and Calders, Toon},
  year = {2010},
  file = {/Users/benblamey/Zotero/storage/3J5MXLWR/Lam and Calders - 2010 - Mining Top-K Frequent Items in a Data Stream with .pdf;/Users/benblamey/Zotero/storage/5CVNU8NS/summary.html}
}

@book{ilyasSurveyTopkQuery,
  title = {A {{Survey}} of {{Top}}-k {{Query Processing Techniques}} in {{Relational Database Systems}}},
  abstract = {Efficient processing of top-k queries is a crucial requirement in many interactive environments that involve massive amounts of data. In particular, efficient top-k processing in domains such as the Web, multimedia search and distributed systems has shown a great impact on performance. In this survey, we describe and classify top-k processing techniques in relational databases. We discuss different design dimensions in the current techniques including query models, data access methods, implementation levels, data and query certainty, and supported scoring functions. We show the implications of each dimension on the design of the underlying techniques. We also discuss top-k queries in XML domain, and show their connections to relational approaches.},
  author = {Ilyas, Ihab F. and Beskales, George and Soliman, Mohamed A.},
  file = {/Users/benblamey/Zotero/storage/CAQ94RZZ/Ilyas et al. - A Survey of Top-k Query Processing Techniques in R.pdf;/Users/benblamey/Zotero/storage/L5LJEA9C/summary.html}
}

@article{weikumSpecialIssueSelfTuning,
  title = {Special {{Issue}} on {{Self}}-{{Tuning Databases}} and {{Application Tuning}}},
  language = {en},
  author = {Weikum, Gerhard and Christian, Arnd and Kraiss, Achim and Sinnwell, Markus},
  pages = {53},
  file = {/Users/benblamey/Zotero/storage/PNN9V7JT/Weikum et al. - Special Issue on Self-Tuning Databases and Applica.pdf}
}

@book{sapinoLoganalysisBasedCharacterization2006,
  title = {Log-Analysis Based Characterization of Multimedia Documents for Effective Delivery of Distributed Multimedia Presentations. {{DMS}}},
  abstract = {In a distributed environment, media files have to be downloaded, coordinated, and presented to the clients, according to the specification given by the authors of multimedia documents. We present a server-side logO middleware which learns the structure of a multimedia presentation, without accessing the author's specification, and constructs an automaton which captures the dynamic aspects of the evolution of the document, relevant for the optimization of the presentation delivery, prefetching, and scheduling. 1},
  author = {Sapino, Maria Luisa and Torino, Universit\`a Di},
  year = {2006},
  file = {/Users/benblamey/Zotero/storage/3DW8CHSE/Sapino and Torino - 2006 - Log-analysis based characterization of multimedia .pdf;/Users/benblamey/Zotero/storage/4JMTGBV4/summary.html}
}

@article{kraissIntegratedDocumentCaching1998,
  title = {Integrated Document Caching and Prefetching in Storage Hierarchies Based on {{Markov}}-Chain Predictions},
  volume = {7},
  issn = {0949-877X},
  doi = {10.1007/s007780050060},
  abstract = {. Large multimedia document archives may hold a major fraction of their data in tertiary storage libraries for cost reasons. This paper develops an integrated approach to the vertical data migration between the tertiary, secondary, and primary storage in that it reconciles speculative prefetching, to mask the high latency of the tertiary storage, with the replacement policy of the document caches at the secondary and primary storage level, and also considers the interaction of these policies with the tertiary and secondary storage request scheduling. The integrated migration policy is based on a continuous-time Markov chain model for predicting the expected number of accesses to a document within a specified time horizon. Prefetching is initiated only if that expectation is higher than those of the documents that need to be dropped from secondary storage to free up the necessary space. In addition, the possible resource contention at the tertiary and secondary storage is taken into account by dynamically assessing the response-time benefit of prefetching a document versus the penalty that it would incur on the response time of the pending document requests. The parameters of the continuous-time Markov chain model, the probabilities of co-accessing certain documents and the interaction times between successive accesses, are dynamically estimated and adjusted to evolving workload patterns by keeping online statistics. The integrated policy for vertical data migration has been implemented in a prototype system. The system makes profitable use of the Markov chain model also for the scheduling of volume exchanges in the tertiary storage library. Detailed simulation experiments with Web-server-like synthetic workloads indicate significant gains in terms of client response time. The experiments also show that the overhead of the statistical bookkeeping and the computations for the access predictions is affordable.},
  language = {en},
  number = {3},
  journal = {The VLDB Journal},
  author = {Kraiss, Achim and Weikum, Gerhard},
  month = aug,
  year = {1998},
  keywords = {Key words:Performance – Caching – Prefetching – Scheduling – Tertiary storage – Stochastic modeling – Markov chains},
  pages = {141-162},
  file = {/Users/benblamey/Zotero/storage/LQR9SWAQ/Kraiss and Weikum - 1998 - Integrated document caching and prefetching in sto.pdf}
}

@article{knesslDynamicPriorityQueue2003,
  title = {A {{Dynamic Priority Queue Model}} for {{Simultaneous Service}} of {{Two Traffic Types}}},
  volume = {63},
  issn = {0036-1399, 1095-712X},
  doi = {10.1137/S0036139901390842},
  abstract = {We consider a priority queue with a dynamic, queue-length-threshold scheduling policy. Customers are classed into two types (type-1 and type-2), and the service order of the two classes depends on the queue length of the type-1 queue. The high priority (type-2) class (e.g., voice) is served until the low priority (e.g., data) queue exceeds the threshold L, at which time service is given to the low priority class until its queue length decreases to L. The arrivals of the two classes follow independent Poisson processes, and the service time of each customer has an exponential distribution with parameter $\mathrm{\mu}$. We derive the balance equations in the steady state, and explicitly obtain the joint probability generating function for the queue lengths of the two customer classes. This gives the joint queue length distribution as an integral. We then obtain detailed asymptotic results for the joint distribution. In particular, we study the tail behavior. We also discuss heavy traffic diffusion approximations for this model.},
  language = {en},
  number = {2},
  journal = {SIAM Journal on Applied Mathematics},
  author = {Knessl, Charles and Tier, Charles and Il Choi, Doo},
  month = jan,
  year = {2003},
  pages = {398-422},
  file = {/Users/benblamey/Zotero/storage/2F8CCL9T/Knessl et al. - 2003 - A Dynamic Priority Queue Model for Simultaneous Se.pdf}
}

@inproceedings{khanaferConstrainedSkiRentalProblem2013,
  title = {The Constrained {{Ski}}-{{Rental}} Problem and Its Application to Online Cloud Cost Optimization},
  doi = {10.1109/INFCOM.2013.6566944},
  abstract = {Cloud service providers (CSPs) enable tenants to elastically scale their resources to meet their demands. In fact, there are various types of resources offered at various price points. While running applications on the cloud, a tenant aiming to minimize cost is often faced with crucial trade-off considerations. For instance, upon each arrival of a query, a web application can either choose to pay for CPU to compute the response fresh, or pay for cache storage to store the response so as to reduce the compute costs of future requests. The SkiRental problem abstracts such scenarios where a tenant is faced with a to-rent-or-to-buy trade-off; in its basic form, a skier should choose between renting or buying a set of skis without knowing the number of days she will be skiing. In this paper, we introduce a variant of the classical SkiRental problem in which we assume that the skier knows the first (or second) moment of the distribution of the number of ski days in a season. We demonstrate that utilizing this information leads to achieving the best worst-case expected competitive ratio (CR) performance. Our method yields a new class of randomized algorithms that provide arrivals-distribution-free performance guarantees. Further, we apply our solution to a cloud file system and demonstrate the cost savings obtained in comparison to other competing schemes. Simulations illustrate that our scheme exhibits robust average-cost performance that combines the best of the well-known deterministic and randomized schemes previously proposed to tackle the Ski-Rental problem.},
  booktitle = {2013 {{Proceedings IEEE INFOCOM}}},
  author = {Khanafer, A. and Kodialam, M. and Puttaswamy, K. P. N.},
  month = apr,
  year = {2013},
  keywords = {cloud computing,Algorithm design and analysis,Optimization,cache storage,cloud service providers,arrival-distribution-free performance guarantees,Cache storage,cloud file system,constrained ski-rental problem,cost minimization,cost reduction,cost savings,CPU,CSP,deterministic algorithms,deterministic schemes,first-moment-constrained ski-rental problem,Game theory,Games,online cloud cost optimization,optimisation,pricing,query arrival,randomised algorithms,randomized algorithms,response computation,response storage,robust average-cost performance,second-moment-constrained ski-rental problem,ski buying,Snow,Standards,to-rent-or-to-buy trade-off,Web application,worst-case expected competitive ratio performance,worst-case expected CR performance},
  pages = {1492-1500},
  file = {/Users/benblamey/Zotero/storage/3RPTI723/references.html}
}

@inproceedings{khanaferConstrainedSkiRentalProblem2013a,
  title = {The Constrained {{Ski}}-{{Rental}} Problem and Its Application to Online Cloud Cost Optimization},
  doi = {10.1109/INFCOM.2013.6566944},
  abstract = {Cloud service providers (CSPs) enable tenants to elastically scale their resources to meet their demands. In fact, there are various types of resources offered at various price points. While running applications on the cloud, a tenant aiming to minimize cost is often faced with crucial trade-off considerations. For instance, upon each arrival of a query, a web application can either choose to pay for CPU to compute the response fresh, or pay for cache storage to store the response so as to reduce the compute costs of future requests. The SkiRental problem abstracts such scenarios where a tenant is faced with a to-rent-or-to-buy trade-off; in its basic form, a skier should choose between renting or buying a set of skis without knowing the number of days she will be skiing. In this paper, we introduce a variant of the classical SkiRental problem in which we assume that the skier knows the first (or second) moment of the distribution of the number of ski days in a season. We demonstrate that utilizing this information leads to achieving the best worst-case expected competitive ratio (CR) performance. Our method yields a new class of randomized algorithms that provide arrivals-distribution-free performance guarantees. Further, we apply our solution to a cloud file system and demonstrate the cost savings obtained in comparison to other competing schemes. Simulations illustrate that our scheme exhibits robust average-cost performance that combines the best of the well-known deterministic and randomized schemes previously proposed to tackle the Ski-Rental problem.},
  booktitle = {2013 {{Proceedings IEEE INFOCOM}}},
  author = {Khanafer, A. and Kodialam, M. and Puttaswamy, K. P. N.},
  month = apr,
  year = {2013},
  keywords = {cloud computing,Algorithm design and analysis,Optimization,cache storage,cloud service providers,arrival-distribution-free performance guarantees,Cache storage,cloud file system,constrained ski-rental problem,cost minimization,cost reduction,cost savings,CPU,CSP,deterministic algorithms,deterministic schemes,first-moment-constrained ski-rental problem,Game theory,Games,online cloud cost optimization,optimisation,pricing,query arrival,randomised algorithms,randomized algorithms,response computation,response storage,robust average-cost performance,second-moment-constrained ski-rental problem,ski buying,Snow,Standards,to-rent-or-to-buy trade-off,Web application,worst-case expected competitive ratio performance,worst-case expected CR performance},
  pages = {1492-1500},
  file = {/Users/benblamey/Zotero/storage/A5G8SAA9/Khanafer et al. - 2013 - The constrained Ski-Rental problem and its applica.pdf;/Users/benblamey/Zotero/storage/SCAZQ3BW/Khanafer et al. - 2013 - The constrained Ski-Rental problem and its applica.pdf;/Users/benblamey/Zotero/storage/9J5XAPLR/6566944.html}
}

@inproceedings{karlinCompetitiveRandomizedAlgorithms1990,
  title = {Competitive Randomized Algorithms for Non-Uniform Problems},
  booktitle = {Proceedings of the First Annual {{ACM}}-{{SIAM}} Symposium on {{Discrete}} Algorithms},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Karlin, Anna R. and Manasse, Mark S. and McGeoch, Lyle A. and Owicki, Susan},
  year = {1990},
  pages = {301--309},
  file = {/Users/benblamey/Zotero/storage/48LCQ9T7/Karlin et al. - 1990 - Competitive randomized algorithms for non-uniform .pdf}
}

@inproceedings{tatbulLoadSheddingData2003,
  title = {Load Shedding in a Data Stream Manager},
  booktitle = {Proceedings of the 29th International Conference on {{Very}} Large Data Bases-{{Volume}} 29},
  publisher = {{VLDB Endowment}},
  author = {Tatbul, Nesime and {\c C}etintemel, U{\u g}ur and Zdonik, Stan and Cherniack, Mitch and Stonebraker, Michael},
  year = {2003},
  pages = {309--320},
  file = {/Users/benblamey/Zotero/storage/YFKNF4AI/S10P03.pdf;/Users/benblamey/Zotero/storage/J9QL6DQ8/citation.html}
}

@article{wredeSmartComputationalExploration2018,
  title = {Smart Computational Exploration of Stochastic Gene Regulatory Network Models Using Human-in-the-Loop Semi-Supervised Learning},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  doi = {10.1101/490623},
  abstract = {Discrete stochastic models of gene regulatory network models are indispensable tools for biological inquiry since they allow the modeler to predict how molecular interactions give rise to nonlinear system output. Model exploration with the objective of generating qualitative hypotheses about the workings of a pathway is usually the first step in the modeling process. It involves simulating the gene network model under a very large range of conditions, due to the the large uncertainty in interactions and kinetic parameters. This makes model exploration highly computational demanding. Furthermore, with no prior information about the model behavior, labour-intensive manual inspection of very large amounts of simulation results becomes necessary. This limits systematic computational exploration to simplistic models. We address this by developing an interactive, smart workflow for model exploration based on semi-supervised learning and human-in-the-loop labeling of data. The proposed methodology lets a modeler rapidly discover ranges of interesting behaviors predicted by a model. Utilizing the notion that similar simulation output is in proximity of each other in feature space, the modeler can focus on informing the system about what behaviors are more interesting than others instead of configuring and analyzing simulation results. This large reduction in time-consuming manual work by the modeler early in a modeling project can substantially reduce the time needed to go from an initial model to testable predictions and downstream analysis.},
  language = {en},
  journal = {bioRxiv},
  author = {Wrede, Fredrik and Hellander, Andreas},
  month = dec,
  year = {2018},
  pages = {490623},
  file = {/Users/benblamey/Zotero/storage/GS77PDGF/Wrede and Hellander - 2018 - Smart computational exploration of stochastic gene.pdf;/Users/benblamey/Zotero/storage/8UK6BT7Z/490623.html}
}

@inproceedings{puttaswamyFrugalStorageCloud2012,
  title = {Frugal Storage for Cloud File Systems},
  booktitle = {Proceedings of the 7th {{ACM}} European Conference on {{Computer Systems}}},
  publisher = {{ACM}},
  author = {Puttaswamy, Krishna PN and Nandagopal, Thyaga and Kodialam, Murali},
  year = {2012},
  pages = {71--84},
  file = {/Users/benblamey/Zotero/storage/Q28UGHDU/Puttaswamy et al. - 2012 - Frugal storage for cloud file systems.pdf;/Users/benblamey/Zotero/storage/7W4XESZS/citation.html}
}

@article{chanModelingDimensioningHierarchical2003,
  title = {Modeling and Dimensioning Hierarchical Storage Systems for Low-Delay Video Services},
  volume = {52},
  issn = {0018-9340},
  doi = {10.1109/TC.2003.1214339},
  abstract = {In order to cost-effectively accommodate a large number of titles in a video system, a hierarchical storage system can be used. In this system, not-so-popular video files are stored in a tertiary level such as a disk/tape library. These files are transferred, or "staged," to a secondary level composed of magnetic disks before being streamed to the users. This system overcomes the current limitations in using disk/tape libraries to stream videos and resolves the bandwidth difference between staging and streaming. We present, via analysis, a model of the system and determine the minimum storage and bandwidth required, at each level, to meet a given user delay goal. We also analyze a number of system operations pertaining to whether or not a file is played while it is being staged (i.e., stage-streaming) and whether or not the displayed segments are deleted (i.e., trail-deletion). We show that stage-streaming and trail-deletion can achieve substantially lower bandwidth and storage requirements. In order to further increase the streaming and storage scalability, a distributed storage system can be used where multiple local servers are put close to user pools and get their files from one of the libraries through a network. We extend the models developed to such a system and specify the resource requirements to meet a given delay goal.},
  number = {7},
  journal = {IEEE Transactions on Computers},
  author = {Chan, S.- G. and Tobagi, F. A.},
  month = jul,
  year = {2003},
  keywords = {storage management,distributed storage system,hierarchical storage systems,low-delay video services,magnetic disc storage,Magnetic disk recording,magnetic disks,memory architecture,Memory architecture,Memory management,resource requirements,stage-streaming,storage scalability,tape libraries,trail-deletion,user delay goal,video file storage,video on demand,video servers},
  pages = {907-919},
  file = {/Users/benblamey/Zotero/storage/ITRC7XLY/Chan and Tobagi - 2003 - Modeling and dimensioning hierarchical storage sys.pdf;/Users/benblamey/Zotero/storage/L592Q8UH/1214339.html}
}

@article{wilkesHPAutoRAIDHierarchical1996,
  title = {The {{HP AutoRAID}} Hierarchical Storage System},
  volume = {14},
  number = {1},
  journal = {ACM Transactions on Computer Systems (TOCS)},
  author = {Wilkes, John and Golding, Richard and Staelin, Carl and Sullivan, Tim},
  year = {1996},
  pages = {108--136},
  file = {/Users/benblamey/Zotero/storage/I325IUA6/citation.html;/Users/benblamey/Zotero/storage/NLL4CASG/citation.html}
}

@inproceedings{kakoulliOctopusFSDistributedFile2017,
  address = {New York, NY, USA},
  series = {SIGMOD '17},
  title = {{{OctopusFS}}: {{A Distributed File System}} with {{Tiered Storage Management}}},
  isbn = {978-1-4503-4197-4},
  shorttitle = {{{OctopusFS}}},
  doi = {10.1145/3035918.3064023},
  abstract = {The ever-growing data storage and I/O demands of modern large-scale data analytics are challenging the current distributed storage systems. A promising trend is to exploit the recent improvements in memory, storage media, and networks for sustaining high performance and low cost. While past work explores using memory or SSDs as local storage or combine local with network-attached storage in cluster computing, this work focuses on managing multiple storage tiers in a distributed setting. We present OctopusFS, a novel distributed file system that is aware of heterogeneous storage media (e.g., memory, SSDs, HDDs, NAS) with different capacities and performance characteristics. The system offers a variety of pluggable policies for automating data management across the storage tiers and cluster nodes. The policies employ multi-objective optimization techniques for making intelligent data management decisions based on the requirements of fault tolerance, data and load balancing, and throughput maximization. At the same time, the storage media are explicitly exposed to users and applications, allowing them to choose the distribution and placement of replicas in the cluster based on their own performance and fault tolerance requirements. Our extensive evaluation shows the immediate benefits of using OctopusFS with data-intensive processing systems, such as Hadoop and Spark, in terms of both increased performance and better cluster utilization.},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  publisher = {{ACM}},
  author = {Kakoulli, Elena and Herodotou, Herodotos},
  year = {2017},
  keywords = {distributed file system,tiered storage management},
  pages = {65--78},
  file = {/Users/benblamey/Zotero/storage/MUPLJIMJ/Kakoulli and Herodotou - 2017 - OctopusFS A Distributed File System with Tiered S.pdf}
}

@article{guerraCostEffectiveStorage2011,
  title = {Cost {{Effective Storage}} Using {{Extent Based Dynamic Tiering}}},
  abstract = {Multi-tier systems that combine SSDs with SAS/FC and/or SATA disks mitigate the capital cost burden of SSDs, while benefiting from their superior I/O performance per unit cost and low power. Though commercial SSD-based multi-tier solutions are available, configuring such a system with the optimal number of devices per tier to achieve performance goals at minimum cost remains a challenge. Furthermore, these solutions do not leverage the opportunity to dynamically consolidate load and reduce power/operating cost.},
  language = {en},
  author = {Guerra, Jorge and Pucha, Himabindu and Glider, Joseph and Belluomini, Wendy and Rangaswami, Raju},
  year = {2011},
  pages = {14},
  file = {/Users/benblamey/Zotero/storage/4S7IIZD9/Guerra et al. - Cost Effective Storage using Extent Based Dynamic .pdf}
}

@inproceedings{raghavanTieraFlexibleMultitiered2014,
  address = {Bordeaux, France},
  title = {Tiera: Towards Flexible Multi-Tiered Cloud Storage Instances},
  isbn = {978-1-4503-2785-5},
  shorttitle = {Tiera},
  doi = {10.1145/2663165.2663333},
  abstract = {Cloud providers offer an array of storage services that represent different points along the performance, cost, and durability spectrum. If an application desires the composite benefits of multiple storage tiers, then it must manage the complexity of different interfaces to these storage services and their diverse policies. We believe that it is possible to provide the benefits of customized tiered cloud storage to applications without compromising simplicity using a lightweight middleware. In this paper, we introduce Tiera, a middleware that enables the provision of multi-tiered cloud storage instances that are easy to specify, flexible, and enable a rich array of storage policies and desired metrics to be realized. Tiera's novelty lies in the first-class support for encapsulated tiered cloud storage, ease of programmability of data management policies, and support for runtime replacement and addition of policies and tiers. Tiera enables an application to realize a desired metric (e.g., low latency or low cost) by selecting different storage services that constitute a Tiera instance, and easily specifying a policy, using event and response pairs, to manage the life cycle of data stored in the instance. We illustrate the benefits of Tiera through a prototype implemented on the Amazon cloud. By deploying unmodified MySQL database engine and a TPC-W Web bookstore application on Tiera, we are able to improve their respective throughputs by 47\% - 125\% and 46\% - 69\%, over standard deployments. We further show the flexibility of Tiera in achieving different desired application metrics with minimal overhead.},
  language = {en},
  booktitle = {Proceedings of the 15th {{International Middleware Conference}} on - {{Middleware}} '14},
  publisher = {{ACM Press}},
  author = {Raghavan, Ajaykrishna and Chandra, Abhishek and Weissman, Jon B.},
  year = {2014},
  pages = {1-12},
  file = {/Users/benblamey/Zotero/storage/G7I3A7ZD/Raghavan et al. - 2014 - Tiera towards flexible multi-tiered cloud storage.pdf}
}

@inproceedings{caoEfficientTopKQuery2004,
  title = {Efficient {{Top}}-{{K Query Calculation}} in {{Distributed Networks}}},
  abstract = {This paper presents a new algorithm to answer top-k queries (e.g. ``find the k objects with the highest aggregate values'') in a distributed network. Existing algorithms such as the Threshold Algorithm [10] consume an excessive amount of bandwidth when the number of nodes, m, is high. We propose a new algorithm called ``Three-Phase Uniform Threshold'' (TPUT). TPUT reduces network bandwidth consumption by pruning away ineligible objects, and terminates in three round-trips regardless of data input. The paper presents two sets of results about TPUT. First, trace-driven simulations show that, depending on the size of the network, TPUT reduces network traffic by one to two orders of magnitude compared to existing algorithms. Second, TPUT is proven to be instance-optimal on common data series. In particular, analysis shows that by using a pruning parameter $\alpha{}<$1, TPUT achieves a qualitative reduction in network traffic, lowering the optimality ratio from O(m ${_\ast}$m) to O(m ${_\ast}$  $\surd$ m) for data series following Zipf distribution.},
  booktitle = {In {{PODC}}},
  author = {Cao, Pei},
  year = {2004},
  pages = {206--215},
  file = {/Users/benblamey/Zotero/storage/VMAPJPXV/Cao - 2004 - Efficient Top-K Query Calculation in Distributed N.pdf;/Users/benblamey/Zotero/storage/VPZUDVB8/summary.html}
}

@inproceedings{babcockDistributedTopkMonitoring2003,
  title = {Distributed Top-k Monitoring},
  booktitle = {Proceedings of the 2003 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  publisher = {{ACM}},
  author = {Babcock, Brian and Olston, Chris},
  year = {2003},
  pages = {28--39},
  file = {/Users/benblamey/Zotero/storage/FMQLUK34/Babcock and Olston - 2003 - Distributed top-k monitoring.pdf;/Users/benblamey/Zotero/storage/CJZ82PTC/citation.html}
}

@inproceedings{babcockDistributedTopkMonitoring2003a,
  title = {Distributed Top-k Monitoring},
  booktitle = {Proceedings of the 2003 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  publisher = {{ACM}},
  author = {Babcock, Brian and Olston, Chris},
  year = {2003},
  pages = {28--39},
  file = {/Users/benblamey/Zotero/storage/QU6TQN87/Babcock and Olston - 2003 - Distributed top-k monitoring.pdf;/Users/benblamey/Zotero/storage/5596XHAH/citation.html}
}

@inproceedings{charikarFindingFrequentItems2002,
  title = {Finding Frequent Items in Data Streams},
  booktitle = {International {{Colloquium}} on {{Automata}}, {{Languages}}, and {{Programming}}},
  publisher = {{Springer}},
  author = {Charikar, Moses and Chen, Kevin and {Farach-Colton}, Martin},
  year = {2002},
  pages = {693--703},
  file = {/Users/benblamey/Zotero/storage/5UBTHNYZ/Charikar et al. - 2002 - Finding frequent items in data streams.pdf;/Users/benblamey/Zotero/storage/QX8HDK5U/3-540-45465-9_59.html}
}

@inproceedings{caoEfficientTopkQuery2004,
  title = {Efficient Top-k Query Calculation in Distributed Networks},
  booktitle = {Proceedings of the Twenty-Third Annual {{ACM}} Symposium on {{Principles}} of Distributed Computing},
  publisher = {{ACM}},
  author = {Cao, Pei and Wang, Zhe},
  year = {2004},
  pages = {206--215},
  file = {/Users/benblamey/Zotero/storage/R265RJXK/Cao and Wang - 2004 - Efficient top-k query calculation in distributed n.pdf;/Users/benblamey/Zotero/storage/C7LCAB2Y/citation.html}
}

@article{stonebrakerRequirementsRealtimeStream2005a,
  title = {The 8 Requirements of Real-Time Stream Processing},
  volume = {34},
  issn = {01635808},
  doi = {10.1145/1107499.1107504},
  abstract = {Applications that require real-time processing of high-volume data steams are pushing the limits of traditional data processing infrastructures. These stream-based applications include market feed processing and electronic trading on Wall Street, network and infrastructure monitoring, fraud detection, and command and control in military environments. Furthermore, as the ``sea change'' caused by cheap micro-sensor technology takes hold, we expect to see everything of material significance on the planet get ``sensor-tagged'' and report its state or location in real time. This sensorization of the real world will lead to a ``green field'' of novel monitoring and control applications with high-volume and low-latency processing requirements.},
  language = {en},
  number = {4},
  journal = {ACM SIGMOD Record},
  author = {Stonebraker, Michael and {\c C}etintemel, U{\v g}ur and Zdonik, Stan},
  month = dec,
  year = {2005},
  pages = {42-47},
  file = {/Users/benblamey/Zotero/storage/X4Z436TX/Stonebraker et al. - 2005 - The 8 requirements of real-time stream processing.pdf}
}

@inproceedings{yuLocationawareAssociatedData2015,
  address = {Kowloon, Hong Kong},
  title = {Location-Aware Associated Data Placement for Geo-Distributed Data-Intensive Applications},
  isbn = {978-1-4799-8381-0},
  doi = {10.1109/INFOCOM.2015.7218428},
  abstract = {Data-intensive applications need to address the problem of how to properly place the set of data items to distributed storage nodes. Traditional techniques use the hashing method to achieve the load balance among nodes such as those in Hadoop and Cassandra, but they do not work efficiently for the requests reading multiple data items in one transaction, especially when the source locations of requests are also distributed. Recent works proposed the managed data placement schemes for online social networks, but have a limited scope of applications due to their focuses. We propose an associated data placement (ADP) scheme, which improves the co-location of associated data and the localized data serving while ensuring the balance between nodes. In ADP, we employ the hypergraph partitioning technique to efficiently partition the set of data items and place them to the distributed nodes, and we also take replicas and incremental adjustment into considerations. Through extensive experiments with both synthesized and trace-based datasets, we evaluate the performance of ADP and demonstrate its effectiveness.},
  language = {en},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Communications}} ({{INFOCOM}})},
  publisher = {{IEEE}},
  author = {Yu, Boyang and Pan, Jianping},
  month = apr,
  year = {2015},
  pages = {603-611},
  file = {/Users/benblamey/Zotero/storage/LRFJP8Q3/Yu and Pan - 2015 - Location-aware associated data placement for geo-d.pdf}
}

@inproceedings{abu-libdehRACSCaseCloud2010,
  address = {New York, NY, USA},
  series = {SoCC '10},
  title = {{{RACS}}: {{A Case}} for {{Cloud Storage Diversity}}},
  isbn = {978-1-4503-0036-0},
  shorttitle = {{{RACS}}},
  doi = {10.1145/1807128.1807165},
  abstract = {The increasing popularity of cloud storage is leading organizations to consider moving data out of their own data centers and into the cloud. However, success for cloud storage providers can present a significant risk to customers; namely, it becomes very expensive to switch storage providers. In this paper, we make a case for applying RAID-like techniques used by disks and file systems, but at the cloud storage level. We argue that striping user data across multiple providers can allow customers to avoid vendor lock-in, reduce the cost of switching providers, and better tolerate provider outages or failures. We introduce RACS, a proxy that transparently spreads the storage load over many providers. We evaluate a prototype of our system and estimate the costs incurred and benefits reaped. Finally, we use trace-driven simulations to demonstrate how RACS can reduce the cost of switching storage vendors for a large organization such as the Internet Archive by seven-fold or more by varying erasure-coding parameters.},
  booktitle = {Proceedings of the 1st {{ACM Symposium}} on {{Cloud Computing}}},
  publisher = {{ACM}},
  author = {{Abu-Libdeh}, Hussam and Princehouse, Lonnie and Weatherspoon, Hakim},
  year = {2010},
  keywords = {cloud computing,fault tolerance,cloud storage,distributed systems,erasure codes,vendor lock-in},
  pages = {229--240},
  file = {/Users/benblamey/Zotero/storage/ATTHBDND/Abu-Libdeh et al. - 2010 - RACS A Case for Cloud Storage Diversity.pdf}
}

@inproceedings{deelmanDataManagementChallenges2008,
  title = {Data {{Management Challenges}} of {{Data}}-{{Intensive Scientific Workflows}}},
  doi = {10.1109/CCGRID.2008.24},
  abstract = {Scientific workflows play an important role in today's science. Many disciplines rely on workflow technologies to orchestrate the execution of thousands of computational tasks. Much research to-date focuses on efficient, scalable, and robust workflow execution, especially in distributed environments. However, many challenges remain in the area of data management related to workflow creation, execution, and result management. In this paper we examine some of these issues in the context of the entire workflow lifecycle.},
  booktitle = {2008 {{Eighth IEEE International Symposium}} on {{Cluster Computing}} and the {{Grid}} ({{CCGRID}})},
  author = {Deelman, E. and Chervenak, A.},
  month = may,
  year = {2008},
  keywords = {distributed processing,Data analysis,Application software,Astronomy,Catalogs,Collaborative work,Computer applications,data management,data-intensive scientific workflow,database management systems,distributed computing,distributed environment,Earthquakes,Information analysis,natural sciences computing,Physics,scientific workflows,Space technology,workflow creation,workflow execution,workflow lifecycle,workflow management software},
  pages = {687-692},
  file = {/Users/benblamey/Zotero/storage/NFQ3VYVC/Deelman and Chervenak - 2008 - Data Management Challenges of Data-Intensive Scien.pdf;/Users/benblamey/Zotero/storage/VPHA58HD/4534284.html}
}

@inproceedings{muralidharF4FacebookWarm2014,
  title = {F4: {{Facebook}}'s {{Warm BLOB Storage System}}},
  shorttitle = {F4},
  booktitle = {Proceedings of the 11th {{USENIX}} Conference on {{Operating Systems Design}} and {{Implementation}}},
  publisher = {{USENIX Association}},
  author = {Muralidhar, Subramanian and Lloyd, Wyatt and Roy, Sabyasachi and Hill, Cory and Lin, Ernest and Liu, Weiwen and Pan, Satadru and Shankar, Shiva and Sivakumar, Viswanath and Tang, Linpeng},
  year = {2014},
  pages = {383--398},
  file = {/Users/benblamey/Zotero/storage/7LWAG9S6/Muralidhar et al. - 2014 - f4 Facebook’s warm blob storage system.pdf}
}

@article{maRemoteSensingBig2015,
  series = {Special Section: A Note on New Trends in Data-Aware Scheduling and Resource Provisioning in Modern HPC Systems},
  title = {Remote Sensing Big Data Computing: {{Challenges}} and Opportunities},
  volume = {51},
  issn = {0167-739X},
  shorttitle = {Remote Sensing Big Data Computing},
  doi = {10.1016/j.future.2014.10.029},
  abstract = {As we have entered an era of high resolution earth observation, the RS data are undergoing an explosive growth. The proliferation of data also give rise to the increasing complexity of RS data, like the diversity and higher dimensionality characteristic of the data. RS data are regarded as RS ``Big Data''. Fortunately, we are witness the coming technological leapfrogging. In this paper, we give a brief overview on the Big Data and data-intensive problems, including the analysis of RS Big Data, Big Data challenges, current techniques and works for processing RS Big Data.},
  journal = {Future Generation Computer Systems},
  author = {Ma, Yan and Wu, Haiping and Wang, Lizhe and Huang, Bormin and Ranjan, Rajiv and Zomaya, Albert and Jie, Wei},
  month = oct,
  year = {2015},
  keywords = {Big data,Data-intensive computing,Remote sensing data processing},
  pages = {47-60},
  file = {/Users/benblamey/Zotero/storage/9V9NZ2YB/Ma et al. - 2015 - Remote sensing big data computing Challenges and .pdf;/Users/benblamey/Zotero/storage/D7VDKX35/S0167739X14002234.html}
}

@inproceedings{zhouCharacteristicsDiscoveryCloud2018,
  title = {I/{{O Characteristics Discovery}} in {{Cloud Storage Systems}}},
  doi = {10.1109/CLOUD.2018.00029},
  abstract = {The data growth from many applications in clouds poses significant challenges to cloud storage systems. To deliver the best storage and I/O performance possible, it is often required to understand and leverage the I/O characteristics based on data accesses. A number of research studies have been carried out on this topic. However, most of them either utilize a limited number of data-access attributes, restricting the general applicability of the method for different applications, or heavily rely on the domain knowledge or expertise about applications' I/O behaviors to select the best representative features, introducing bias for certain workloads. To overcome these limitations, in this study, we present a new I/O characteristic discovery methodology. This method enables capturing data-access features as many as possible to eliminate human bias. It utilizes a machine-learning based strategy to derive the most important set of features automatically, and groups data objects with a clustering algorithm (DBSCAN) to reveal I/O characteristics discovered. These I/O characteristics revealed can direct I/O performance optimizations in numerous scenarios, such as in data prefeteching and data reorganization optimizations in cloud storage systems.},
  booktitle = {2018 {{IEEE}} 11th {{International Conference}} on {{Cloud Computing}} ({{CLOUD}})},
  author = {Zhou, J. and Dai, D. and Mao, Y. and Chen, X. and Zhuang, Y. and Chen, Y.},
  month = jul,
  year = {2018},
  keywords = {cloud computing,Cloud computing,Semantics,Optimization,storage management,characteristic discovery methodology,characteristics discovery,cloud storage systems,Cloud storage systems; file systems; I/O characteristics discovery,Clustering algorithms,Correlation,data mining,data prefeteching,data reorganization optimizations,data-access attributes,data-access features,groups data objects,I/O characteristics,I/O performance optimizations,learning (artificial intelligence),Machine learning,pattern clustering,Principal component analysis},
  pages = {170-177},
  file = {/Users/benblamey/Zotero/storage/JP8WNL76/Zhou et al. - 2018 - IO Characteristics Discovery in Cloud Storage Sys.pdf;/Users/benblamey/Zotero/storage/2SC7IU5P/8457797.html}
}

@inproceedings{dorierOmniscIOGrammarBased2014,
  address = {New Orleans, LA, USA},
  title = {Omnisc'{{IO}}: {{A Grammar}}-{{Based Approach}} to {{Spatial}} and {{Temporal I}}/{{O Patterns Prediction}}},
  isbn = {978-1-4799-5500-8 978-1-4799-5499-5},
  shorttitle = {Omnisc'{{IO}}},
  doi = {10.1109/SC.2014.56},
  abstract = {The increasing gap between the computation performance of post-petascale machines and the performance of their I/O subsystem has motivated many I/O optimizations including prefetching, caching, and scheduling techniques. In order to further improve these techniques, modeling and predicting spatial and temporal I/O patterns of HPC applications as they run has became crucial.},
  language = {en},
  booktitle = {{{SC14}}: {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  publisher = {{IEEE}},
  author = {Dorier, Matthieu and Ibrahim, Shadi and Antoniu, Gabriel and Ross, Rob},
  month = nov,
  year = {2014},
  pages = {623-634},
  file = {/Users/benblamey/Zotero/storage/YA7NCDW2/Dorier et al. - 2014 - Omnisc'IO A Grammar-Based Approach to Spatial and.pdf}
}

@inproceedings{olyMarkovModelPrediction2002,
  address = {New York, NY, USA},
  series = {ICS '02},
  title = {Markov {{Model Prediction}} of {{I}}/{{O Requests}} for {{Scientific Applications}}},
  isbn = {978-1-58113-483-4},
  doi = {10.1145/514191.514214},
  abstract = {Given the increasing performance disparity between processors and storage devices, exploiting knowledge of spatial and temporal I/O requests is critical to achieving high performance, particularly on parallel systems. Although perfect foreknowledge of I/O requests is rarely possible, even estimates of request patterns can potentially yield large performance gains. This paper evaluates Markov models to represent the spatial patterns of I/O requests in scientific codes. The paper also proposes three algorithms for I/O prefetching. Evaluation using I/O traces from scientific codes shows that highly accurate prediction of spatial access patterns, resulting in reduced execution times, is possible.},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Supercomputing}}},
  publisher = {{ACM}},
  author = {Oly, James and Reed, Daniel A.},
  year = {2002},
  keywords = {I/O,Markov model,parallel computing,storage},
  pages = {147--155},
  file = {/Users/benblamey/Zotero/storage/6KDLDGRI/Oly and Reed - 2002 - Markov Model Prediction of IO Requests for Scient.pdf}
}

@inproceedings{xiaFARMERNovelApproach2008,
  title = {{{FARMER}}: A Novel Approach to File Access Correlation Mining and Evaluation Reference Model for Optimizing Peta-Scale File System Performance},
  shorttitle = {{{FARMER}}},
  booktitle = {Proceedings of the 17th International Symposium on {{High}} Performance Distributed Computing},
  publisher = {{ACM}},
  author = {Xia, Peng and Feng, Dan and Jiang, Hong and Tian, Lei and Wang, Fang},
  year = {2008},
  pages = {185--196},
  file = {/Users/benblamey/Zotero/storage/Q2YQ93KP/Xia et al. - 2008 - FARMER a novel approach to file access correlatio.pdf;/Users/benblamey/Zotero/storage/LEJNZFRB/citation.html}
}

@inproceedings{weilCephScalableHighperformance2006,
  title = {Ceph: {{A}} Scalable, High-Performance Distributed File System},
  shorttitle = {Ceph},
  booktitle = {Proceedings of the 7th Symposium on {{Operating}} Systems Design and Implementation},
  publisher = {{USENIX Association}},
  author = {Weil, Sage A. and Brandt, Scott A. and Miller, Ethan L. and Long, Darrell DE and Maltzahn, Carlos},
  year = {2006},
  pages = {307--320},
  file = {/Users/benblamey/Zotero/storage/3ZJ9KX7U/citation.html}
}

@misc{FAST04Technicala,
  title = {{{FAST}} '04 \textemdash{} {{Technical Paper}}},
  howpublished = {https://www.usenix.org/legacy/events/fast04/tech/full\_papers/li/li\_html/},
  file = {/Users/benblamey/Zotero/storage/DJJCY4JJ/li_html.html}
}

@article{drawertMOLNsCloudPlatform2015,
  title = {{{MOLNs}}: {{A Cloud Platform}} for {{Interactive}}, {{Reproducible}} and {{Scalable Spatial Stochastic Computational Experiments}} in {{Systems Biology Using PyURDME}}},
  volume = {abs/1508.03604},
  journal = {CoRR},
  author = {Drawert, Brian and Trogdon, Michael and Toor, Salman Zubair and Petzold, Linda and Hellander, Andreas},
  year = {2015},
  keywords = {aim2.bib}
}

@article{drawertStochasticSimulationService2016,
  title = {Stochastic {{Simulation Service}}: {{Bridging}} the {{Gap}} between the {{Computational Expert}} and the {{Biologist}}},
  volume = {12},
  abstract = {We present StochSS: Stochastic Simulation as a Service, an integrated development environment for modeling and simulation of both deterministic and discrete stochastic biochemical systems in up to three dimensions. An easy to use graphical user interface enables researchers to quickly develop and simulate a biological model on a desktop or laptop, which can then be expanded to incorporate increasing levels of complexity. StochSS features state-of-the-art simulation engines. As the demand for computational power increases, StochSS can seamlessly scale computing resources in the cloud. In addition, StochSS can be deployed as a multi-user software environment where collaborators share computational resources and exchange models via a public model repository. We demonstrate the capabilities and ease of use of StochSS with an example of model development and simulation at increasing levels of complexity.},
  language = {English},
  number = {12},
  journal = {PLoS Comput. Biol.},
  author = {Drawert, Brian and Hellander, Andreas and Bales, Ben and Banerjee, Debjani and Bellesia, Giovanni and Daigle, Jr, Bernie J and Douglas, Geoffrey and Gu, Mengyuan and Gupta, Anand and Hellander, Stefan and Horuk, Chris and Nath, Dibyendu and Takkar, Aviral and Wu, Sheng and L\"otstedt, Per and Krintz, Chandra and Petzold, Linda R},
  month = dec,
  year = {2016},
  pages = {e1005220}
}

@inproceedings{liCMinerMiningBlock2004,
  title = {C-{{Miner}}: {{Mining Block Correlations}} in {{Storage Systems}}.},
  volume = {4},
  shorttitle = {C-{{Miner}}},
  booktitle = {{{FAST}}},
  author = {Li, Zhenmin and Chen, Zhifeng and Srinivasan, Sudarshan M. and Zhou, Yuanyuan},
  year = {2004},
  pages = {173--186},
  file = {/Users/benblamey/Zotero/storage/AWFGZ9RI/li_html.html}
}

@article{ilyasSurveyTopkQuery2008,
  title = {A {{Survey}} of {{Top}}-k {{Query Processing Techniques}} in {{Relational Database Systems}}},
  volume = {40},
  issn = {0360-0300},
  doi = {10.1145/1391729.1391730},
  abstract = {Efficient processing of top-k queries is a crucial requirement in many interactive environments that involve massive amounts of data. In particular, efficient top-k processing in domains such as the Web, multimedia search, and distributed systems has shown a great impact on performance. In this survey, we describe and classify top-k processing techniques in relational databases. We discuss different design dimensions in the current techniques including query models, data access methods, implementation levels, data and query certainty, and supported scoring functions. We show the implications of each dimension on the design of the underlying techniques. We also discuss top-k queries in XML domain, and show their connections to relational approaches.},
  number = {4},
  journal = {ACM Comput. Surv.},
  author = {Ilyas, Ihab F. and Beskales, George and Soliman, Mohamed A.},
  month = oct,
  year = {2008},
  keywords = {rank aggregation,rank-aware processing,Top-<i>k</i>,voting},
  pages = {11:1--11:58},
  file = {/Users/benblamey/Zotero/storage/7IC55CNW/Ilyas et al. - 2008 - A Survey of Top-k Query Processing Techniques in R.pdf}
}

@misc{ElsevierEnhancedReader,
  title = {Elsevier {{Enhanced Reader}}},
  language = {en},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0167739X17311202?token=8DE5B9021500144A54566F3CA39FBDEF754738CA8DE7C1CD5991B69FE16C4641DA2504ECA1396A562FA06FE3248516FA},
  file = {/Users/benblamey/Zotero/storage/IMIU6W5S/Elsevier Enhanced Reader.pdf;/Users/benblamey/Zotero/storage/8TELMZ78/S0167739X17311202.html},
  doi = {10.1016/j.future.2017.05.041}
}

@article{ausmeesBAMSIMulticloudService2018,
  title = {{{BAMSI}}: A Multi-Cloud Service for Scalable Distributed Filtering of Massive Genome Data},
  volume = {19},
  issn = {1471-2105},
  shorttitle = {{{BAMSI}}},
  doi = {10.1186/s12859-018-2241-z},
  abstract = {The advent of next-generation sequencing (NGS) has made whole-genome sequencing of cohorts of individuals a reality. Primary datasets of raw or aligned reads of this sort can get very large. For scientific questions where curated called variants are not sufficient, the sheer size of the datasets makes analysis prohibitively expensive. In order to make re-analysis of such data feasible without the need to have access to a large-scale computing facility, we have developed a highly scalable, storage-agnostic framework, an associated API and an easy-to-use web user interface to execute custom filters on large genomic datasets.},
  number = {1},
  journal = {BMC Bioinformatics},
  author = {Ausmees, Kristiina and John, Aji and Toor, Salman Z. and Hellander, Andreas and Nettelblad, Carl},
  month = jun,
  year = {2018},
  pages = {240},
  file = {/Users/benblamey/Zotero/storage/EIQGVMDN/Ausmees et al. - 2018 - BAMSI a multi-cloud service for scalable distribu.pdf;/Users/benblamey/Zotero/storage/DZXK6PCB/s12859-018-2241-z.html}
}


